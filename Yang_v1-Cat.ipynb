{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as th\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import LSMDC as LD2\n",
    "import MSRVTT as MSR\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "import argparse\n",
    "#from loss import MaxMarginRankingLoss\n",
    "from loss import MaxMarginRankingLoss_Distance  #Yang changed\n",
    "from model_yang import Net_Fuse_Cat\n",
    "from torch.autograd import Variable\n",
    "import os\n",
    "import random\n",
    "from qcm_sampler import QCMSampler\n",
    "from MSR_sampler import MSRSampler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(GPU=True, MSRVTT=False, batch_size=128, coco=False, coco_sampling_rate=1.0, epochs=50, eval_qcm=False, lr=0.0001, lr_decay=0.95, margin=0.2, model_name='test', momentum=0.9, n_cpu=1, n_display=100, optimizer='adam', seed=1, text_cluster_size=32)\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser(description='LSMDC2017')\n",
    "\n",
    "parser.add_argument('--coco', type=bool, default=False,\n",
    "                            help='add coco dataset')\n",
    "\n",
    "parser.add_argument('--lr', type=float, default=0.0001,\n",
    "                            help='initial learning rate')\n",
    "parser.add_argument('--epochs', type=int, default=50,\n",
    "                            help='upper epoch limit')\n",
    "parser.add_argument('--batch_size', type=int, default=128,\n",
    "                            help='batch size')\n",
    "parser.add_argument('--text_cluster_size', type=int, default=32,\n",
    "                            help='Text cluster size')\n",
    "parser.add_argument('--margin', type=float, default=0.2,\n",
    "                            help='MaxMargin margin value')\n",
    "parser.add_argument('--lr_decay', type=float, default=0.95,\n",
    "                            help='Learning rate exp epoch decay')\n",
    "parser.add_argument('--n_display', type=int, default=100,\n",
    "                            help='Information display frequence')\n",
    "parser.add_argument('--GPU', type=bool, default=True,\n",
    "                            help='Use of GPU')\n",
    "parser.add_argument('--n_cpu', type=int, default=1,\n",
    "                            help='Number of CPU')\n",
    "\n",
    "parser.add_argument('--model_name', type=str, default='test',\n",
    "                            help='Model name')\n",
    "parser.add_argument('--seed', type=int, default=1,\n",
    "                            help='Initial Random Seed')\n",
    "\n",
    "parser.add_argument('--optimizer', type=str, default='adam',\n",
    "                            help='optimizer')\n",
    "parser.add_argument('--momentum', type=float, default=0.9,\n",
    "                            help='Nesterov Momentum for SGD')\n",
    "\n",
    "\n",
    "parser.add_argument('--eval_qcm', type=bool, default=False,\n",
    "                            help='Eval or not QCM')\n",
    "\n",
    "parser.add_argument('--MSRVTT', type=bool, default=False,\n",
    "                            help='MSRVTT')\n",
    "\n",
    "parser.add_argument('--coco_sampling_rate', type=float, default=1.0,\n",
    "                            help='coco sampling rate')\n",
    "\n",
    "\n",
    "#args = parser.parse_args()\n",
    "args, unknown =  parser.parse_known_args()\n",
    "\n",
    "print (args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_feat = 'data'\n",
    "\n",
    "mp_visual_path = os.path.join(root_feat,'X_resnet.npy')\n",
    "mp_flow_path = os.path.join(root_feat,'X_flow.npy')\n",
    "mp_face_path = os.path.join(root_feat,'X_face.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predefining random initial seeds\n",
    "th.manual_seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "random.seed(args.seed)\n",
    "\n",
    "if args.eval_qcm and not(args.MSRVTT):\n",
    "    print ('sha')\n",
    "    qcm_dataset = LD2.LSMDC_qcm(os.path.join(root_feat,'resnet-qcm.npy'),\n",
    "            os.path.join(root_feat,'w2v_LSMDC_qcm.npy'), os.path.join(root_feat,'X_audio_test.npy'),\n",
    "            os.path.join(root_feat,'flow-qcm.npy'),\n",
    "            os.path.join(root_feat,'face-qcm.npy')) \n",
    "    \n",
    "    qcm_sampler = QCMSampler(len(qcm_dataset))\n",
    "    qcm_dataloader = DataLoader(qcm_dataset, batch_size=500, sampler=qcm_sampler, num_workers=1)\n",
    "    qcm_gt_fn = os.path.join(root_feat,'multiple_choice_gt.txt')\n",
    "    qcm_gt = [line.rstrip('\\n') for line in open(qcm_gt_fn)]\n",
    "    qcm_gt = np.array(map(int,qcm_gt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_tensor(l, max_len):\n",
    "    tensor = np.zeros((len(l),max_len,l[0].shape[-1]))\n",
    "    for i in range(len(l)):\n",
    "        if len(l[i]):\n",
    "            tensor[i,:min(max_len,l[i].shape[0]),:] = l[i][:min(max_len,l[i].shape[0])]\n",
    "\n",
    "    return th.from_numpy(tensor).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verbose(epoch, status, metrics, name='TEST'):\n",
    "    print(name+' - epoch: %d, epoch status: %.2f, r@1: %.3f, r@5: %.3f, r@10: %.3f, mr: %d' % \n",
    "            (epoch + 1, status, \n",
    "                metrics['R1'], metrics['R5'], metrics['R10'],\n",
    "                metrics['MR']))\n",
    "\n",
    "\n",
    "def compute_metric(x):\n",
    "    sx = np.sort(-x, axis=1)\n",
    "    d = np.diag(-x)\n",
    "    d = d[:,np.newaxis]\n",
    "    ind = sx - d\n",
    "    ind = np.where(ind == 0)\n",
    "    ind = ind[1]\n",
    "\n",
    "    metrics = {}\n",
    "    metrics['R1'] = float(np.sum(ind == 0))/len(ind)\n",
    "    metrics['R5'] = float(np.sum(ind < 5))/len(ind)\n",
    "    metrics['R10'] = float(np.sum(ind < 10))/len(ind)\n",
    "    metrics['MR'] = np.median(ind) + 1\n",
    "\n",
    "    return metrics\n",
    "\n",
    "def compute_metric_yang(x):\n",
    "    sx = np.sort(x, axis=1)\n",
    "    d = np.diag(x) #(1000)\n",
    "    d = d[:,np.newaxis] #(1000,1)\n",
    "    ind = sx - d\n",
    "    ind = np.where(ind == 0)\n",
    "    ind = ind[1]\n",
    "\n",
    "    metrics = {}\n",
    "    metrics['R1'] = float(np.sum(ind == 0))/len(ind)  # for 1000 sample, howmany correct one is at the first\n",
    "    metrics['R5'] = float(np.sum(ind < 5))/len(ind)   # for 1000 sample, howmany correct one is among the first five\n",
    "    metrics['R10'] = float(np.sum(ind < 10))/len(ind) \n",
    "    metrics['MR'] = np.median(ind) + 1\n",
    "\n",
    "    return metrics\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = th.device(\"cuda:0\" if th.cuda.is_available() else \"cpu\")\n",
    "moe_weights=np.zeros((128,4))+0.25\n",
    "moe_weights_tensor = th.from_numpy(moe_weights).float().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-loading features ... This may takes several minutes ...\n",
      "Done.\n",
      "Reading test data ...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "print ('Pre-loading features ... This may takes several minutes ...')\n",
    "\n",
    "if args.MSRVTT:\n",
    "    print ('MSRVTT')\n",
    "    visual_feat_path = os.path.join(root_feat,'resnet_features.pickle')  \n",
    "    flow_feat_path = os.path.join(root_feat,'flow_features.pickle')\n",
    "    text_feat_path = os.path.join(root_feat,'w2v_MSRVTT.pickle')\n",
    "    audio_feat_path = os.path.join(root_feat,'audio_features.pickle')\n",
    "    face_feat_path = os.path.join(root_feat,'face_features.pickle')\n",
    "    train_list_path = os.path.join(root_feat,'train_list.txt')\n",
    "    test_list_path = os.path.join(root_feat,'test_list.txt')\n",
    "\n",
    "    dataset = MSR.MSRVTT(visual_feat_path, flow_feat_path, text_feat_path,\n",
    "            audio_feat_path, face_feat_path, train_list_path,test_list_path, coco=args.coco) \n",
    "    msr_sampler = MSRSampler(dataset.n_MSR, dataset.n_coco, args.coco_sampling_rate)\n",
    "    \n",
    "    if args.coco:\n",
    "        dataloader = DataLoader(dataset, batch_size=args.batch_size,\n",
    "                sampler=msr_sampler, num_workers=1,collate_fn=dataset.collate_data, drop_last=True)\n",
    "    else:\n",
    "        dataloader = DataLoader(dataset, batch_size=args.batch_size,\n",
    "                shuffle=True, num_workers=1,collate_fn=dataset.collate_data, drop_last=True)\n",
    "\n",
    "else:\n",
    "    path_to_text = os.path.join(root_feat,'w2v_LSMDC.npy')\n",
    "    path_to_audio = os.path.join(root_feat,'X_audio_train.npy')\n",
    "    path_to_coco_visual_path= os.path.join(root_feat,'X_train2014_resnet152.npy') #Yang add\n",
    "    path_to_coco_text_path=os.path.join(root_feat,'w2v_coco_train2014_1.npy')#Yang add\n",
    "    \n",
    "\n",
    "    #dataset = LD2.LSMDC(mp_visual_path, path_to_text,\n",
    "    #        path_to_audio, mp_flow_path, mp_face_path, coco=args.coco) \n",
    "    #coco=args.coco\n",
    "    dataset = LD2.LSMDC(mp_visual_path, path_to_text,\n",
    "            path_to_audio, mp_flow_path, mp_face_path,path_to_coco_visual_path,path_to_coco_text_path, args.coco) \n",
    "    dataloader = DataLoader(dataset, batch_size=args.batch_size,\n",
    "            shuffle=True, num_workers=1, drop_last=True)\n",
    "    print ('Done.')\n",
    "\n",
    "    print ('Reading test data ...')\n",
    "    resnet_features_path = os.path.join(root_feat,'resnet152-retrieval.npy.tensor.npy')\n",
    "    flow_features_path = os.path.join(root_feat,'flow-retrieval.npy.tensor.npy')\n",
    "    face_features_path = os.path.join(root_feat,'face-retrieval.npy.tensor.npy')\n",
    "    text_features_path = os.path.join(root_feat,'w2v_LSMDC_retrieval.npy')\n",
    "    audio_features_path = os.path.join(root_feat,'X_audio_retrieval.npy.tensor.npy')\n",
    "\n",
    "    vid_retrieval = np.load(resnet_features_path, encoding='latin1') # torch size [1000,2048]\n",
    "    flow_retrieval = np.load(flow_features_path, encoding='latin1') # torch size [1000,1024]\n",
    "    face_retrieval = np.load(face_features_path, encoding='latin1')# torch size [1000,128]\n",
    "    text_retrieval = np.load(text_features_path, encoding='latin1')# torch Size([1000, 29, 300]) \n",
    "                                                                   # left -->right, maximum word length 29, \n",
    "                                                                   # if not enough, fill all zeros\n",
    "    audio_retrieval = np.load(audio_features_path, encoding='latin1') #torch.Size([1000, 30, 128])\n",
    "\n",
    "    mm = max(map(len,text_retrieval)) # map(func, iterable)  =29\n",
    "\n",
    "    text_retrieval = make_tensor(text_retrieval,mm)\n",
    "\n",
    "    vid_retrieval = th.from_numpy(vid_retrieval).float() # torch size [1000,2048]\n",
    "    flow_retrieval = th.from_numpy(flow_retrieval).float() # torch size [1000,1024]\n",
    "    face_retrieval = th.from_numpy(face_retrieval).float() # torch size [1000,128]\n",
    "    audio_retrieval = th.from_numpy(audio_retrieval).float() #torch.Size([1000, 30, 128])\n",
    "\n",
    "    text_retrieval_val = text_retrieval\n",
    "    vid_retrieval_val = vid_retrieval\n",
    "    flow_retrieval_val = flow_retrieval\n",
    "    face_retrieval_val = face_retrieval\n",
    "    audio_retrieval_val = audio_retrieval\n",
    "\n",
    "\n",
    "    face_ind_test = np.load(os.path.join(root_feat,'no_face_ind_retrieval.npy'))\n",
    "    face_ind_test = 1 - face_ind_test # for no face data, all descriptors==0\n",
    "print ('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([101074, 30, 300])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.text_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2048\n",
      "9600\n"
     ]
    }
   ],
   "source": [
    "# Model\n",
    "#video_modality_dim = {'face': (128,128), 'audio': (128*16,128),\n",
    "#'visual': (2048,2048), 'motion': (1024,1024)}\n",
    "\n",
    "video_modality_dim_fuse = {'face': (128,128,256), 'audio': (128*16,128,256),\n",
    "'visual': (2048,2048,256), 'motion': (1024,1024,256)}\n",
    "\n",
    "text_dim={'text':(300,128,256)}\n",
    "net = Net_Fuse_Cat(video_modality_dim_fuse,text_dim,\n",
    "        audio_cluster=16,text_cluster=args.text_cluster_size)\n",
    "#net.train()\n",
    "\n",
    "if args.GPU:\n",
    "    net.cuda()\n",
    "\n",
    "# Optimizers + Loss\n",
    "#max_margin = MaxMarginRankingLoss(margin=args.margin) \n",
    "max_margin = MaxMarginRankingLoss_Distance(margin=args.margin)  #Yang changed\n",
    "\n",
    "\n",
    "if args.optimizer == 'adam':\n",
    "    optimizer = optim.Adam(net.parameters(), lr=args.lr)\n",
    "elif args.optimizer == 'sgd':\n",
    "    optimizer = optim.SGD(net.parameters(), lr=args.lr, momentum=args.momentum)\n",
    "\n",
    "if args.GPU:\n",
    "    max_margin.cuda()\n",
    "\n",
    "n_display = args.n_display\n",
    "dataset_size = len(dataset)\n",
    "lr_decay = args.lr_decay\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training loop ...\n",
      "epoch: 0\n",
      "True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/yangl/external/anaconda3/envs/pytorch/lib/python3.7/site-packages/ipykernel/__main__.py:53: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Epoch status: 0.13, Training loss: 0.0023\n",
      "Epoch 1, Epoch status: 0.25, Training loss: 0.0022\n",
      "Epoch 1, Epoch status: 0.38, Training loss: 0.0021\n",
      "Epoch 1, Epoch status: 0.51, Training loss: 0.0021\n",
      "Epoch 1, Epoch status: 0.63, Training loss: 0.0020\n",
      "Epoch 1, Epoch status: 0.76, Training loss: 0.0019\n",
      "Epoch 1, Epoch status: 0.89, Training loss: 0.0019\n",
      "evaluating epoch 1 ...\n",
      "False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/yangl/external/anaconda3/envs/pytorch/lib/python3.7/site-packages/ipykernel/__main__.py:93: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "/users/yangl/external/anaconda3/envs/pytorch/lib/python3.7/site-packages/ipykernel/__main__.py:94: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "/users/yangl/external/anaconda3/envs/pytorch/lib/python3.7/site-packages/ipykernel/__main__.py:95: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "/users/yangl/external/anaconda3/envs/pytorch/lib/python3.7/site-packages/ipykernel/__main__.py:96: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "/users/yangl/external/anaconda3/envs/pytorch/lib/python3.7/site-packages/ipykernel/__main__.py:97: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPII - epoch: 1, epoch status: 0.00, r@1: 0.994, r@5: 1.000, r@10: 1.000, mr: 1\n",
      "epoch: 1\n",
      "True\n",
      "Epoch 2, Epoch status: 0.13, Training loss: 0.0018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-4:\n",
      "Traceback (most recent call last):\n",
      "  File \"/users/yangl/external/anaconda3/envs/pytorch/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/users/yangl/external/anaconda3/envs/pytorch/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/users/yangl/external/anaconda3/envs/pytorch/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 106, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/users/yangl/external/anaconda3/envs/pytorch/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 184, in default_collate\n",
      "    return {key: default_collate([d[key] for d in batch]) for key in batch[0]}\n",
      "  File \"/users/yangl/external/anaconda3/envs/pytorch/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 184, in <dictcomp>\n",
      "    return {key: default_collate([d[key] for d in batch]) for key in batch[0]}\n",
      "  File \"/users/yangl/external/anaconda3/envs/pytorch/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 164, in default_collate\n",
      "    return torch.stack(batch, 0, out=out)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-89d75ab59279>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     48\u001b[0m                 {'face': face, 'audio': audio, 'visual': video, 'motion': flow}, ind, True)\n\u001b[1;32m     49\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax_margin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/external/anaconda3/envs/pytorch/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/external/anaconda3/envs/pytorch/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print ('Starting training loop ...')\n",
    "\n",
    "#for epoch in range(args.epochs):\n",
    "    \n",
    "for epoch in range(args.epochs):\n",
    "#     net.train()\n",
    "#     #net.train(False)\n",
    "    net.train()\n",
    "    running_loss = 0.0\n",
    "    print ('epoch: %d'%epoch)\n",
    "    print (net.training)\n",
    "\n",
    "    for i_batch, sample_batched in enumerate(dataloader):\n",
    "\n",
    "        if args.MSRVTT:\n",
    "            captions = sample_batched['text']\n",
    "            audio = sample_batched['audio']\n",
    "        else:\n",
    "            captions = dataset.shorteningTextTensor(sample_batched['text'],\n",
    "                    sample_batched['text_size']) #[128,30,300] ==>[128,28,300] , max length 30\n",
    "                                                 # the longest text length is 28 in this batch\n",
    "            \n",
    "            audio = dataset.shorteningTextTensor(sample_batched['audio'],\n",
    "                    sample_batched['audio_size']) #[128, 183, 128]==> [128, 16, 128], max length 183\n",
    "                                                  # the longest text length is 16 in this batch\n",
    "       \n",
    "\n",
    "        face = sample_batched['face'] #[128,128]\n",
    "        video = sample_batched['video']  #[128,2048]\n",
    "        flow = sample_batched['flow'] #[128,1024]\n",
    "        coco_ind = sample_batched['coco_ind'] #[128], 0 indicates not coco image\n",
    "        face_ind = sample_batched['face_ind'] #[128], 0 indicates no face descriptor\n",
    "\n",
    "        ind = {}\n",
    "        ind['face'] = face_ind  # [128]\n",
    "        ind['visual'] = np.ones((len(face_ind))) # all one mask for 'visual'\n",
    "        ind['motion'] = 1 - coco_ind # lsmdb has one for motion and audio,\n",
    "        ind['audio'] = 1 - coco_ind # coco is 0 since no motion/audio\n",
    "\n",
    "        if args.GPU:\n",
    "            captions, video = Variable(captions.cuda()), Variable(video.cuda())\n",
    "            audio, flow  =  Variable(audio.cuda()), Variable(flow.cuda())\n",
    "            face = Variable(face.cuda())\n",
    "\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        confusion_matrix = net(captions,\n",
    "                {'face': face, 'audio': audio, 'visual': video, 'motion': flow}, ind, True)\n",
    "        loss = max_margin(confusion_matrix)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        running_loss += loss.data[0]\n",
    "        #print(loss.data[0])\n",
    "        #if i_batch==2:\n",
    "        #    break\n",
    "        \n",
    "        if (i_batch+1) % n_display == 0:\n",
    "            print ('Epoch %d, Epoch status: %.2f, Training loss: %.4f'%(epoch + 1,\n",
    "                    args.batch_size*float(i_batch)/dataset_size,running_loss/n_display))\n",
    "            running_loss = 0.0\n",
    "\n",
    "    print ('evaluating epoch %d ...'%(epoch+1))\n",
    "    net.eval()  \n",
    "    with torch.no_grad():\n",
    "    #if True:\n",
    "        if args.MSRVTT:\n",
    "            retrieval_samples = dataset.getRetrievalSamples()\n",
    "\n",
    "            video = Variable(retrieval_samples['video'].cuda(), volatile=True)\n",
    "            captions = Variable(retrieval_samples['text'].cuda(), volatile=True)\n",
    "            audio = Variable(retrieval_samples['audio'].cuda(), volatile=True)\n",
    "            flow = Variable(retrieval_samples['flow'].cuda(), volatile=True)\n",
    "            face = Variable(retrieval_samples['face'].cuda(), volatile=True)\n",
    "            face_ind = retrieval_samples['face_ind']\n",
    "\n",
    "            ind = {}\n",
    "            ind['face'] = face_ind\n",
    "            ind['visual'] = np.ones((len(face_ind)))\n",
    "            ind['motion'] = np.ones((len(face_ind)))\n",
    "            ind['audio'] = np.ones((len(face_ind)))\n",
    "\n",
    "\n",
    "            conf = net(captions,\n",
    "                    {'face': face, 'audio': audio, 'visual': video, 'motion': flow}, ind, True)\n",
    "            confusion_matrix = conf.data.cpu().float().numpy() #[1000,1000] ==>change tensor to numpy\n",
    "            metrics = compute_metric_yang(confusion_matrix)\n",
    "            verbose(epoch, args.batch_size*float(i_batch)/dataset_size, metrics, name='MSRVTT')\n",
    "\n",
    "        else:\n",
    "            i_batch=1\n",
    "            print (net.training)\n",
    "            video = Variable(vid_retrieval_val.cuda(), volatile=True)\n",
    "            captions = Variable(text_retrieval_val.cuda(), volatile=True)\n",
    "            audio = Variable(audio_retrieval_val.cuda(), volatile=True)\n",
    "            flow = Variable(flow_retrieval_val.cuda(), volatile=True)\n",
    "            face = Variable(face_retrieval_val.cuda(), volatile=True)\n",
    "\n",
    "            ind = {}\n",
    "            ind['face'] = face_ind_test\n",
    "            ind['visual'] = np.ones((len(face_ind_test)))\n",
    "            ind['motion'] = np.ones((len(face_ind_test)))\n",
    "            ind['audio'] = np.ones((len(face_ind_test)))\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            conf = net(captions,\n",
    "                    {'face': face, 'audio': audio, 'visual': video, 'motion': flow}, ind, True)\n",
    "            confusion_matrix_text = conf.data.cpu().float().numpy()\n",
    "            metrics = compute_metric_yang(confusion_matrix_text)\n",
    "            #verbose(epoch, args.batch_size*float(i_batch)/dataset_size, metrics, name='MPII')\n",
    "            verbose(epoch, args.batch_size*float(i_batch)/dataset_size, metrics, name='MPII')\n",
    "\n",
    "\n",
    "    #net.train()\n",
    "    \n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] *= lr_decay\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 1000)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix_text.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fd6bc0a5470>]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.plot(confusion_matrix_text[0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.2133508"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmin(confusion_matrix_text[2,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on built-in function sum:\n",
      "\n",
      "sum(...)\n",
      "    .. function:: sum(input, dtype=None) -> Tensor\n",
      "    \n",
      "    Returns the sum of all elements in the :attr:`input` tensor.\n",
      "    \n",
      "    Args:\n",
      "        input (Tensor): the input tensor\n",
      "        dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor.\n",
      "            If specified, the input tensor is casted to :attr:`dtype` before the operation\n",
      "            is performed. This is useful for preventing data type overflows. Default: None.\n",
      "    \n",
      "    Example::\n",
      "    \n",
      "        >>> a = torch.randn(1, 3)\n",
      "        >>> a\n",
      "        tensor([[ 0.1133, -0.9567,  0.2958]])\n",
      "        >>> torch.sum(a)\n",
      "        tensor(-0.5475)\n",
      "    \n",
      "    .. function:: sum(input, dim, keepdim=False, dtype=None) -> Tensor\n",
      "    \n",
      "    Returns the sum of each row of the :attr:`input` tensor in the given\n",
      "    dimension :attr:`dim`. If :attr::`dim` is a list of dimensions,\n",
      "    reduce over all of them.\n",
      "    \n",
      "    If :attr:`keepdim` is ``True``, the output tensor is of the same size\n",
      "    as :attr:`input` except in the dimension :attr:`dim` where it is of size 1.\n",
      "    Otherwise, :attr:`dim` is squeezed (see :func:`torch.squeeze`), resulting in\n",
      "    the output tensor having 1 fewer dimension than :attr:`input`.\n",
      "    \n",
      "    Args:\n",
      "        input (Tensor): the input tensor\n",
      "        dim (int or tuple of ints): the dimension or dimensions to reduce\n",
      "        keepdim (bool): whether the output tensor has :attr:`dim` retained or not\n",
      "        dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor.\n",
      "            If specified, the input tensor is casted to :attr:`dtype` before the operation\n",
      "            is performed. This is useful for preventing data type overflows. Default: None.\n",
      "    \n",
      "    Example::\n",
      "    \n",
      "        >>> a = torch.randn(4, 4)\n",
      "        >>> a\n",
      "        tensor([[ 0.0569, -0.2475,  0.0737, -0.3429],\n",
      "                [-0.2993,  0.9138,  0.9337, -1.6864],\n",
      "                [ 0.1132,  0.7892, -0.1003,  0.5688],\n",
      "                [ 0.3637, -0.9906, -0.4752, -1.5197]])\n",
      "        >>> torch.sum(a, 1)\n",
      "        tensor([-0.4598, -0.1381,  1.3708, -2.6217])\n",
      "        >>> b = torch.arange(4 * 5 * 6).view(4, 5, 6)\n",
      "        >>> torch.sum(b, (2, 1))\n",
      "        tensor([  435.,  1335.,  2235.,  3135.])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "    help(th.sum)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aa=confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sx = np.sort(aa, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = np.diag(aa) #(1000)\n",
    "d = d[:,np.newaxis] #(1000,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = sx - d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_pos = np.where(ind == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_pos[1][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metric_yang(x):\n",
    "    sx = np.sort(x, axis=1)\n",
    "    d = np.diag(x) #(1000)\n",
    "    d = d[:,np.newaxis] #(1000,1)\n",
    "    ind = sx - d\n",
    "    ind = np.where(ind == 0)\n",
    "    ind = ind[1]\n",
    "\n",
    "    metrics = {}\n",
    "    metrics['R1'] = float(np.sum(ind == 0))/len(ind)\n",
    "    metrics['R5'] = float(np.sum(ind < 5))/len(ind)\n",
    "    metrics['R10'] = float(np.sum(ind < 10))/len(ind)\n",
    "    metrics['MR'] = np.median(ind) + 1\n",
    "\n",
    "    return metrics\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " metrics_yang = compute_metric_distance(confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(GPU=True, MSRVTT=False, batch_size=128, coco=False, coco_sampling_rate=1.0, epochs=50, eval_qcm=False, lr=0.0001, lr_decay=0.95, margin=0.2, model_name='test', momentum=0.9, n_cpu=4, n_display=100, optimizer='adam', seed=1, text_cluster_size=32)\n",
      "Pre-loading features ... This may takes several minutes ...\n",
      "Done.\n",
      "Reading test data ...\n",
      "Done.\n",
      "Starting training loop ...\n",
      "epoch: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/shared/nfs1/yangl/code/VideoText/VideoRetrival/train.py:275: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "  running_loss += loss.data[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Epoch status: 0.13, Training loss: 0.0950\n",
      "Epoch 1, Epoch status: 0.25, Training loss: 0.0624\n",
      "Epoch 1, Epoch status: 0.38, Training loss: 0.0544\n",
      "Epoch 1, Epoch status: 0.51, Training loss: 0.0516\n",
      "Epoch 1, Epoch status: 0.63, Training loss: 0.0484\n",
      "Epoch 1, Epoch status: 0.76, Training loss: 0.0474\n",
      "Epoch 1, Epoch status: 0.89, Training loss: 0.0458\n",
      "evaluating epoch 1 ...\n",
      "MPII - epoch: 1, epoch status: 1.00, r@1: 0.079, r@5: 0.196, r@10: 0.276, mr: 37\n",
      "epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/shared/nfs1/yangl/code/VideoText/VideoRetrival/train.py:308: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  video = Variable(vid_retrieval_val.cuda(), volatile=True)\n",
      "/scratch/shared/nfs1/yangl/code/VideoText/VideoRetrival/train.py:309: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  captions = Variable(text_retrieval_val.cuda(), volatile=True)\n",
      "/scratch/shared/nfs1/yangl/code/VideoText/VideoRetrival/train.py:310: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  audio = Variable(audio_retrieval_val.cuda(), volatile=True)\n",
      "/scratch/shared/nfs1/yangl/code/VideoText/VideoRetrival/train.py:311: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  flow = Variable(flow_retrieval_val.cuda(), volatile=True)\n",
      "/scratch/shared/nfs1/yangl/code/VideoText/VideoRetrival/train.py:312: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  face = Variable(face_retrieval_val.cuda(), volatile=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Epoch status: 0.13, Training loss: 0.0391\n",
      "Epoch 2, Epoch status: 0.25, Training loss: 0.0389\n",
      "Epoch 2, Epoch status: 0.38, Training loss: 0.0383\n",
      "Epoch 2, Epoch status: 0.51, Training loss: 0.0383\n",
      "Epoch 2, Epoch status: 0.63, Training loss: 0.0371\n",
      "Epoch 2, Epoch status: 0.76, Training loss: 0.0382\n",
      "Epoch 2, Epoch status: 0.89, Training loss: 0.0386\n",
      "evaluating epoch 2 ...\n",
      "MPII - epoch: 2, epoch status: 1.00, r@1: 0.092, r@5: 0.236, r@10: 0.323, mr: 32\n",
      "epoch: 2\n",
      "Epoch 3, Epoch status: 0.13, Training loss: 0.0326\n",
      "Epoch 3, Epoch status: 0.25, Training loss: 0.0328\n",
      "Epoch 3, Epoch status: 0.38, Training loss: 0.0327\n",
      "Epoch 3, Epoch status: 0.51, Training loss: 0.0328\n",
      "Epoch 3, Epoch status: 0.63, Training loss: 0.0324\n",
      "Epoch 3, Epoch status: 0.76, Training loss: 0.0325\n",
      "Epoch 3, Epoch status: 0.89, Training loss: 0.0326\n",
      "evaluating epoch 3 ...\n",
      "MPII - epoch: 3, epoch status: 1.00, r@1: 0.095, r@5: 0.257, r@10: 0.343, mr: 28\n",
      "epoch: 3\n",
      "Epoch 4, Epoch status: 0.13, Training loss: 0.0280\n",
      "Epoch 4, Epoch status: 0.25, Training loss: 0.0278\n",
      "Epoch 4, Epoch status: 0.38, Training loss: 0.0287\n",
      "Epoch 4, Epoch status: 0.51, Training loss: 0.0291\n",
      "Epoch 4, Epoch status: 0.63, Training loss: 0.0290\n",
      "Epoch 4, Epoch status: 0.76, Training loss: 0.0292\n",
      "Epoch 4, Epoch status: 0.89, Training loss: 0.0289\n",
      "evaluating epoch 4 ...\n",
      "MPII - epoch: 4, epoch status: 1.00, r@1: 0.110, r@5: 0.264, r@10: 0.362, mr: 24\n",
      "epoch: 4\n",
      "Epoch 5, Epoch status: 0.13, Training loss: 0.0248\n",
      "Epoch 5, Epoch status: 0.25, Training loss: 0.0249\n",
      "Epoch 5, Epoch status: 0.38, Training loss: 0.0253\n",
      "Epoch 5, Epoch status: 0.51, Training loss: 0.0259\n",
      "Epoch 5, Epoch status: 0.63, Training loss: 0.0254\n",
      "Epoch 5, Epoch status: 0.76, Training loss: 0.0264\n",
      "Epoch 5, Epoch status: 0.89, Training loss: 0.0263\n",
      "evaluating epoch 5 ...\n",
      "MPII - epoch: 5, epoch status: 1.00, r@1: 0.108, r@5: 0.277, r@10: 0.357, mr: 26\n",
      "epoch: 5\n",
      "Epoch 6, Epoch status: 0.13, Training loss: 0.0217\n",
      "Epoch 6, Epoch status: 0.25, Training loss: 0.0224\n",
      "Epoch 6, Epoch status: 0.38, Training loss: 0.0227\n",
      "Epoch 6, Epoch status: 0.51, Training loss: 0.0226\n",
      "Epoch 6, Epoch status: 0.63, Training loss: 0.0237\n",
      "Epoch 6, Epoch status: 0.76, Training loss: 0.0233\n",
      "Epoch 6, Epoch status: 0.89, Training loss: 0.0231\n",
      "evaluating epoch 6 ...\n",
      "MPII - epoch: 6, epoch status: 1.00, r@1: 0.118, r@5: 0.272, r@10: 0.371, mr: 22\n",
      "epoch: 6\n",
      "Epoch 7, Epoch status: 0.13, Training loss: 0.0194\n",
      "Epoch 7, Epoch status: 0.25, Training loss: 0.0201\n",
      "Epoch 7, Epoch status: 0.38, Training loss: 0.0204\n",
      "Epoch 7, Epoch status: 0.51, Training loss: 0.0205\n",
      "Epoch 7, Epoch status: 0.63, Training loss: 0.0208\n",
      "Epoch 7, Epoch status: 0.76, Training loss: 0.0211\n",
      "Epoch 7, Epoch status: 0.89, Training loss: 0.0215\n",
      "evaluating epoch 7 ...\n",
      "MPII - epoch: 7, epoch status: 1.00, r@1: 0.125, r@5: 0.267, r@10: 0.380, mr: 22\n",
      "epoch: 7\n",
      "Epoch 8, Epoch status: 0.13, Training loss: 0.0178\n",
      "Epoch 8, Epoch status: 0.25, Training loss: 0.0182\n",
      "Epoch 8, Epoch status: 0.38, Training loss: 0.0184\n",
      "Epoch 8, Epoch status: 0.51, Training loss: 0.0185\n",
      "Epoch 8, Epoch status: 0.63, Training loss: 0.0187\n",
      "Epoch 8, Epoch status: 0.76, Training loss: 0.0189\n",
      "Epoch 8, Epoch status: 0.89, Training loss: 0.0192\n",
      "evaluating epoch 8 ...\n",
      "MPII - epoch: 8, epoch status: 1.00, r@1: 0.124, r@5: 0.281, r@10: 0.372, mr: 22\n",
      "epoch: 8\n",
      "Epoch 9, Epoch status: 0.13, Training loss: 0.0157\n",
      "Epoch 9, Epoch status: 0.25, Training loss: 0.0160\n",
      "Epoch 9, Epoch status: 0.38, Training loss: 0.0165\n",
      "Epoch 9, Epoch status: 0.51, Training loss: 0.0169\n",
      "Epoch 9, Epoch status: 0.63, Training loss: 0.0172\n",
      "Epoch 9, Epoch status: 0.76, Training loss: 0.0173\n",
      "Epoch 9, Epoch status: 0.89, Training loss: 0.0172\n",
      "evaluating epoch 9 ...\n",
      "MPII - epoch: 9, epoch status: 1.00, r@1: 0.122, r@5: 0.274, r@10: 0.369, mr: 22\n",
      "epoch: 9\n",
      "Epoch 10, Epoch status: 0.13, Training loss: 0.0142\n",
      "Epoch 10, Epoch status: 0.25, Training loss: 0.0145\n",
      "Epoch 10, Epoch status: 0.38, Training loss: 0.0148\n",
      "Epoch 10, Epoch status: 0.51, Training loss: 0.0153\n",
      "Epoch 10, Epoch status: 0.63, Training loss: 0.0159\n",
      "Epoch 10, Epoch status: 0.76, Training loss: 0.0159\n",
      "Epoch 10, Epoch status: 0.89, Training loss: 0.0160\n",
      "evaluating epoch 10 ...\n",
      "MPII - epoch: 10, epoch status: 1.00, r@1: 0.131, r@5: 0.288, r@10: 0.375, mr: 23\n",
      "epoch: 10\n",
      "Epoch 11, Epoch status: 0.13, Training loss: 0.0127\n",
      "Epoch 11, Epoch status: 0.25, Training loss: 0.0133\n",
      "Epoch 11, Epoch status: 0.38, Training loss: 0.0136\n",
      "Epoch 11, Epoch status: 0.51, Training loss: 0.0141\n",
      "Epoch 11, Epoch status: 0.63, Training loss: 0.0138\n",
      "Epoch 11, Epoch status: 0.76, Training loss: 0.0143\n",
      "Epoch 11, Epoch status: 0.89, Training loss: 0.0147\n",
      "evaluating epoch 11 ...\n",
      "MPII - epoch: 11, epoch status: 1.00, r@1: 0.113, r@5: 0.286, r@10: 0.387, mr: 22\n",
      "epoch: 11\n",
      "Epoch 12, Epoch status: 0.13, Training loss: 0.0118\n",
      "Epoch 12, Epoch status: 0.25, Training loss: 0.0119\n",
      "Epoch 12, Epoch status: 0.38, Training loss: 0.0127\n",
      "Epoch 12, Epoch status: 0.51, Training loss: 0.0126\n",
      "Epoch 12, Epoch status: 0.63, Training loss: 0.0127\n",
      "Epoch 12, Epoch status: 0.76, Training loss: 0.0132\n",
      "Epoch 12, Epoch status: 0.89, Training loss: 0.0134\n",
      "evaluating epoch 12 ...\n",
      "MPII - epoch: 12, epoch status: 1.00, r@1: 0.108, r@5: 0.280, r@10: 0.373, mr: 22\n",
      "epoch: 12\n",
      "Epoch 13, Epoch status: 0.13, Training loss: 0.0107\n",
      "Epoch 13, Epoch status: 0.25, Training loss: 0.0112\n",
      "Epoch 13, Epoch status: 0.38, Training loss: 0.0113\n",
      "Epoch 13, Epoch status: 0.51, Training loss: 0.0117\n",
      "Epoch 13, Epoch status: 0.63, Training loss: 0.0117\n",
      "Epoch 13, Epoch status: 0.76, Training loss: 0.0119\n",
      "Epoch 13, Epoch status: 0.89, Training loss: 0.0124\n",
      "evaluating epoch 13 ...\n",
      "MPII - epoch: 13, epoch status: 1.00, r@1: 0.115, r@5: 0.275, r@10: 0.388, mr: 21\n",
      "epoch: 13\n",
      "Epoch 14, Epoch status: 0.13, Training loss: 0.0099\n",
      "Epoch 14, Epoch status: 0.25, Training loss: 0.0101\n",
      "Epoch 14, Epoch status: 0.38, Training loss: 0.0106\n",
      "Epoch 14, Epoch status: 0.51, Training loss: 0.0108\n",
      "Epoch 14, Epoch status: 0.63, Training loss: 0.0108\n",
      "Epoch 14, Epoch status: 0.76, Training loss: 0.0111\n",
      "Epoch 14, Epoch status: 0.89, Training loss: 0.0113\n",
      "evaluating epoch 14 ...\n",
      "MPII - epoch: 14, epoch status: 1.00, r@1: 0.109, r@5: 0.285, r@10: 0.377, mr: 20\n",
      "epoch: 14\n",
      "Epoch 15, Epoch status: 0.13, Training loss: 0.0091\n",
      "Epoch 15, Epoch status: 0.25, Training loss: 0.0094\n",
      "Epoch 15, Epoch status: 0.38, Training loss: 0.0095\n",
      "Epoch 15, Epoch status: 0.51, Training loss: 0.0100\n",
      "Epoch 15, Epoch status: 0.63, Training loss: 0.0102\n",
      "Epoch 15, Epoch status: 0.76, Training loss: 0.0105\n",
      "Epoch 15, Epoch status: 0.89, Training loss: 0.0103\n",
      "evaluating epoch 15 ...\n",
      "MPII - epoch: 15, epoch status: 1.00, r@1: 0.112, r@5: 0.290, r@10: 0.386, mr: 22\n",
      "epoch: 15\n",
      "Epoch 16, Epoch status: 0.13, Training loss: 0.0084\n",
      "Epoch 16, Epoch status: 0.25, Training loss: 0.0087\n",
      "Epoch 16, Epoch status: 0.38, Training loss: 0.0088\n",
      "Epoch 16, Epoch status: 0.51, Training loss: 0.0092\n",
      "Epoch 16, Epoch status: 0.63, Training loss: 0.0093\n",
      "Epoch 16, Epoch status: 0.76, Training loss: 0.0094\n",
      "Epoch 16, Epoch status: 0.89, Training loss: 0.0095\n",
      "evaluating epoch 16 ...\n",
      "MPII - epoch: 16, epoch status: 1.00, r@1: 0.103, r@5: 0.282, r@10: 0.386, mr: 22\n",
      "epoch: 16\n",
      "Epoch 17, Epoch status: 0.13, Training loss: 0.0078\n",
      "Epoch 17, Epoch status: 0.25, Training loss: 0.0080\n",
      "Epoch 17, Epoch status: 0.38, Training loss: 0.0082\n",
      "Epoch 17, Epoch status: 0.51, Training loss: 0.0086\n",
      "Epoch 17, Epoch status: 0.63, Training loss: 0.0085\n",
      "Epoch 17, Epoch status: 0.76, Training loss: 0.0089\n",
      "Epoch 17, Epoch status: 0.89, Training loss: 0.0090\n",
      "evaluating epoch 17 ...\n",
      "MPII - epoch: 17, epoch status: 1.00, r@1: 0.113, r@5: 0.285, r@10: 0.381, mr: 23\n",
      "epoch: 17\n",
      "Epoch 18, Epoch status: 0.13, Training loss: 0.0073\n",
      "Epoch 18, Epoch status: 0.25, Training loss: 0.0076\n",
      "Epoch 18, Epoch status: 0.38, Training loss: 0.0077\n",
      "Epoch 18, Epoch status: 0.51, Training loss: 0.0078\n",
      "Epoch 18, Epoch status: 0.63, Training loss: 0.0079\n",
      "Epoch 18, Epoch status: 0.76, Training loss: 0.0081\n",
      "Epoch 18, Epoch status: 0.89, Training loss: 0.0084\n",
      "evaluating epoch 18 ...\n",
      "MPII - epoch: 18, epoch status: 1.00, r@1: 0.108, r@5: 0.290, r@10: 0.388, mr: 23\n",
      "epoch: 18\n",
      "Epoch 19, Epoch status: 0.13, Training loss: 0.0066\n",
      "Epoch 19, Epoch status: 0.25, Training loss: 0.0069\n",
      "Epoch 19, Epoch status: 0.38, Training loss: 0.0072\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19, Epoch status: 0.51, Training loss: 0.0073\n",
      "Epoch 19, Epoch status: 0.63, Training loss: 0.0075\n",
      "Epoch 19, Epoch status: 0.76, Training loss: 0.0076\n",
      "Epoch 19, Epoch status: 0.89, Training loss: 0.0079\n",
      "evaluating epoch 19 ...\n",
      "MPII - epoch: 19, epoch status: 1.00, r@1: 0.104, r@5: 0.288, r@10: 0.399, mr: 23\n",
      "epoch: 19\n",
      "Epoch 20, Epoch status: 0.13, Training loss: 0.0062\n",
      "Epoch 20, Epoch status: 0.25, Training loss: 0.0065\n",
      "Epoch 20, Epoch status: 0.38, Training loss: 0.0067\n",
      "Epoch 20, Epoch status: 0.51, Training loss: 0.0068\n",
      "Epoch 20, Epoch status: 0.63, Training loss: 0.0069\n",
      "Epoch 20, Epoch status: 0.76, Training loss: 0.0072\n",
      "Epoch 20, Epoch status: 0.89, Training loss: 0.0073\n",
      "evaluating epoch 20 ...\n",
      "MPII - epoch: 20, epoch status: 1.00, r@1: 0.104, r@5: 0.284, r@10: 0.384, mr: 22\n",
      "epoch: 20\n",
      "Epoch 21, Epoch status: 0.13, Training loss: 0.0059\n",
      "Epoch 21, Epoch status: 0.25, Training loss: 0.0061\n",
      "Epoch 21, Epoch status: 0.38, Training loss: 0.0062\n",
      "Epoch 21, Epoch status: 0.51, Training loss: 0.0063\n",
      "Epoch 21, Epoch status: 0.63, Training loss: 0.0065\n",
      "Epoch 21, Epoch status: 0.76, Training loss: 0.0068\n",
      "Epoch 21, Epoch status: 0.89, Training loss: 0.0067\n",
      "evaluating epoch 21 ...\n",
      "MPII - epoch: 21, epoch status: 1.00, r@1: 0.111, r@5: 0.282, r@10: 0.392, mr: 23\n",
      "epoch: 21\n",
      "Epoch 22, Epoch status: 0.13, Training loss: 0.0055\n",
      "Epoch 22, Epoch status: 0.25, Training loss: 0.0058\n",
      "Epoch 22, Epoch status: 0.38, Training loss: 0.0060\n",
      "Epoch 22, Epoch status: 0.51, Training loss: 0.0060\n",
      "Epoch 22, Epoch status: 0.63, Training loss: 0.0061\n",
      "Epoch 22, Epoch status: 0.76, Training loss: 0.0063\n",
      "Epoch 22, Epoch status: 0.89, Training loss: 0.0064\n",
      "evaluating epoch 22 ...\n",
      "MPII - epoch: 22, epoch status: 1.00, r@1: 0.103, r@5: 0.289, r@10: 0.391, mr: 22\n",
      "epoch: 22\n",
      "Epoch 23, Epoch status: 0.13, Training loss: 0.0053\n",
      "Epoch 23, Epoch status: 0.25, Training loss: 0.0054\n",
      "Epoch 23, Epoch status: 0.38, Training loss: 0.0056\n",
      "Epoch 23, Epoch status: 0.51, Training loss: 0.0057\n",
      "Epoch 23, Epoch status: 0.63, Training loss: 0.0058\n",
      "Epoch 23, Epoch status: 0.76, Training loss: 0.0059\n",
      "Epoch 23, Epoch status: 0.89, Training loss: 0.0060\n",
      "evaluating epoch 23 ...\n",
      "MPII - epoch: 23, epoch status: 1.00, r@1: 0.110, r@5: 0.279, r@10: 0.381, mr: 24\n",
      "epoch: 23\n",
      "Epoch 24, Epoch status: 0.13, Training loss: 0.0051\n",
      "Epoch 24, Epoch status: 0.25, Training loss: 0.0052\n",
      "Epoch 24, Epoch status: 0.38, Training loss: 0.0052\n",
      "Epoch 24, Epoch status: 0.51, Training loss: 0.0054\n",
      "Epoch 24, Epoch status: 0.63, Training loss: 0.0055\n",
      "Epoch 24, Epoch status: 0.76, Training loss: 0.0055\n",
      "Epoch 24, Epoch status: 0.89, Training loss: 0.0058\n",
      "evaluating epoch 24 ...\n",
      "MPII - epoch: 24, epoch status: 1.00, r@1: 0.103, r@5: 0.283, r@10: 0.383, mr: 23\n",
      "epoch: 24\n",
      "Epoch 25, Epoch status: 0.13, Training loss: 0.0047\n",
      "Epoch 25, Epoch status: 0.25, Training loss: 0.0048\n",
      "Epoch 25, Epoch status: 0.38, Training loss: 0.0049\n",
      "Epoch 25, Epoch status: 0.51, Training loss: 0.0051\n",
      "Epoch 25, Epoch status: 0.63, Training loss: 0.0052\n",
      "Epoch 25, Epoch status: 0.76, Training loss: 0.0053\n",
      "Epoch 25, Epoch status: 0.89, Training loss: 0.0054\n",
      "evaluating epoch 25 ...\n",
      "MPII - epoch: 25, epoch status: 1.00, r@1: 0.101, r@5: 0.279, r@10: 0.384, mr: 24\n",
      "epoch: 25\n",
      "Epoch 26, Epoch status: 0.13, Training loss: 0.0045\n",
      "Epoch 26, Epoch status: 0.25, Training loss: 0.0046\n",
      "Epoch 26, Epoch status: 0.38, Training loss: 0.0048\n",
      "Epoch 26, Epoch status: 0.51, Training loss: 0.0048\n",
      "Epoch 26, Epoch status: 0.63, Training loss: 0.0050\n",
      "Epoch 26, Epoch status: 0.76, Training loss: 0.0050\n",
      "Epoch 26, Epoch status: 0.89, Training loss: 0.0050\n",
      "evaluating epoch 26 ...\n",
      "MPII - epoch: 26, epoch status: 1.00, r@1: 0.102, r@5: 0.273, r@10: 0.374, mr: 24\n",
      "epoch: 26\n",
      "Epoch 27, Epoch status: 0.13, Training loss: 0.0043\n",
      "Epoch 27, Epoch status: 0.25, Training loss: 0.0045\n",
      "Epoch 27, Epoch status: 0.38, Training loss: 0.0045\n",
      "Epoch 27, Epoch status: 0.51, Training loss: 0.0046\n",
      "Epoch 27, Epoch status: 0.63, Training loss: 0.0046\n",
      "Epoch 27, Epoch status: 0.76, Training loss: 0.0048\n",
      "Epoch 27, Epoch status: 0.89, Training loss: 0.0049\n",
      "evaluating epoch 27 ...\n",
      "MPII - epoch: 27, epoch status: 1.00, r@1: 0.106, r@5: 0.274, r@10: 0.394, mr: 23\n",
      "epoch: 27\n",
      "Epoch 28, Epoch status: 0.13, Training loss: 0.0041\n",
      "Epoch 28, Epoch status: 0.25, Training loss: 0.0042\n",
      "Epoch 28, Epoch status: 0.38, Training loss: 0.0042\n",
      "Epoch 28, Epoch status: 0.51, Training loss: 0.0043\n",
      "Epoch 28, Epoch status: 0.63, Training loss: 0.0044\n",
      "Epoch 28, Epoch status: 0.76, Training loss: 0.0046\n",
      "Epoch 28, Epoch status: 0.89, Training loss: 0.0046\n",
      "evaluating epoch 28 ...\n",
      "MPII - epoch: 28, epoch status: 1.00, r@1: 0.102, r@5: 0.287, r@10: 0.388, mr: 24\n",
      "epoch: 28\n",
      "Epoch 29, Epoch status: 0.13, Training loss: 0.0040\n",
      "Epoch 29, Epoch status: 0.25, Training loss: 0.0040\n",
      "Epoch 29, Epoch status: 0.38, Training loss: 0.0041\n",
      "Epoch 29, Epoch status: 0.51, Training loss: 0.0042\n",
      "Epoch 29, Epoch status: 0.63, Training loss: 0.0043\n",
      "Epoch 29, Epoch status: 0.76, Training loss: 0.0043\n",
      "Epoch 29, Epoch status: 0.89, Training loss: 0.0044\n",
      "evaluating epoch 29 ...\n",
      "MPII - epoch: 29, epoch status: 1.00, r@1: 0.104, r@5: 0.276, r@10: 0.376, mr: 23\n",
      "epoch: 29\n",
      "Epoch 30, Epoch status: 0.13, Training loss: 0.0038\n",
      "Epoch 30, Epoch status: 0.25, Training loss: 0.0038\n",
      "Epoch 30, Epoch status: 0.38, Training loss: 0.0039\n",
      "Epoch 30, Epoch status: 0.51, Training loss: 0.0040\n",
      "Epoch 30, Epoch status: 0.63, Training loss: 0.0041\n",
      "Epoch 30, Epoch status: 0.76, Training loss: 0.0041\n",
      "Epoch 30, Epoch status: 0.89, Training loss: 0.0042\n",
      "evaluating epoch 30 ...\n",
      "MPII - epoch: 30, epoch status: 1.00, r@1: 0.100, r@5: 0.269, r@10: 0.385, mr: 23\n",
      "epoch: 30\n",
      "Epoch 31, Epoch status: 0.13, Training loss: 0.0036\n",
      "Epoch 31, Epoch status: 0.25, Training loss: 0.0037\n",
      "Epoch 31, Epoch status: 0.38, Training loss: 0.0037\n",
      "Epoch 31, Epoch status: 0.51, Training loss: 0.0038\n",
      "Epoch 31, Epoch status: 0.63, Training loss: 0.0039\n",
      "Epoch 31, Epoch status: 0.76, Training loss: 0.0039\n",
      "Epoch 31, Epoch status: 0.89, Training loss: 0.0041\n",
      "evaluating epoch 31 ...\n",
      "MPII - epoch: 31, epoch status: 1.00, r@1: 0.099, r@5: 0.278, r@10: 0.388, mr: 25\n",
      "epoch: 31\n",
      "Epoch 32, Epoch status: 0.13, Training loss: 0.0035\n",
      "Epoch 32, Epoch status: 0.25, Training loss: 0.0035\n",
      "Epoch 32, Epoch status: 0.38, Training loss: 0.0037\n",
      "Epoch 32, Epoch status: 0.51, Training loss: 0.0037\n",
      "Epoch 32, Epoch status: 0.63, Training loss: 0.0038\n",
      "Epoch 32, Epoch status: 0.76, Training loss: 0.0038\n",
      "Epoch 32, Epoch status: 0.89, Training loss: 0.0038\n",
      "evaluating epoch 32 ...\n",
      "MPII - epoch: 32, epoch status: 1.00, r@1: 0.103, r@5: 0.273, r@10: 0.374, mr: 25\n",
      "epoch: 32\n",
      "Epoch 33, Epoch status: 0.13, Training loss: 0.0034\n",
      "Epoch 33, Epoch status: 0.25, Training loss: 0.0035\n",
      "Epoch 33, Epoch status: 0.38, Training loss: 0.0035\n",
      "Epoch 33, Epoch status: 0.51, Training loss: 0.0035\n",
      "Epoch 33, Epoch status: 0.63, Training loss: 0.0036\n",
      "Epoch 33, Epoch status: 0.76, Training loss: 0.0037\n",
      "Epoch 33, Epoch status: 0.89, Training loss: 0.0037\n",
      "evaluating epoch 33 ...\n",
      "MPII - epoch: 33, epoch status: 1.00, r@1: 0.101, r@5: 0.270, r@10: 0.371, mr: 27\n",
      "epoch: 33\n",
      "Epoch 34, Epoch status: 0.13, Training loss: 0.0033\n",
      "Epoch 34, Epoch status: 0.25, Training loss: 0.0033\n",
      "Epoch 34, Epoch status: 0.38, Training loss: 0.0034\n",
      "Epoch 34, Epoch status: 0.51, Training loss: 0.0034\n",
      "Epoch 34, Epoch status: 0.63, Training loss: 0.0035\n",
      "Epoch 34, Epoch status: 0.76, Training loss: 0.0035\n",
      "Epoch 34, Epoch status: 0.89, Training loss: 0.0036\n",
      "evaluating epoch 34 ...\n",
      "MPII - epoch: 34, epoch status: 1.00, r@1: 0.100, r@5: 0.270, r@10: 0.382, mr: 26\n",
      "epoch: 34\n",
      "Epoch 35, Epoch status: 0.13, Training loss: 0.0032\n",
      "Epoch 35, Epoch status: 0.25, Training loss: 0.0032\n",
      "Epoch 35, Epoch status: 0.38, Training loss: 0.0032\n",
      "Epoch 35, Epoch status: 0.51, Training loss: 0.0033\n",
      "Epoch 35, Epoch status: 0.63, Training loss: 0.0034\n",
      "Epoch 35, Epoch status: 0.76, Training loss: 0.0034\n",
      "Epoch 35, Epoch status: 0.89, Training loss: 0.0035\n",
      "evaluating epoch 35 ...\n",
      "MPII - epoch: 35, epoch status: 1.00, r@1: 0.091, r@5: 0.275, r@10: 0.370, mr: 26\n",
      "epoch: 35\n",
      "Epoch 36, Epoch status: 0.13, Training loss: 0.0031\n",
      "Epoch 36, Epoch status: 0.25, Training loss: 0.0031\n",
      "Epoch 36, Epoch status: 0.38, Training loss: 0.0032\n",
      "Epoch 36, Epoch status: 0.51, Training loss: 0.0032\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36, Epoch status: 0.63, Training loss: 0.0032\n",
      "Epoch 36, Epoch status: 0.76, Training loss: 0.0033\n",
      "Epoch 36, Epoch status: 0.89, Training loss: 0.0034\n",
      "evaluating epoch 36 ...\n",
      "MPII - epoch: 36, epoch status: 1.00, r@1: 0.101, r@5: 0.270, r@10: 0.369, mr: 25\n",
      "epoch: 36\n",
      "Epoch 37, Epoch status: 0.13, Training loss: 0.0030\n",
      "Epoch 37, Epoch status: 0.25, Training loss: 0.0030\n",
      "Epoch 37, Epoch status: 0.38, Training loss: 0.0031\n",
      "Epoch 37, Epoch status: 0.51, Training loss: 0.0031\n",
      "Epoch 37, Epoch status: 0.63, Training loss: 0.0032\n",
      "Epoch 37, Epoch status: 0.76, Training loss: 0.0032\n",
      "Epoch 37, Epoch status: 0.89, Training loss: 0.0033\n",
      "evaluating epoch 37 ...\n",
      "MPII - epoch: 37, epoch status: 1.00, r@1: 0.096, r@5: 0.266, r@10: 0.364, mr: 25\n",
      "epoch: 37\n",
      "Epoch 38, Epoch status: 0.13, Training loss: 0.0029\n",
      "Epoch 38, Epoch status: 0.25, Training loss: 0.0030\n",
      "Epoch 38, Epoch status: 0.38, Training loss: 0.0030\n",
      "Epoch 38, Epoch status: 0.51, Training loss: 0.0030\n",
      "Epoch 38, Epoch status: 0.63, Training loss: 0.0031\n",
      "Epoch 38, Epoch status: 0.76, Training loss: 0.0031\n",
      "Epoch 38, Epoch status: 0.89, Training loss: 0.0032\n",
      "evaluating epoch 38 ...\n",
      "MPII - epoch: 38, epoch status: 1.00, r@1: 0.103, r@5: 0.263, r@10: 0.376, mr: 25\n",
      "epoch: 38\n",
      "Epoch 39, Epoch status: 0.13, Training loss: 0.0028\n",
      "Epoch 39, Epoch status: 0.25, Training loss: 0.0029\n",
      "Epoch 39, Epoch status: 0.38, Training loss: 0.0029\n",
      "Epoch 39, Epoch status: 0.51, Training loss: 0.0030\n",
      "Epoch 39, Epoch status: 0.63, Training loss: 0.0030\n",
      "Epoch 39, Epoch status: 0.76, Training loss: 0.0030\n",
      "Epoch 39, Epoch status: 0.89, Training loss: 0.0030\n",
      "evaluating epoch 39 ...\n",
      "MPII - epoch: 39, epoch status: 1.00, r@1: 0.097, r@5: 0.260, r@10: 0.361, mr: 28\n",
      "epoch: 39\n",
      "Epoch 40, Epoch status: 0.13, Training loss: 0.0028\n",
      "Epoch 40, Epoch status: 0.25, Training loss: 0.0028\n",
      "Epoch 40, Epoch status: 0.38, Training loss: 0.0028\n",
      "Epoch 40, Epoch status: 0.51, Training loss: 0.0029\n",
      "Epoch 40, Epoch status: 0.63, Training loss: 0.0030\n",
      "Epoch 40, Epoch status: 0.76, Training loss: 0.0029\n",
      "Epoch 40, Epoch status: 0.89, Training loss: 0.0030\n",
      "evaluating epoch 40 ...\n",
      "MPII - epoch: 40, epoch status: 1.00, r@1: 0.096, r@5: 0.266, r@10: 0.361, mr: 27\n",
      "epoch: 40\n",
      "Epoch 41, Epoch status: 0.13, Training loss: 0.0027\n",
      "Epoch 41, Epoch status: 0.25, Training loss: 0.0028\n",
      "Epoch 41, Epoch status: 0.38, Training loss: 0.0028\n",
      "Epoch 41, Epoch status: 0.51, Training loss: 0.0028\n",
      "Epoch 41, Epoch status: 0.63, Training loss: 0.0028\n",
      "Epoch 41, Epoch status: 0.76, Training loss: 0.0029\n",
      "Epoch 41, Epoch status: 0.89, Training loss: 0.0029\n",
      "evaluating epoch 41 ...\n",
      "MPII - epoch: 41, epoch status: 1.00, r@1: 0.095, r@5: 0.261, r@10: 0.361, mr: 27\n",
      "epoch: 41\n",
      "Epoch 42, Epoch status: 0.13, Training loss: 0.0027\n",
      "Epoch 42, Epoch status: 0.25, Training loss: 0.0027\n",
      "Epoch 42, Epoch status: 0.38, Training loss: 0.0027\n",
      "Epoch 42, Epoch status: 0.51, Training loss: 0.0028\n",
      "Epoch 42, Epoch status: 0.63, Training loss: 0.0028\n",
      "Epoch 42, Epoch status: 0.76, Training loss: 0.0028\n",
      "Epoch 42, Epoch status: 0.89, Training loss: 0.0028\n",
      "evaluating epoch 42 ...\n",
      "MPII - epoch: 42, epoch status: 1.00, r@1: 0.094, r@5: 0.262, r@10: 0.362, mr: 28\n",
      "epoch: 42\n",
      "Epoch 43, Epoch status: 0.13, Training loss: 0.0027\n",
      "Epoch 43, Epoch status: 0.25, Training loss: 0.0027\n",
      "Epoch 43, Epoch status: 0.38, Training loss: 0.0027\n",
      "Epoch 43, Epoch status: 0.51, Training loss: 0.0027\n",
      "Epoch 43, Epoch status: 0.63, Training loss: 0.0027\n",
      "Epoch 43, Epoch status: 0.76, Training loss: 0.0027\n",
      "Epoch 43, Epoch status: 0.89, Training loss: 0.0028\n",
      "evaluating epoch 43 ...\n",
      "MPII - epoch: 43, epoch status: 1.00, r@1: 0.103, r@5: 0.257, r@10: 0.365, mr: 27\n",
      "epoch: 43\n",
      "Epoch 44, Epoch status: 0.13, Training loss: 0.0026\n",
      "Epoch 44, Epoch status: 0.25, Training loss: 0.0026\n",
      "Epoch 44, Epoch status: 0.38, Training loss: 0.0026\n",
      "Epoch 44, Epoch status: 0.51, Training loss: 0.0026\n",
      "Epoch 44, Epoch status: 0.63, Training loss: 0.0027\n",
      "Epoch 44, Epoch status: 0.76, Training loss: 0.0027\n",
      "Epoch 44, Epoch status: 0.89, Training loss: 0.0027\n",
      "evaluating epoch 44 ...\n",
      "MPII - epoch: 44, epoch status: 1.00, r@1: 0.095, r@5: 0.265, r@10: 0.367, mr: 27\n",
      "epoch: 44\n",
      "Epoch 45, Epoch status: 0.13, Training loss: 0.0025\n",
      "Epoch 45, Epoch status: 0.25, Training loss: 0.0026\n",
      "Epoch 45, Epoch status: 0.38, Training loss: 0.0026\n",
      "Epoch 45, Epoch status: 0.51, Training loss: 0.0026\n",
      "Epoch 45, Epoch status: 0.63, Training loss: 0.0026\n",
      "Epoch 45, Epoch status: 0.76, Training loss: 0.0027\n",
      "Epoch 45, Epoch status: 0.89, Training loss: 0.0027\n",
      "evaluating epoch 45 ...\n",
      "MPII - epoch: 45, epoch status: 1.00, r@1: 0.096, r@5: 0.266, r@10: 0.363, mr: 28\n",
      "epoch: 45\n",
      "Epoch 46, Epoch status: 0.13, Training loss: 0.0025\n",
      "Epoch 46, Epoch status: 0.25, Training loss: 0.0025\n",
      "Epoch 46, Epoch status: 0.38, Training loss: 0.0025\n",
      "Epoch 46, Epoch status: 0.51, Training loss: 0.0025\n",
      "Epoch 46, Epoch status: 0.63, Training loss: 0.0026\n",
      "Epoch 46, Epoch status: 0.76, Training loss: 0.0026\n",
      "Epoch 46, Epoch status: 0.89, Training loss: 0.0027\n",
      "evaluating epoch 46 ...\n",
      "MPII - epoch: 46, epoch status: 1.00, r@1: 0.096, r@5: 0.257, r@10: 0.362, mr: 27\n",
      "epoch: 46\n",
      "Epoch 47, Epoch status: 0.13, Training loss: 0.0025\n",
      "Epoch 47, Epoch status: 0.25, Training loss: 0.0025\n",
      "Epoch 47, Epoch status: 0.38, Training loss: 0.0025\n",
      "Epoch 47, Epoch status: 0.51, Training loss: 0.0025\n",
      "Epoch 47, Epoch status: 0.63, Training loss: 0.0026\n",
      "Epoch 47, Epoch status: 0.76, Training loss: 0.0026\n",
      "Epoch 47, Epoch status: 0.89, Training loss: 0.0026\n",
      "evaluating epoch 47 ...\n",
      "MPII - epoch: 47, epoch status: 1.00, r@1: 0.096, r@5: 0.261, r@10: 0.354, mr: 29\n",
      "epoch: 47\n",
      "Epoch 48, Epoch status: 0.13, Training loss: 0.0024\n",
      "Epoch 48, Epoch status: 0.25, Training loss: 0.0024\n",
      "Epoch 48, Epoch status: 0.38, Training loss: 0.0025\n",
      "Epoch 48, Epoch status: 0.51, Training loss: 0.0025\n",
      "Epoch 48, Epoch status: 0.63, Training loss: 0.0025\n",
      "Epoch 48, Epoch status: 0.76, Training loss: 0.0025\n",
      "Epoch 48, Epoch status: 0.89, Training loss: 0.0025\n",
      "evaluating epoch 48 ...\n",
      "MPII - epoch: 48, epoch status: 1.00, r@1: 0.096, r@5: 0.267, r@10: 0.357, mr: 28\n",
      "epoch: 48\n",
      "Epoch 49, Epoch status: 0.13, Training loss: 0.0024\n",
      "Epoch 49, Epoch status: 0.25, Training loss: 0.0024\n",
      "Epoch 49, Epoch status: 0.38, Training loss: 0.0024\n",
      "Epoch 49, Epoch status: 0.51, Training loss: 0.0025\n",
      "Epoch 49, Epoch status: 0.63, Training loss: 0.0025\n",
      "Epoch 49, Epoch status: 0.76, Training loss: 0.0025\n",
      "Epoch 49, Epoch status: 0.89, Training loss: 0.0025\n",
      "evaluating epoch 49 ...\n",
      "MPII - epoch: 49, epoch status: 1.00, r@1: 0.094, r@5: 0.262, r@10: 0.355, mr: 27\n",
      "epoch: 49\n",
      "Epoch 50, Epoch status: 0.13, Training loss: 0.0024\n",
      "Epoch 50, Epoch status: 0.25, Training loss: 0.0024\n",
      "Epoch 50, Epoch status: 0.38, Training loss: 0.0024\n",
      "Epoch 50, Epoch status: 0.51, Training loss: 0.0024\n",
      "Epoch 50, Epoch status: 0.63, Training loss: 0.0024\n",
      "Epoch 50, Epoch status: 0.76, Training loss: 0.0024\n",
      "Epoch 50, Epoch status: 0.89, Training loss: 0.0025\n",
      "evaluating epoch 50 ...\n",
      "MPII - epoch: 50, epoch status: 1.00, r@1: 0.092, r@5: 0.257, r@10: 0.351, mr: 30\n"
     ]
    }
   ],
   "source": [
    "%run train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as th"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "moe_weights=np.zeros((128,4))+0.25\n",
    "moe_weights = th.from_numpy(aa).float().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 4])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch]",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

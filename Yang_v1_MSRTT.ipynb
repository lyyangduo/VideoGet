{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as th\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import LSMDC as LD2\n",
    "import MSRVTT as MSR\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "import argparse\n",
    "#from loss import MaxMarginRankingLoss\n",
    "from loss import MaxMarginRankingLoss_Distance  #Yang changed\n",
    "from model_yang import Net_Fuse\n",
    "from torch.autograd import Variable\n",
    "import os\n",
    "import random\n",
    "from qcm_sampler import QCMSampler\n",
    "from MSR_sampler import MSRSampler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(GPU=True, MSRVTT=True, batch_size=64, coco=False, coco_sampling_rate=1.0, epochs=50, eval_qcm=False, lr=0.0004, lr_decay=0.95, margin=0.2, model_name='test', momentum=0.9, n_cpu=1, n_display=100, optimizer='adam', seed=1, text_cluster_size=32)\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser(description='LSMDC2017')\n",
    "\n",
    "parser.add_argument('--coco', type=bool, default=False,\n",
    "                            help='add coco dataset')\n",
    "\n",
    "parser.add_argument('--lr', type=float, default=0.0004,\n",
    "                            help='initial learning rate')  #Yang changed and search a number\n",
    "parser.add_argument('--epochs', type=int, default=50,\n",
    "                            help='upper epoch limit')\n",
    "parser.add_argument('--batch_size', type=int, default=64,\n",
    "                            help='batch size')\n",
    "parser.add_argument('--text_cluster_size', type=int, default=32,\n",
    "                            help='Text cluster size')\n",
    "parser.add_argument('--margin', type=float, default=0.2,\n",
    "                            help='MaxMargin margin value')\n",
    "parser.add_argument('--lr_decay', type=float, default=0.95,\n",
    "                            help='Learning rate exp epoch decay')\n",
    "parser.add_argument('--n_display', type=int, default=100,\n",
    "                            help='Information display frequence')\n",
    "parser.add_argument('--GPU', type=bool, default=True,\n",
    "                            help='Use of GPU')\n",
    "parser.add_argument('--n_cpu', type=int, default=1,\n",
    "                            help='Number of CPU')\n",
    "\n",
    "parser.add_argument('--model_name', type=str, default='test',\n",
    "                            help='Model name')\n",
    "parser.add_argument('--seed', type=int, default=1,\n",
    "                            help='Initial Random Seed')\n",
    "\n",
    "parser.add_argument('--optimizer', type=str, default='adam',\n",
    "                            help='optimizer')\n",
    "parser.add_argument('--momentum', type=float, default=0.9,\n",
    "                            help='Nesterov Momentum for SGD')\n",
    "\n",
    "\n",
    "parser.add_argument('--eval_qcm', type=bool, default=False,\n",
    "                            help='Eval or not QCM')\n",
    "\n",
    "parser.add_argument('--MSRVTT', type=bool, default=True,\n",
    "                            help='MSRVTT')\n",
    "\n",
    "parser.add_argument('--coco_sampling_rate', type=float, default=1.0,\n",
    "                            help='coco sampling rate')\n",
    "\n",
    "\n",
    "#args = parser.parse_args()\n",
    "args, unknown =  parser.parse_known_args()\n",
    "\n",
    "print (args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_feat = 'data'\n",
    "\n",
    "mp_visual_path = os.path.join(root_feat,'X_resnet.npy')\n",
    "mp_flow_path = os.path.join(root_feat,'X_flow.npy')\n",
    "mp_face_path = os.path.join(root_feat,'X_face.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predefining random initial seeds\n",
    "th.manual_seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "random.seed(args.seed)\n",
    "\n",
    "if args.eval_qcm and not(args.MSRVTT):\n",
    "    print ('sha')\n",
    "    qcm_dataset = LD2.LSMDC_qcm(os.path.join(root_feat,'resnet-qcm.npy'),\n",
    "            os.path.join(root_feat,'w2v_LSMDC_qcm.npy'), os.path.join(root_feat,'X_audio_test.npy'),\n",
    "            os.path.join(root_feat,'flow-qcm.npy'),\n",
    "            os.path.join(root_feat,'face-qcm.npy')) \n",
    "    \n",
    "    qcm_sampler = QCMSampler(len(qcm_dataset))\n",
    "    qcm_dataloader = DataLoader(qcm_dataset, batch_size=500, sampler=qcm_sampler, num_workers=1)\n",
    "    qcm_gt_fn = os.path.join(root_feat,'multiple_choice_gt.txt')\n",
    "    qcm_gt = [line.rstrip('\\n') for line in open(qcm_gt_fn)]\n",
    "    qcm_gt = np.array(map(int,qcm_gt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_tensor(l, max_len):\n",
    "    tensor = np.zeros((len(l),max_len,l[0].shape[-1]))\n",
    "    for i in range(len(l)):\n",
    "        if len(l[i]):\n",
    "            tensor[i,:min(max_len,l[i].shape[0]),:] = l[i][:min(max_len,l[i].shape[0])]\n",
    "\n",
    "    return th.from_numpy(tensor).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verbose(epoch, status, metrics, name='TEST'):\n",
    "    print(name+' - epoch: %d, epoch status: %.2f, r@1: %.3f, r@5: %.3f, r@10: %.3f, mr: %d' % \n",
    "            (epoch + 1, status, \n",
    "                metrics['R1'], metrics['R5'], metrics['R10'],\n",
    "                metrics['MR']))\n",
    "\n",
    "\n",
    "def compute_metric(x):\n",
    "    sx = np.sort(-x, axis=1)\n",
    "    d = np.diag(-x)\n",
    "    d = d[:,np.newaxis]\n",
    "    ind = sx - d\n",
    "    ind = np.where(ind == 0)\n",
    "    ind = ind[1]\n",
    "\n",
    "    metrics = {}\n",
    "    metrics['R1'] = float(np.sum(ind == 0))/len(ind)\n",
    "    metrics['R5'] = float(np.sum(ind < 5))/len(ind)\n",
    "    metrics['R10'] = float(np.sum(ind < 10))/len(ind)\n",
    "    metrics['MR'] = np.median(ind) + 1\n",
    "\n",
    "    return metrics\n",
    "\n",
    "def compute_metric_yang(x):\n",
    "    sx = np.sort(x, axis=1)\n",
    "    d = np.diag(x) #(1000)\n",
    "    d = d[:,np.newaxis] #(1000,1)\n",
    "    ind = sx - d\n",
    "    ind = np.where(ind == 0)\n",
    "    ind = ind[1]\n",
    "\n",
    "    metrics = {}\n",
    "    metrics['R1'] = float(np.sum(ind == 0))/len(ind)  # for 1000 sample, howmany correct one is at the first\n",
    "    metrics['R5'] = float(np.sum(ind < 5))/len(ind)   # for 1000 sample, howmany correct one is among the first five\n",
    "    metrics['R10'] = float(np.sum(ind < 10))/len(ind) \n",
    "    metrics['MR'] = np.median(ind) + 1\n",
    "\n",
    "    return metrics\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-loading features ... This may takes several minutes ...\n",
      "MSRVTT\n",
      "loading data ...\n",
      "done\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "print ('Pre-loading features ... This may takes several minutes ...')\n",
    "\n",
    "if args.MSRVTT:\n",
    "    print ('MSRVTT')\n",
    "    visual_feat_path = os.path.join(root_feat,'resnet_features.pickle')   ##7656 training samples in total\n",
    "    flow_feat_path = os.path.join(root_feat,'flow_features.pickle')\n",
    "    text_feat_path = os.path.join(root_feat,'w2v_MSRVTT.pickle')\n",
    "    audio_feat_path = os.path.join(root_feat,'audio_features.pickle')\n",
    "    face_feat_path = os.path.join(root_feat,'face_features.pickle')\n",
    "    train_list_path = os.path.join(root_feat,'train_list.txt')\n",
    "    test_list_path = os.path.join(root_feat,'test_list.txt')\n",
    "\n",
    "    dataset = MSR.MSRVTT(visual_feat_path, flow_feat_path, text_feat_path,\n",
    "            audio_feat_path, face_feat_path, train_list_path,test_list_path, coco=args.coco) \n",
    "    msr_sampler = MSRSampler(dataset.n_MSR, dataset.n_coco, args.coco_sampling_rate)\n",
    "    \n",
    "    if args.coco:\n",
    "        dataloader = DataLoader(dataset, batch_size=args.batch_size,\n",
    "                sampler=msr_sampler, num_workers=1,collate_fn=dataset.collate_data, drop_last=True)\n",
    "    else:\n",
    "        dataloader = DataLoader(dataset, batch_size=args.batch_size,\n",
    "                shuffle=True, num_workers=1,collate_fn=dataset.collate_data, drop_last=True)\n",
    "\n",
    "else:\n",
    "    path_to_text = os.path.join(root_feat,'w2v_LSMDC.npy')\n",
    "    path_to_audio = os.path.join(root_feat,'X_audio_train.npy')\n",
    "    path_to_coco_visual_path= os.path.join(root_feat,'X_train2014_resnet152.npy') #Yang add\n",
    "    path_to_coco_text_path=os.path.join(root_feat,'w2v_coco_train2014_1.npy')#Yang add\n",
    "    \n",
    "\n",
    "    #dataset = LD2.LSMDC(mp_visual_path, path_to_text,\n",
    "    #        path_to_audio, mp_flow_path, mp_face_path, coco=args.coco) \n",
    "    #coco=args.coco\n",
    "    dataset = LD2.LSMDC(mp_visual_path, path_to_text,\n",
    "            path_to_audio, mp_flow_path, mp_face_path,path_to_coco_visual_path,path_to_coco_text_path, args.coco) \n",
    "    dataloader = DataLoader(dataset, batch_size=args.batch_size,\n",
    "            shuffle=True, num_workers=1, drop_last=True)\n",
    "    print ('Done.')\n",
    "\n",
    "    print ('Reading test data ...')\n",
    "    resnet_features_path = os.path.join(root_feat,'resnet152-retrieval.npy.tensor.npy')\n",
    "    flow_features_path = os.path.join(root_feat,'flow-retrieval.npy.tensor.npy')\n",
    "    face_features_path = os.path.join(root_feat,'face-retrieval.npy.tensor.npy')\n",
    "    text_features_path = os.path.join(root_feat,'w2v_LSMDC_retrieval.npy')\n",
    "    audio_features_path = os.path.join(root_feat,'X_audio_retrieval.npy.tensor.npy')\n",
    "\n",
    "    vid_retrieval = np.load(resnet_features_path, encoding='latin1') # torch size [1000,2048]\n",
    "    flow_retrieval = np.load(flow_features_path, encoding='latin1') # torch size [1000,1024]\n",
    "    face_retrieval = np.load(face_features_path, encoding='latin1')# torch size [1000,128]\n",
    "    text_retrieval = np.load(text_features_path, encoding='latin1')# torch Size([1000, 29, 300]) \n",
    "                                                                   # left -->right, maximum word length 29, \n",
    "                                                                   # if not enough, fill all zeros\n",
    "    audio_retrieval = np.load(audio_features_path, encoding='latin1') #torch.Size([1000, 30, 128])\n",
    "\n",
    "    mm = max(map(len,text_retrieval)) # map(func, iterable)  =29\n",
    "\n",
    "    text_retrieval = make_tensor(text_retrieval,mm)\n",
    "\n",
    "    vid_retrieval = th.from_numpy(vid_retrieval).float() # torch size [1000,2048]\n",
    "    flow_retrieval = th.from_numpy(flow_retrieval).float() # torch size [1000,1024]\n",
    "    face_retrieval = th.from_numpy(face_retrieval).float() # torch size [1000,128]\n",
    "    audio_retrieval = th.from_numpy(audio_retrieval).float() #torch.Size([1000, 30, 128])\n",
    "\n",
    "    text_retrieval_val = text_retrieval\n",
    "    vid_retrieval_val = vid_retrieval\n",
    "    flow_retrieval_val = flow_retrieval\n",
    "    face_retrieval_val = face_retrieval\n",
    "    audio_retrieval_val = audio_retrieval\n",
    "\n",
    "\n",
    "    face_ind_test = np.load(os.path.join(root_feat,'no_face_ind_retrieval.npy'))\n",
    "    face_ind_test = 1 - face_ind_test # for no face data, all descriptors==0\n",
    "print ('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6656"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset.coco_ind) #7656 training samples in total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7656"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset.face_features.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7656"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset.audio_features.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "82782"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(np.where(dataset.coco_ind==1)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2048\n",
      "9600\n"
     ]
    }
   ],
   "source": [
    "# Model\n",
    "#video_modality_dim = {'face': (128,128), 'audio': (128*16,128),\n",
    "#'visual': (2048,2048), 'motion': (1024,1024)}\n",
    "\n",
    "video_modality_dim_fuse = {'face': (128,128,256), 'audio': (128*16,128,256),\n",
    "'visual': (2048,2048,256), 'motion': (1024,1024,256)}\n",
    "\n",
    "text_dim={'text':(300,128,256)}\n",
    "net = Net_Fuse(video_modality_dim_fuse,text_dim,\n",
    "        audio_cluster=16,text_cluster=args.text_cluster_size)\n",
    "#net.train()\n",
    "\n",
    "if args.GPU:\n",
    "    net.cuda()\n",
    "\n",
    "# Optimizers + Loss\n",
    "#max_margin = MaxMarginRankingLoss(margin=args.margin) \n",
    "max_margin = MaxMarginRankingLoss_Distance(margin=args.margin)  #Yang changed\n",
    "\n",
    "\n",
    "if args.optimizer == 'adam':\n",
    "    optimizer = optim.Adam(net.parameters(), lr=args.lr)\n",
    "elif args.optimizer == 'sgd':\n",
    "    optimizer = optim.SGD(net.parameters(), lr=args.lr, momentum=args.momentum)\n",
    "\n",
    "if args.GPU:\n",
    "    max_margin.cuda()\n",
    "\n",
    "n_display = args.n_display\n",
    "dataset_size = len(dataset)\n",
    "lr_decay = args.lr_decay\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_display=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training loop ...\n",
      "epoch: 0\n",
      "True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/yangl/external/anaconda3/envs/pytorch/lib/python3.7/site-packages/ipykernel/__main__.py:53: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Epoch status: 0.95, Training loss: 0.0827\n",
      "evaluating epoch 1 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/yangl/external/anaconda3/envs/pytorch/lib/python3.7/site-packages/ipykernel/__main__.py:70: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "/users/yangl/external/anaconda3/envs/pytorch/lib/python3.7/site-packages/ipykernel/__main__.py:71: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "/users/yangl/external/anaconda3/envs/pytorch/lib/python3.7/site-packages/ipykernel/__main__.py:72: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "/users/yangl/external/anaconda3/envs/pytorch/lib/python3.7/site-packages/ipykernel/__main__.py:73: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "/users/yangl/external/anaconda3/envs/pytorch/lib/python3.7/site-packages/ipykernel/__main__.py:74: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSRVTT - epoch: 1, epoch status: 0.99, r@1: 0.012, r@5: 0.066, r@10: 0.128, mr: 74\n",
      "epoch: 1\n",
      "True\n",
      "Epoch 2, Epoch status: 0.95, Training loss: 0.0456\n",
      "evaluating epoch 2 ...\n",
      "MSRVTT - epoch: 2, epoch status: 0.99, r@1: 0.022, r@5: 0.116, r@10: 0.201, mr: 45\n",
      "epoch: 2\n",
      "True\n",
      "Epoch 3, Epoch status: 0.95, Training loss: 0.0374\n",
      "evaluating epoch 3 ...\n",
      "MSRVTT - epoch: 3, epoch status: 0.99, r@1: 0.043, r@5: 0.168, r@10: 0.255, mr: 29\n",
      "epoch: 3\n",
      "True\n",
      "Epoch 4, Epoch status: 0.95, Training loss: 0.0265\n",
      "evaluating epoch 4 ...\n",
      "MSRVTT - epoch: 4, epoch status: 0.99, r@1: 0.071, r@5: 0.260, r@10: 0.408, mr: 15\n",
      "epoch: 4\n",
      "True\n",
      "Epoch 5, Epoch status: 0.95, Training loss: 0.0172\n",
      "evaluating epoch 5 ...\n",
      "MSRVTT - epoch: 5, epoch status: 0.99, r@1: 0.126, r@5: 0.392, r@10: 0.540, mr: 9\n",
      "epoch: 5\n",
      "True\n",
      "Epoch 6, Epoch status: 0.95, Training loss: 0.0132\n",
      "evaluating epoch 6 ...\n",
      "MSRVTT - epoch: 6, epoch status: 0.99, r@1: 0.141, r@5: 0.407, r@10: 0.575, mr: 8\n",
      "epoch: 6\n",
      "True\n",
      "Epoch 7, Epoch status: 0.95, Training loss: 0.0122\n",
      "evaluating epoch 7 ...\n",
      "MSRVTT - epoch: 7, epoch status: 0.99, r@1: 0.136, r@5: 0.456, r@10: 0.616, mr: 7\n",
      "epoch: 7\n",
      "True\n",
      "Epoch 8, Epoch status: 0.95, Training loss: 0.0113\n",
      "evaluating epoch 8 ...\n",
      "MSRVTT - epoch: 8, epoch status: 0.99, r@1: 0.162, r@5: 0.510, r@10: 0.664, mr: 5\n",
      "epoch: 8\n",
      "True\n",
      "Epoch 9, Epoch status: 0.95, Training loss: 0.0105\n",
      "evaluating epoch 9 ...\n",
      "MSRVTT - epoch: 9, epoch status: 0.99, r@1: 0.172, r@5: 0.511, r@10: 0.671, mr: 5\n",
      "epoch: 9\n",
      "True\n",
      "Epoch 10, Epoch status: 0.95, Training loss: 0.0099\n",
      "evaluating epoch 10 ...\n",
      "MSRVTT - epoch: 10, epoch status: 0.99, r@1: 0.163, r@5: 0.503, r@10: 0.656, mr: 5\n",
      "epoch: 10\n",
      "True\n",
      "Epoch 11, Epoch status: 0.95, Training loss: 0.0098\n",
      "evaluating epoch 11 ...\n",
      "MSRVTT - epoch: 11, epoch status: 0.99, r@1: 0.176, r@5: 0.525, r@10: 0.680, mr: 5\n",
      "epoch: 11\n",
      "True\n",
      "Epoch 12, Epoch status: 0.95, Training loss: 0.0093\n",
      "evaluating epoch 12 ...\n",
      "MSRVTT - epoch: 12, epoch status: 0.99, r@1: 0.173, r@5: 0.525, r@10: 0.690, mr: 5\n",
      "epoch: 12\n",
      "True\n",
      "Epoch 13, Epoch status: 0.95, Training loss: 0.0090\n",
      "evaluating epoch 13 ...\n",
      "MSRVTT - epoch: 13, epoch status: 0.99, r@1: 0.184, r@5: 0.558, r@10: 0.707, mr: 4\n",
      "epoch: 13\n",
      "True\n",
      "Epoch 14, Epoch status: 0.95, Training loss: 0.0090\n",
      "evaluating epoch 14 ...\n",
      "MSRVTT - epoch: 14, epoch status: 0.99, r@1: 0.201, r@5: 0.554, r@10: 0.709, mr: 5\n",
      "epoch: 14\n",
      "True\n",
      "Epoch 15, Epoch status: 0.95, Training loss: 0.0085\n",
      "evaluating epoch 15 ...\n",
      "MSRVTT - epoch: 15, epoch status: 0.99, r@1: 0.203, r@5: 0.581, r@10: 0.726, mr: 4\n",
      "epoch: 15\n",
      "True\n",
      "Epoch 16, Epoch status: 0.95, Training loss: 0.0084\n",
      "evaluating epoch 16 ...\n",
      "MSRVTT - epoch: 16, epoch status: 0.99, r@1: 0.220, r@5: 0.592, r@10: 0.736, mr: 4\n",
      "epoch: 16\n",
      "True\n",
      "Epoch 17, Epoch status: 0.95, Training loss: 0.0081\n",
      "evaluating epoch 17 ...\n",
      "MSRVTT - epoch: 17, epoch status: 0.99, r@1: 0.216, r@5: 0.578, r@10: 0.724, mr: 4\n",
      "epoch: 17\n",
      "True\n",
      "Epoch 18, Epoch status: 0.95, Training loss: 0.0081\n",
      "evaluating epoch 18 ...\n",
      "MSRVTT - epoch: 18, epoch status: 0.99, r@1: 0.236, r@5: 0.603, r@10: 0.732, mr: 4\n",
      "epoch: 18\n",
      "True\n",
      "Epoch 19, Epoch status: 0.95, Training loss: 0.0079\n",
      "evaluating epoch 19 ...\n",
      "MSRVTT - epoch: 19, epoch status: 0.99, r@1: 0.235, r@5: 0.598, r@10: 0.740, mr: 4\n",
      "epoch: 19\n",
      "True\n",
      "Epoch 20, Epoch status: 0.95, Training loss: 0.0078\n",
      "evaluating epoch 20 ...\n",
      "MSRVTT - epoch: 20, epoch status: 0.99, r@1: 0.235, r@5: 0.592, r@10: 0.729, mr: 4\n",
      "epoch: 20\n",
      "True\n",
      "Epoch 21, Epoch status: 0.95, Training loss: 0.0078\n",
      "evaluating epoch 21 ...\n",
      "MSRVTT - epoch: 21, epoch status: 0.99, r@1: 0.241, r@5: 0.581, r@10: 0.737, mr: 4\n",
      "epoch: 21\n",
      "True\n",
      "Epoch 22, Epoch status: 0.95, Training loss: 0.0079\n",
      "evaluating epoch 22 ...\n",
      "MSRVTT - epoch: 22, epoch status: 0.99, r@1: 0.237, r@5: 0.596, r@10: 0.753, mr: 4\n",
      "epoch: 22\n",
      "True\n",
      "Epoch 23, Epoch status: 0.95, Training loss: 0.0076\n",
      "evaluating epoch 23 ...\n",
      "MSRVTT - epoch: 23, epoch status: 0.99, r@1: 0.256, r@5: 0.610, r@10: 0.756, mr: 4\n",
      "epoch: 23\n",
      "True\n",
      "Epoch 24, Epoch status: 0.95, Training loss: 0.0076\n",
      "evaluating epoch 24 ...\n",
      "MSRVTT - epoch: 24, epoch status: 0.99, r@1: 0.242, r@5: 0.604, r@10: 0.749, mr: 4\n",
      "epoch: 24\n",
      "True\n",
      "Epoch 25, Epoch status: 0.95, Training loss: 0.0074\n",
      "evaluating epoch 25 ...\n",
      "MSRVTT - epoch: 25, epoch status: 0.99, r@1: 0.262, r@5: 0.610, r@10: 0.752, mr: 4\n",
      "epoch: 25\n",
      "True\n",
      "Epoch 26, Epoch status: 0.95, Training loss: 0.0074\n",
      "evaluating epoch 26 ...\n",
      "MSRVTT - epoch: 26, epoch status: 0.99, r@1: 0.269, r@5: 0.609, r@10: 0.751, mr: 3\n",
      "epoch: 26\n",
      "True\n",
      "Epoch 27, Epoch status: 0.95, Training loss: 0.0074\n",
      "evaluating epoch 27 ...\n",
      "MSRVTT - epoch: 27, epoch status: 0.99, r@1: 0.263, r@5: 0.628, r@10: 0.767, mr: 3\n",
      "epoch: 27\n",
      "True\n",
      "Epoch 28, Epoch status: 0.95, Training loss: 0.0070\n",
      "evaluating epoch 28 ...\n",
      "MSRVTT - epoch: 28, epoch status: 0.99, r@1: 0.260, r@5: 0.626, r@10: 0.749, mr: 4\n",
      "epoch: 28\n",
      "True\n",
      "Epoch 29, Epoch status: 0.95, Training loss: 0.0072\n",
      "evaluating epoch 29 ...\n",
      "MSRVTT - epoch: 29, epoch status: 0.99, r@1: 0.263, r@5: 0.626, r@10: 0.757, mr: 3\n",
      "epoch: 29\n",
      "True\n",
      "Epoch 30, Epoch status: 0.95, Training loss: 0.0068\n",
      "evaluating epoch 30 ...\n",
      "MSRVTT - epoch: 30, epoch status: 0.99, r@1: 0.269, r@5: 0.618, r@10: 0.767, mr: 3\n",
      "epoch: 30\n",
      "True\n",
      "Epoch 31, Epoch status: 0.95, Training loss: 0.0068\n",
      "evaluating epoch 31 ...\n",
      "MSRVTT - epoch: 31, epoch status: 0.99, r@1: 0.273, r@5: 0.618, r@10: 0.768, mr: 4\n",
      "epoch: 31\n",
      "True\n",
      "Epoch 32, Epoch status: 0.95, Training loss: 0.0069\n",
      "evaluating epoch 32 ...\n",
      "MSRVTT - epoch: 32, epoch status: 0.99, r@1: 0.281, r@5: 0.627, r@10: 0.769, mr: 3\n",
      "epoch: 32\n",
      "True\n",
      "Epoch 33, Epoch status: 0.95, Training loss: 0.0068\n",
      "evaluating epoch 33 ...\n",
      "MSRVTT - epoch: 33, epoch status: 0.99, r@1: 0.265, r@5: 0.619, r@10: 0.762, mr: 4\n",
      "epoch: 33\n",
      "True\n",
      "Epoch 34, Epoch status: 0.95, Training loss: 0.0069\n",
      "evaluating epoch 34 ...\n",
      "MSRVTT - epoch: 34, epoch status: 0.99, r@1: 0.276, r@5: 0.626, r@10: 0.768, mr: 3\n",
      "epoch: 34\n",
      "True\n",
      "Epoch 35, Epoch status: 0.95, Training loss: 0.0067\n",
      "evaluating epoch 35 ...\n",
      "MSRVTT - epoch: 35, epoch status: 0.99, r@1: 0.285, r@5: 0.653, r@10: 0.786, mr: 3\n",
      "epoch: 35\n",
      "True\n",
      "Epoch 36, Epoch status: 0.95, Training loss: 0.0069\n",
      "evaluating epoch 36 ...\n",
      "MSRVTT - epoch: 36, epoch status: 0.99, r@1: 0.275, r@5: 0.634, r@10: 0.768, mr: 3\n",
      "epoch: 36\n",
      "True\n",
      "Epoch 37, Epoch status: 0.95, Training loss: 0.0066\n",
      "evaluating epoch 37 ...\n",
      "MSRVTT - epoch: 37, epoch status: 0.99, r@1: 0.276, r@5: 0.634, r@10: 0.774, mr: 3\n",
      "epoch: 37\n",
      "True\n",
      "Epoch 38, Epoch status: 0.95, Training loss: 0.0066\n",
      "evaluating epoch 38 ...\n",
      "MSRVTT - epoch: 38, epoch status: 0.99, r@1: 0.277, r@5: 0.643, r@10: 0.771, mr: 3\n",
      "epoch: 38\n",
      "True\n",
      "Epoch 39, Epoch status: 0.95, Training loss: 0.0064\n",
      "evaluating epoch 39 ...\n",
      "MSRVTT - epoch: 39, epoch status: 0.99, r@1: 0.275, r@5: 0.637, r@10: 0.772, mr: 3\n",
      "epoch: 39\n",
      "True\n",
      "Epoch 40, Epoch status: 0.95, Training loss: 0.0064\n",
      "evaluating epoch 40 ...\n",
      "MSRVTT - epoch: 40, epoch status: 0.99, r@1: 0.277, r@5: 0.648, r@10: 0.774, mr: 3\n",
      "epoch: 40\n",
      "True\n",
      "Epoch 41, Epoch status: 0.95, Training loss: 0.0063\n",
      "evaluating epoch 41 ...\n",
      "MSRVTT - epoch: 41, epoch status: 0.99, r@1: 0.277, r@5: 0.622, r@10: 0.759, mr: 3\n",
      "epoch: 41\n",
      "True\n",
      "Epoch 42, Epoch status: 0.95, Training loss: 0.0065\n",
      "evaluating epoch 42 ...\n",
      "MSRVTT - epoch: 42, epoch status: 0.99, r@1: 0.282, r@5: 0.627, r@10: 0.773, mr: 3\n",
      "epoch: 42\n",
      "True\n",
      "Epoch 43, Epoch status: 0.95, Training loss: 0.0063\n",
      "evaluating epoch 43 ...\n",
      "MSRVTT - epoch: 43, epoch status: 0.99, r@1: 0.295, r@5: 0.655, r@10: 0.770, mr: 3\n",
      "epoch: 43\n",
      "True\n",
      "Epoch 44, Epoch status: 0.95, Training loss: 0.0063\n",
      "evaluating epoch 44 ...\n",
      "MSRVTT - epoch: 44, epoch status: 0.99, r@1: 0.293, r@5: 0.641, r@10: 0.769, mr: 3\n",
      "epoch: 44\n",
      "True\n",
      "Epoch 45, Epoch status: 0.95, Training loss: 0.0061\n",
      "evaluating epoch 45 ...\n",
      "MSRVTT - epoch: 45, epoch status: 0.99, r@1: 0.288, r@5: 0.647, r@10: 0.771, mr: 3\n",
      "epoch: 45\n",
      "True\n",
      "Epoch 46, Epoch status: 0.95, Training loss: 0.0063\n",
      "evaluating epoch 46 ...\n",
      "MSRVTT - epoch: 46, epoch status: 0.99, r@1: 0.278, r@5: 0.632, r@10: 0.783, mr: 3\n",
      "epoch: 46\n",
      "True\n",
      "Epoch 47, Epoch status: 0.95, Training loss: 0.0061\n",
      "evaluating epoch 47 ...\n",
      "MSRVTT - epoch: 47, epoch status: 0.99, r@1: 0.303, r@5: 0.643, r@10: 0.772, mr: 3\n",
      "epoch: 47\n",
      "True\n",
      "Epoch 48, Epoch status: 0.95, Training loss: 0.0061\n",
      "evaluating epoch 48 ...\n",
      "MSRVTT - epoch: 48, epoch status: 0.99, r@1: 0.292, r@5: 0.633, r@10: 0.780, mr: 3\n",
      "epoch: 48\n",
      "True\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49, Epoch status: 0.95, Training loss: 0.0062\n",
      "evaluating epoch 49 ...\n",
      "MSRVTT - epoch: 49, epoch status: 0.99, r@1: 0.304, r@5: 0.637, r@10: 0.780, mr: 3\n",
      "epoch: 49\n",
      "True\n",
      "Epoch 50, Epoch status: 0.95, Training loss: 0.0062\n",
      "evaluating epoch 50 ...\n",
      "MSRVTT - epoch: 50, epoch status: 0.99, r@1: 0.286, r@5: 0.638, r@10: 0.776, mr: 3\n"
     ]
    }
   ],
   "source": [
    "print ('Starting training loop ...')\n",
    "\n",
    "#for epoch in range(args.epochs):\n",
    "    \n",
    "for epoch in range(50):\n",
    "#     net.train()\n",
    "#     #net.train(False)\n",
    "    net.train()\n",
    "    running_loss = 0.0\n",
    "    print ('epoch: %d'%epoch)\n",
    "    print (net.training)\n",
    "\n",
    "    for i_batch, sample_batched in enumerate(dataloader):\n",
    "\n",
    "        if args.MSRVTT:\n",
    "            captions = sample_batched['text']\n",
    "            audio = sample_batched['audio']\n",
    "        else:\n",
    "            captions = dataset.shorteningTextTensor(sample_batched['text'],\n",
    "                    sample_batched['text_size']) #[128,30,300] ==>[128,28,300] , max length 30\n",
    "                                                 # the longest text length is 28 in this batch\n",
    "            \n",
    "            audio = dataset.shorteningTextTensor(sample_batched['audio'],\n",
    "                    sample_batched['audio_size']) #[128, 183, 128]==> [128, 16, 128], max length 183\n",
    "                                                  # the longest text length is 16 in this batch\n",
    "       \n",
    "\n",
    "        face = sample_batched['face'] #[128,128]\n",
    "        video = sample_batched['video']  #[128,2048]\n",
    "        flow = sample_batched['flow'] #[128,1024]\n",
    "        coco_ind = sample_batched['coco_ind'] #[128], 0 indicates not coco image\n",
    "        face_ind = sample_batched['face_ind'] #[128], 0 indicates no face descriptor\n",
    "\n",
    "        ind = {}\n",
    "        ind['face'] = face_ind  # [128]\n",
    "        ind['visual'] = np.ones((len(face_ind))) # all one mask for 'visual'\n",
    "        ind['motion'] = 1 - coco_ind # lsmdb has one for motion and audio,\n",
    "        ind['audio'] = 1 - coco_ind # coco is 0 since no motion/audio\n",
    "\n",
    "        if args.GPU:\n",
    "            captions, video = Variable(captions.cuda()), Variable(video.cuda())\n",
    "            audio, flow  =  Variable(audio.cuda()), Variable(flow.cuda())\n",
    "            face = Variable(face.cuda())\n",
    "\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        confusion_matrix = net(captions,\n",
    "                {'face': face, 'audio': audio, 'visual': video, 'motion': flow}, ind, True)\n",
    "        loss = max_margin(confusion_matrix)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        running_loss += loss.data[0]\n",
    "        #print(loss.data[0])\n",
    "        #if i_batch==2:\n",
    "        #    break\n",
    "        \n",
    "        if (i_batch+1) % n_display == 0:\n",
    "            print ('Epoch %d, Epoch status: %.2f, Training loss: %.4f'%(epoch + 1,\n",
    "                    args.batch_size*float(i_batch)/dataset_size,running_loss/n_display))\n",
    "            running_loss = 0.0\n",
    "\n",
    "    print ('evaluating epoch %d ...'%(epoch+1))\n",
    "    net.eval()  \n",
    "    with torch.no_grad():\n",
    "    #if True:\n",
    "        if args.MSRVTT:\n",
    "            retrieval_samples = dataset.getRetrievalSamples()\n",
    "\n",
    "            video = Variable(retrieval_samples['video'].cuda(), volatile=True)\n",
    "            captions = Variable(retrieval_samples['text'].cuda(), volatile=True)\n",
    "            audio = Variable(retrieval_samples['audio'].cuda(), volatile=True)\n",
    "            flow = Variable(retrieval_samples['flow'].cuda(), volatile=True)\n",
    "            face = Variable(retrieval_samples['face'].cuda(), volatile=True)\n",
    "            face_ind = retrieval_samples['face_ind']\n",
    "\n",
    "            ind = {}\n",
    "            ind['face'] = face_ind\n",
    "            ind['visual'] = np.ones((len(face_ind)))\n",
    "            ind['motion'] = np.ones((len(face_ind)))\n",
    "            ind['audio'] = np.ones((len(face_ind)))\n",
    "\n",
    "\n",
    "            conf = net(captions,\n",
    "                    {'face': face, 'audio': audio, 'visual': video, 'motion': flow}, ind, True)\n",
    "            confusion_matrix = conf.data.cpu().float().numpy() #[1000,1000] ==>change tensor to numpy\n",
    "            metrics = compute_metric_yang(confusion_matrix)\n",
    "            verbose(epoch, args.batch_size*float(i_batch)/dataset_size, metrics, name='MSRVTT')\n",
    "\n",
    "        else:\n",
    "            i_batch=1\n",
    "            print (net.training)\n",
    "            video = Variable(vid_retrieval_val.cuda(), volatile=True)\n",
    "            captions = Variable(text_retrieval_val.cuda(), volatile=True)\n",
    "            audio = Variable(audio_retrieval_val.cuda(), volatile=True)\n",
    "            flow = Variable(flow_retrieval_val.cuda(), volatile=True)\n",
    "            face = Variable(face_retrieval_val.cuda(), volatile=True)\n",
    "\n",
    "            ind = {}\n",
    "            ind['face'] = face_ind_test\n",
    "            ind['visual'] = np.ones((len(face_ind_test)))\n",
    "            ind['motion'] = np.ones((len(face_ind_test)))\n",
    "            ind['audio'] = np.ones((len(face_ind_test)))\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            conf = net(captions,\n",
    "                    {'face': face, 'audio': audio, 'visual': video, 'motion': flow}, ind, True)\n",
    "            confusion_matrix = conf.data.cpu().float().numpy()\n",
    "            metrics = compute_metric_yang(confusion_matrix)\n",
    "            #verbose(epoch, args.batch_size*float(i_batch)/dataset_size, metrics, name='MPII')\n",
    "            verbose(epoch, args.batch_size*float(i_batch)/dataset_size, metrics, name='MPII')\n",
    "\n",
    "\n",
    "    #net.train()\n",
    "    \n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] *= lr_decay\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aa=confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sx = np.sort(aa, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = np.diag(aa) #(1000)\n",
    "d = d[:,np.newaxis] #(1000,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = sx - d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_pos = np.where(ind == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_pos[1][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metric_yang(x):\n",
    "    sx = np.sort(x, axis=1)\n",
    "    d = np.diag(x) #(1000)\n",
    "    d = d[:,np.newaxis] #(1000,1)\n",
    "    ind = sx - d\n",
    "    ind = np.where(ind == 0)\n",
    "    ind = ind[1]\n",
    "\n",
    "    metrics = {}\n",
    "    metrics['R1'] = float(np.sum(ind == 0))/len(ind)\n",
    "    metrics['R5'] = float(np.sum(ind < 5))/len(ind)\n",
    "    metrics['R10'] = float(np.sum(ind < 10))/len(ind)\n",
    "    metrics['MR'] = np.median(ind) + 1\n",
    "\n",
    "    return metrics\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " metrics_yang = compute_metric_distance(confusion_matrix)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch]",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

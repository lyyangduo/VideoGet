{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(GPU=True, MSRVTT=False, batch_size=128, coco=False, coco_sampling_rate=1.0, epochs=50, eval_qcm=False, lr=0.0001, lr_decay=0.95, margin=0.2, model_name='test', momentum=0.9, n_cpu=4, n_display=100, optimizer='adam', seed=1, text_cluster_size=32)\n",
      "Pre-loading features ... This may takes several minutes ...\n",
      "Done.\n",
      "Reading test data ...\n",
      "Done.\n",
      "Starting training loop ...\n",
      "epoch: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/yangl/external/anaconda3/envs/pytorch/lib/python3.7/site-packages/ipykernel/__main__.py:293: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Epoch status: 0.13, Training loss: 0.1344\n",
      "Epoch 1, Epoch status: 0.25, Training loss: 0.0842\n",
      "Epoch 1, Epoch status: 0.38, Training loss: 0.0537\n",
      "Epoch 1, Epoch status: 0.51, Training loss: 0.0439\n",
      "Epoch 1, Epoch status: 0.63, Training loss: 0.0388\n",
      "Epoch 1, Epoch status: 0.76, Training loss: 0.0381\n",
      "Epoch 1, Epoch status: 0.89, Training loss: 0.0344\n",
      "evaluating epoch 1 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/yangl/external/anaconda3/envs/pytorch/lib/python3.7/site-packages/ipykernel/__main__.py:329: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "/users/yangl/external/anaconda3/envs/pytorch/lib/python3.7/site-packages/ipykernel/__main__.py:330: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "/users/yangl/external/anaconda3/envs/pytorch/lib/python3.7/site-packages/ipykernel/__main__.py:331: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "/users/yangl/external/anaconda3/envs/pytorch/lib/python3.7/site-packages/ipykernel/__main__.py:332: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "/users/yangl/external/anaconda3/envs/pytorch/lib/python3.7/site-packages/ipykernel/__main__.py:333: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPII - epoch: 1, epoch status: 1.00, r@1: 0.071, r@5: 0.190, r@10: 0.285, mr: 30\n",
      "epoch: 1\n",
      "Epoch 2, Epoch status: 0.13, Training loss: 0.0299\n",
      "Epoch 2, Epoch status: 0.25, Training loss: 0.0285\n",
      "Epoch 2, Epoch status: 0.38, Training loss: 0.0286\n",
      "Epoch 2, Epoch status: 0.51, Training loss: 0.0263\n",
      "Epoch 2, Epoch status: 0.63, Training loss: 0.0259\n",
      "Epoch 2, Epoch status: 0.76, Training loss: 0.0247\n",
      "Epoch 2, Epoch status: 0.89, Training loss: 0.0244\n",
      "evaluating epoch 2 ...\n",
      "MPII - epoch: 2, epoch status: 1.00, r@1: 0.094, r@5: 0.293, r@10: 0.408, mr: 17\n",
      "epoch: 2\n",
      "Epoch 3, Epoch status: 0.13, Training loss: 0.0216\n",
      "Epoch 3, Epoch status: 0.25, Training loss: 0.0208\n",
      "Epoch 3, Epoch status: 0.38, Training loss: 0.0206\n",
      "Epoch 3, Epoch status: 0.51, Training loss: 0.0203\n",
      "Epoch 3, Epoch status: 0.63, Training loss: 0.0200\n",
      "Epoch 3, Epoch status: 0.76, Training loss: 0.0196\n",
      "Epoch 3, Epoch status: 0.89, Training loss: 0.0188\n",
      "evaluating epoch 3 ...\n",
      "MPII - epoch: 3, epoch status: 1.00, r@1: 0.130, r@5: 0.359, r@10: 0.488, mr: 11\n",
      "epoch: 3\n",
      "Epoch 4, Epoch status: 0.13, Training loss: 0.0161\n",
      "Epoch 4, Epoch status: 0.25, Training loss: 0.0155\n",
      "Epoch 4, Epoch status: 0.38, Training loss: 0.0147\n",
      "Epoch 4, Epoch status: 0.51, Training loss: 0.0144\n",
      "Epoch 4, Epoch status: 0.63, Training loss: 0.0137\n",
      "Epoch 4, Epoch status: 0.76, Training loss: 0.0131\n",
      "Epoch 4, Epoch status: 0.89, Training loss: 0.0127\n",
      "evaluating epoch 4 ...\n",
      "MPII - epoch: 4, epoch status: 1.00, r@1: 0.177, r@5: 0.463, r@10: 0.596, mr: 7\n",
      "epoch: 4\n",
      "Epoch 5, Epoch status: 0.13, Training loss: 0.0116\n",
      "Epoch 5, Epoch status: 0.25, Training loss: 0.0112\n",
      "Epoch 5, Epoch status: 0.38, Training loss: 0.0112\n",
      "Epoch 5, Epoch status: 0.51, Training loss: 0.0111\n",
      "Epoch 5, Epoch status: 0.63, Training loss: 0.0110\n",
      "Epoch 5, Epoch status: 0.76, Training loss: 0.0108\n",
      "Epoch 5, Epoch status: 0.89, Training loss: 0.0108\n",
      "evaluating epoch 5 ...\n",
      "MPII - epoch: 5, epoch status: 1.00, r@1: 0.201, r@5: 0.499, r@10: 0.648, mr: 6\n",
      "epoch: 5\n",
      "Epoch 6, Epoch status: 0.13, Training loss: 0.0097\n",
      "Epoch 6, Epoch status: 0.25, Training loss: 0.0096\n",
      "Epoch 6, Epoch status: 0.38, Training loss: 0.0098\n",
      "Epoch 6, Epoch status: 0.51, Training loss: 0.0098\n",
      "Epoch 6, Epoch status: 0.63, Training loss: 0.0097\n",
      "Epoch 6, Epoch status: 0.76, Training loss: 0.0096\n",
      "Epoch 6, Epoch status: 0.89, Training loss: 0.0097\n",
      "evaluating epoch 6 ...\n",
      "MPII - epoch: 6, epoch status: 1.00, r@1: 0.221, r@5: 0.532, r@10: 0.678, mr: 5\n",
      "epoch: 6\n",
      "Epoch 7, Epoch status: 0.13, Training loss: 0.0087\n",
      "Epoch 7, Epoch status: 0.25, Training loss: 0.0087\n",
      "Epoch 7, Epoch status: 0.38, Training loss: 0.0089\n",
      "Epoch 7, Epoch status: 0.51, Training loss: 0.0088\n",
      "Epoch 7, Epoch status: 0.63, Training loss: 0.0090\n",
      "Epoch 7, Epoch status: 0.76, Training loss: 0.0088\n",
      "Epoch 7, Epoch status: 0.89, Training loss: 0.0090\n",
      "evaluating epoch 7 ...\n",
      "MPII - epoch: 7, epoch status: 1.00, r@1: 0.218, r@5: 0.548, r@10: 0.693, mr: 4\n",
      "epoch: 7\n",
      "Epoch 8, Epoch status: 0.13, Training loss: 0.0081\n",
      "Epoch 8, Epoch status: 0.25, Training loss: 0.0083\n",
      "Epoch 8, Epoch status: 0.38, Training loss: 0.0082\n",
      "Epoch 8, Epoch status: 0.51, Training loss: 0.0083\n",
      "Epoch 8, Epoch status: 0.63, Training loss: 0.0086\n",
      "Epoch 8, Epoch status: 0.76, Training loss: 0.0083\n",
      "Epoch 8, Epoch status: 0.89, Training loss: 0.0085\n",
      "evaluating epoch 8 ...\n",
      "MPII - epoch: 8, epoch status: 1.00, r@1: 0.253, r@5: 0.543, r@10: 0.681, mr: 4\n",
      "epoch: 8\n",
      "Epoch 9, Epoch status: 0.13, Training loss: 0.0077\n",
      "Epoch 9, Epoch status: 0.25, Training loss: 0.0078\n",
      "Epoch 9, Epoch status: 0.38, Training loss: 0.0078\n",
      "Epoch 9, Epoch status: 0.51, Training loss: 0.0078\n",
      "Epoch 9, Epoch status: 0.63, Training loss: 0.0080\n",
      "Epoch 9, Epoch status: 0.76, Training loss: 0.0079\n",
      "Epoch 9, Epoch status: 0.89, Training loss: 0.0078\n",
      "evaluating epoch 9 ...\n",
      "MPII - epoch: 9, epoch status: 1.00, r@1: 0.219, r@5: 0.538, r@10: 0.666, mr: 5\n",
      "epoch: 9\n",
      "Epoch 10, Epoch status: 0.13, Training loss: 0.0073\n",
      "Epoch 10, Epoch status: 0.25, Training loss: 0.0073\n",
      "Epoch 10, Epoch status: 0.38, Training loss: 0.0073\n",
      "Epoch 10, Epoch status: 0.51, Training loss: 0.0074\n",
      "Epoch 10, Epoch status: 0.63, Training loss: 0.0073\n",
      "Epoch 10, Epoch status: 0.76, Training loss: 0.0074\n",
      "Epoch 10, Epoch status: 0.89, Training loss: 0.0075\n",
      "evaluating epoch 10 ...\n",
      "MPII - epoch: 10, epoch status: 1.00, r@1: 0.241, r@5: 0.543, r@10: 0.673, mr: 4\n",
      "epoch: 10\n",
      "Epoch 11, Epoch status: 0.13, Training loss: 0.0068\n",
      "Epoch 11, Epoch status: 0.25, Training loss: 0.0069\n",
      "Epoch 11, Epoch status: 0.38, Training loss: 0.0070\n",
      "Epoch 11, Epoch status: 0.51, Training loss: 0.0071\n",
      "Epoch 11, Epoch status: 0.63, Training loss: 0.0071\n",
      "Epoch 11, Epoch status: 0.76, Training loss: 0.0072\n",
      "Epoch 11, Epoch status: 0.89, Training loss: 0.0070\n",
      "evaluating epoch 11 ...\n",
      "MPII - epoch: 11, epoch status: 1.00, r@1: 0.229, r@5: 0.519, r@10: 0.665, mr: 5\n",
      "epoch: 11\n",
      "Epoch 12, Epoch status: 0.13, Training loss: 0.0065\n",
      "Epoch 12, Epoch status: 0.25, Training loss: 0.0065\n",
      "Epoch 12, Epoch status: 0.38, Training loss: 0.0066\n",
      "Epoch 12, Epoch status: 0.51, Training loss: 0.0066\n",
      "Epoch 12, Epoch status: 0.63, Training loss: 0.0067\n",
      "Epoch 12, Epoch status: 0.76, Training loss: 0.0068\n",
      "Epoch 12, Epoch status: 0.89, Training loss: 0.0068\n",
      "evaluating epoch 12 ...\n",
      "MPII - epoch: 12, epoch status: 1.00, r@1: 0.238, r@5: 0.525, r@10: 0.649, mr: 5\n",
      "epoch: 12\n",
      "Epoch 13, Epoch status: 0.13, Training loss: 0.0061\n",
      "Epoch 13, Epoch status: 0.25, Training loss: 0.0062\n",
      "Epoch 13, Epoch status: 0.38, Training loss: 0.0063\n",
      "Epoch 13, Epoch status: 0.51, Training loss: 0.0064\n",
      "Epoch 13, Epoch status: 0.63, Training loss: 0.0064\n",
      "Epoch 13, Epoch status: 0.76, Training loss: 0.0065\n",
      "Epoch 13, Epoch status: 0.89, Training loss: 0.0065\n",
      "evaluating epoch 13 ...\n",
      "MPII - epoch: 13, epoch status: 1.00, r@1: 0.233, r@5: 0.513, r@10: 0.659, mr: 5\n",
      "epoch: 13\n",
      "Epoch 14, Epoch status: 0.13, Training loss: 0.0059\n",
      "Epoch 14, Epoch status: 0.25, Training loss: 0.0058\n",
      "Epoch 14, Epoch status: 0.38, Training loss: 0.0061\n",
      "Epoch 14, Epoch status: 0.51, Training loss: 0.0061\n",
      "Epoch 14, Epoch status: 0.63, Training loss: 0.0061\n",
      "Epoch 14, Epoch status: 0.76, Training loss: 0.0060\n",
      "Epoch 14, Epoch status: 0.89, Training loss: 0.0061\n",
      "evaluating epoch 14 ...\n",
      "MPII - epoch: 14, epoch status: 1.00, r@1: 0.235, r@5: 0.529, r@10: 0.670, mr: 5\n",
      "epoch: 14\n",
      "Epoch 15, Epoch status: 0.13, Training loss: 0.0055\n",
      "Epoch 15, Epoch status: 0.25, Training loss: 0.0056\n",
      "Epoch 15, Epoch status: 0.38, Training loss: 0.0058\n",
      "Epoch 15, Epoch status: 0.51, Training loss: 0.0058\n",
      "Epoch 15, Epoch status: 0.63, Training loss: 0.0058\n",
      "Epoch 15, Epoch status: 0.76, Training loss: 0.0060\n",
      "Epoch 15, Epoch status: 0.89, Training loss: 0.0059\n",
      "evaluating epoch 15 ...\n",
      "MPII - epoch: 15, epoch status: 1.00, r@1: 0.224, r@5: 0.513, r@10: 0.664, mr: 5\n",
      "epoch: 15\n",
      "Epoch 16, Epoch status: 0.13, Training loss: 0.0054\n",
      "Epoch 16, Epoch status: 0.25, Training loss: 0.0055\n",
      "Epoch 16, Epoch status: 0.38, Training loss: 0.0057\n",
      "Epoch 16, Epoch status: 0.51, Training loss: 0.0056\n",
      "Epoch 16, Epoch status: 0.63, Training loss: 0.0056\n",
      "Epoch 16, Epoch status: 0.76, Training loss: 0.0056\n",
      "Epoch 16, Epoch status: 0.89, Training loss: 0.0056\n",
      "evaluating epoch 16 ...\n",
      "MPII - epoch: 16, epoch status: 1.00, r@1: 0.237, r@5: 0.526, r@10: 0.662, mr: 5\n",
      "epoch: 16\n",
      "Epoch 17, Epoch status: 0.13, Training loss: 0.0052\n",
      "Epoch 17, Epoch status: 0.25, Training loss: 0.0052\n",
      "Epoch 17, Epoch status: 0.38, Training loss: 0.0052\n",
      "Epoch 17, Epoch status: 0.51, Training loss: 0.0053\n",
      "Epoch 17, Epoch status: 0.63, Training loss: 0.0054\n",
      "Epoch 17, Epoch status: 0.76, Training loss: 0.0054\n",
      "Epoch 17, Epoch status: 0.89, Training loss: 0.0054\n",
      "evaluating epoch 17 ...\n",
      "MPII - epoch: 17, epoch status: 1.00, r@1: 0.243, r@5: 0.523, r@10: 0.643, mr: 5\n",
      "epoch: 17\n",
      "Epoch 18, Epoch status: 0.13, Training loss: 0.0050\n",
      "Epoch 18, Epoch status: 0.25, Training loss: 0.0051\n",
      "Epoch 18, Epoch status: 0.38, Training loss: 0.0052\n",
      "Epoch 18, Epoch status: 0.51, Training loss: 0.0051\n",
      "Epoch 18, Epoch status: 0.63, Training loss: 0.0051\n",
      "Epoch 18, Epoch status: 0.76, Training loss: 0.0052\n",
      "Epoch 18, Epoch status: 0.89, Training loss: 0.0053\n",
      "evaluating epoch 18 ...\n",
      "MPII - epoch: 18, epoch status: 1.00, r@1: 0.234, r@5: 0.511, r@10: 0.657, mr: 5\n",
      "epoch: 18\n",
      "Epoch 19, Epoch status: 0.13, Training loss: 0.0049\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19, Epoch status: 0.25, Training loss: 0.0049\n",
      "Epoch 19, Epoch status: 0.38, Training loss: 0.0050\n",
      "Epoch 19, Epoch status: 0.51, Training loss: 0.0050\n",
      "Epoch 19, Epoch status: 0.63, Training loss: 0.0050\n",
      "Epoch 19, Epoch status: 0.76, Training loss: 0.0050\n",
      "Epoch 19, Epoch status: 0.89, Training loss: 0.0051\n",
      "evaluating epoch 19 ...\n",
      "MPII - epoch: 19, epoch status: 1.00, r@1: 0.240, r@5: 0.512, r@10: 0.664, mr: 5\n",
      "epoch: 19\n",
      "Epoch 20, Epoch status: 0.13, Training loss: 0.0047\n",
      "Epoch 20, Epoch status: 0.25, Training loss: 0.0047\n",
      "Epoch 20, Epoch status: 0.38, Training loss: 0.0048\n",
      "Epoch 20, Epoch status: 0.51, Training loss: 0.0048\n",
      "Epoch 20, Epoch status: 0.63, Training loss: 0.0048\n",
      "Epoch 20, Epoch status: 0.76, Training loss: 0.0050\n",
      "Epoch 20, Epoch status: 0.89, Training loss: 0.0049\n",
      "evaluating epoch 20 ...\n",
      "MPII - epoch: 20, epoch status: 1.00, r@1: 0.224, r@5: 0.511, r@10: 0.660, mr: 5\n",
      "epoch: 20\n",
      "Epoch 21, Epoch status: 0.13, Training loss: 0.0045\n",
      "Epoch 21, Epoch status: 0.25, Training loss: 0.0047\n",
      "Epoch 21, Epoch status: 0.38, Training loss: 0.0046\n",
      "Epoch 21, Epoch status: 0.51, Training loss: 0.0047\n",
      "Epoch 21, Epoch status: 0.63, Training loss: 0.0047\n",
      "Epoch 21, Epoch status: 0.76, Training loss: 0.0048\n",
      "Epoch 21, Epoch status: 0.89, Training loss: 0.0048\n",
      "evaluating epoch 21 ...\n",
      "MPII - epoch: 21, epoch status: 1.00, r@1: 0.222, r@5: 0.513, r@10: 0.646, mr: 5\n",
      "epoch: 21\n",
      "Epoch 22, Epoch status: 0.13, Training loss: 0.0044\n",
      "Epoch 22, Epoch status: 0.25, Training loss: 0.0044\n",
      "Epoch 22, Epoch status: 0.38, Training loss: 0.0045\n",
      "Epoch 22, Epoch status: 0.51, Training loss: 0.0045\n",
      "Epoch 22, Epoch status: 0.63, Training loss: 0.0047\n",
      "Epoch 22, Epoch status: 0.76, Training loss: 0.0046\n",
      "Epoch 22, Epoch status: 0.89, Training loss: 0.0046\n",
      "evaluating epoch 22 ...\n",
      "MPII - epoch: 22, epoch status: 1.00, r@1: 0.233, r@5: 0.516, r@10: 0.650, mr: 5\n",
      "epoch: 22\n",
      "Epoch 23, Epoch status: 0.13, Training loss: 0.0043\n",
      "Epoch 23, Epoch status: 0.25, Training loss: 0.0044\n",
      "Epoch 23, Epoch status: 0.38, Training loss: 0.0044\n",
      "Epoch 23, Epoch status: 0.51, Training loss: 0.0044\n",
      "Epoch 23, Epoch status: 0.63, Training loss: 0.0045\n",
      "Epoch 23, Epoch status: 0.76, Training loss: 0.0044\n",
      "Epoch 23, Epoch status: 0.89, Training loss: 0.0045\n",
      "evaluating epoch 23 ...\n",
      "MPII - epoch: 23, epoch status: 1.00, r@1: 0.231, r@5: 0.498, r@10: 0.632, mr: 6\n",
      "epoch: 23\n",
      "Epoch 24, Epoch status: 0.13, Training loss: 0.0042\n",
      "Epoch 24, Epoch status: 0.25, Training loss: 0.0042\n",
      "Epoch 24, Epoch status: 0.38, Training loss: 0.0043\n",
      "Epoch 24, Epoch status: 0.51, Training loss: 0.0044\n",
      "Epoch 24, Epoch status: 0.63, Training loss: 0.0043\n",
      "Epoch 24, Epoch status: 0.76, Training loss: 0.0044\n",
      "Epoch 24, Epoch status: 0.89, Training loss: 0.0045\n",
      "evaluating epoch 24 ...\n",
      "MPII - epoch: 24, epoch status: 1.00, r@1: 0.236, r@5: 0.501, r@10: 0.624, mr: 5\n",
      "epoch: 24\n",
      "Epoch 25, Epoch status: 0.13, Training loss: 0.0041\n",
      "Epoch 25, Epoch status: 0.25, Training loss: 0.0041\n",
      "Epoch 25, Epoch status: 0.38, Training loss: 0.0041\n",
      "Epoch 25, Epoch status: 0.51, Training loss: 0.0042\n",
      "Epoch 25, Epoch status: 0.63, Training loss: 0.0042\n",
      "Epoch 25, Epoch status: 0.76, Training loss: 0.0043\n",
      "Epoch 25, Epoch status: 0.89, Training loss: 0.0043\n",
      "evaluating epoch 25 ...\n",
      "MPII - epoch: 25, epoch status: 1.00, r@1: 0.233, r@5: 0.500, r@10: 0.641, mr: 5\n",
      "epoch: 25\n",
      "Epoch 26, Epoch status: 0.13, Training loss: 0.0040\n",
      "Epoch 26, Epoch status: 0.25, Training loss: 0.0040\n",
      "Epoch 26, Epoch status: 0.38, Training loss: 0.0041\n",
      "Epoch 26, Epoch status: 0.51, Training loss: 0.0041\n",
      "Epoch 26, Epoch status: 0.63, Training loss: 0.0042\n",
      "Epoch 26, Epoch status: 0.76, Training loss: 0.0042\n",
      "Epoch 26, Epoch status: 0.89, Training loss: 0.0042\n",
      "evaluating epoch 26 ...\n",
      "MPII - epoch: 26, epoch status: 1.00, r@1: 0.241, r@5: 0.502, r@10: 0.636, mr: 5\n",
      "epoch: 26\n",
      "Epoch 27, Epoch status: 0.13, Training loss: 0.0039\n",
      "Epoch 27, Epoch status: 0.25, Training loss: 0.0040\n",
      "Epoch 27, Epoch status: 0.38, Training loss: 0.0040\n",
      "Epoch 27, Epoch status: 0.51, Training loss: 0.0040\n",
      "Epoch 27, Epoch status: 0.63, Training loss: 0.0040\n",
      "Epoch 27, Epoch status: 0.76, Training loss: 0.0040\n",
      "Epoch 27, Epoch status: 0.89, Training loss: 0.0040\n",
      "evaluating epoch 27 ...\n",
      "MPII - epoch: 27, epoch status: 1.00, r@1: 0.232, r@5: 0.492, r@10: 0.624, mr: 6\n",
      "epoch: 27\n",
      "Epoch 28, Epoch status: 0.13, Training loss: 0.0039\n",
      "Epoch 28, Epoch status: 0.25, Training loss: 0.0039\n",
      "Epoch 28, Epoch status: 0.38, Training loss: 0.0039\n",
      "Epoch 28, Epoch status: 0.51, Training loss: 0.0039\n",
      "Epoch 28, Epoch status: 0.63, Training loss: 0.0040\n",
      "Epoch 28, Epoch status: 0.76, Training loss: 0.0040\n",
      "Epoch 28, Epoch status: 0.89, Training loss: 0.0040\n",
      "evaluating epoch 28 ...\n",
      "MPII - epoch: 28, epoch status: 1.00, r@1: 0.222, r@5: 0.495, r@10: 0.624, mr: 6\n",
      "epoch: 28\n",
      "Epoch 29, Epoch status: 0.13, Training loss: 0.0037\n",
      "Epoch 29, Epoch status: 0.25, Training loss: 0.0038\n",
      "Epoch 29, Epoch status: 0.38, Training loss: 0.0038\n",
      "Epoch 29, Epoch status: 0.51, Training loss: 0.0038\n",
      "Epoch 29, Epoch status: 0.63, Training loss: 0.0039\n",
      "Epoch 29, Epoch status: 0.76, Training loss: 0.0039\n",
      "Epoch 29, Epoch status: 0.89, Training loss: 0.0039\n",
      "evaluating epoch 29 ...\n",
      "MPII - epoch: 29, epoch status: 1.00, r@1: 0.233, r@5: 0.490, r@10: 0.619, mr: 6\n",
      "epoch: 29\n",
      "Epoch 30, Epoch status: 0.13, Training loss: 0.0037\n",
      "Epoch 30, Epoch status: 0.25, Training loss: 0.0038\n",
      "Epoch 30, Epoch status: 0.38, Training loss: 0.0037\n",
      "Epoch 30, Epoch status: 0.51, Training loss: 0.0038\n",
      "Epoch 30, Epoch status: 0.63, Training loss: 0.0038\n",
      "Epoch 30, Epoch status: 0.76, Training loss: 0.0038\n",
      "Epoch 30, Epoch status: 0.89, Training loss: 0.0039\n",
      "evaluating epoch 30 ...\n",
      "MPII - epoch: 30, epoch status: 1.00, r@1: 0.227, r@5: 0.485, r@10: 0.624, mr: 6\n",
      "epoch: 30\n",
      "Epoch 31, Epoch status: 0.13, Training loss: 0.0036\n",
      "Epoch 31, Epoch status: 0.25, Training loss: 0.0037\n",
      "Epoch 31, Epoch status: 0.38, Training loss: 0.0037\n",
      "Epoch 31, Epoch status: 0.51, Training loss: 0.0037\n",
      "Epoch 31, Epoch status: 0.63, Training loss: 0.0036\n",
      "Epoch 31, Epoch status: 0.76, Training loss: 0.0037\n",
      "Epoch 31, Epoch status: 0.89, Training loss: 0.0038\n",
      "evaluating epoch 31 ...\n",
      "MPII - epoch: 31, epoch status: 1.00, r@1: 0.221, r@5: 0.495, r@10: 0.621, mr: 6\n",
      "epoch: 31\n",
      "Epoch 32, Epoch status: 0.13, Training loss: 0.0035\n",
      "Epoch 32, Epoch status: 0.25, Training loss: 0.0036\n",
      "Epoch 32, Epoch status: 0.38, Training loss: 0.0036\n",
      "Epoch 32, Epoch status: 0.51, Training loss: 0.0037\n",
      "Epoch 32, Epoch status: 0.63, Training loss: 0.0036\n",
      "Epoch 32, Epoch status: 0.76, Training loss: 0.0037\n",
      "Epoch 32, Epoch status: 0.89, Training loss: 0.0037\n",
      "evaluating epoch 32 ...\n",
      "MPII - epoch: 32, epoch status: 1.00, r@1: 0.220, r@5: 0.484, r@10: 0.614, mr: 6\n",
      "epoch: 32\n",
      "Epoch 33, Epoch status: 0.13, Training loss: 0.0035\n",
      "Epoch 33, Epoch status: 0.25, Training loss: 0.0035\n",
      "Epoch 33, Epoch status: 0.38, Training loss: 0.0036\n",
      "Epoch 33, Epoch status: 0.51, Training loss: 0.0036\n",
      "Epoch 33, Epoch status: 0.63, Training loss: 0.0036\n",
      "Epoch 33, Epoch status: 0.76, Training loss: 0.0037\n",
      "Epoch 33, Epoch status: 0.89, Training loss: 0.0036\n",
      "evaluating epoch 33 ...\n",
      "MPII - epoch: 33, epoch status: 1.00, r@1: 0.225, r@5: 0.476, r@10: 0.597, mr: 6\n",
      "epoch: 33\n",
      "Epoch 34, Epoch status: 0.13, Training loss: 0.0035\n",
      "Epoch 34, Epoch status: 0.25, Training loss: 0.0035\n",
      "Epoch 34, Epoch status: 0.38, Training loss: 0.0035\n",
      "Epoch 34, Epoch status: 0.51, Training loss: 0.0035\n",
      "Epoch 34, Epoch status: 0.63, Training loss: 0.0036\n",
      "Epoch 34, Epoch status: 0.76, Training loss: 0.0035\n",
      "Epoch 34, Epoch status: 0.89, Training loss: 0.0036\n",
      "evaluating epoch 34 ...\n",
      "MPII - epoch: 34, epoch status: 1.00, r@1: 0.235, r@5: 0.484, r@10: 0.616, mr: 6\n",
      "epoch: 34\n",
      "Epoch 35, Epoch status: 0.13, Training loss: 0.0035\n",
      "Epoch 35, Epoch status: 0.25, Training loss: 0.0034\n",
      "Epoch 35, Epoch status: 0.38, Training loss: 0.0035\n",
      "Epoch 35, Epoch status: 0.51, Training loss: 0.0034\n",
      "Epoch 35, Epoch status: 0.63, Training loss: 0.0035\n",
      "Epoch 35, Epoch status: 0.76, Training loss: 0.0035\n",
      "Epoch 35, Epoch status: 0.89, Training loss: 0.0035\n",
      "evaluating epoch 35 ...\n",
      "MPII - epoch: 35, epoch status: 1.00, r@1: 0.223, r@5: 0.489, r@10: 0.606, mr: 6\n",
      "epoch: 35\n",
      "Epoch 36, Epoch status: 0.13, Training loss: 0.0033\n",
      "Epoch 36, Epoch status: 0.25, Training loss: 0.0034\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36, Epoch status: 0.38, Training loss: 0.0034\n",
      "Epoch 36, Epoch status: 0.51, Training loss: 0.0034\n",
      "Epoch 36, Epoch status: 0.63, Training loss: 0.0035\n",
      "Epoch 36, Epoch status: 0.76, Training loss: 0.0035\n",
      "Epoch 36, Epoch status: 0.89, Training loss: 0.0035\n",
      "evaluating epoch 36 ...\n",
      "MPII - epoch: 36, epoch status: 1.00, r@1: 0.224, r@5: 0.479, r@10: 0.602, mr: 6\n",
      "epoch: 36\n",
      "Epoch 37, Epoch status: 0.13, Training loss: 0.0033\n",
      "Epoch 37, Epoch status: 0.25, Training loss: 0.0034\n",
      "Epoch 37, Epoch status: 0.38, Training loss: 0.0034\n",
      "Epoch 37, Epoch status: 0.51, Training loss: 0.0034\n",
      "Epoch 37, Epoch status: 0.63, Training loss: 0.0035\n",
      "Epoch 37, Epoch status: 0.76, Training loss: 0.0034\n",
      "Epoch 37, Epoch status: 0.89, Training loss: 0.0035\n",
      "evaluating epoch 37 ...\n",
      "MPII - epoch: 37, epoch status: 1.00, r@1: 0.228, r@5: 0.473, r@10: 0.612, mr: 6\n",
      "epoch: 37\n",
      "Epoch 38, Epoch status: 0.13, Training loss: 0.0033\n",
      "Epoch 38, Epoch status: 0.25, Training loss: 0.0033\n",
      "Epoch 38, Epoch status: 0.38, Training loss: 0.0033\n",
      "Epoch 38, Epoch status: 0.51, Training loss: 0.0033\n",
      "Epoch 38, Epoch status: 0.63, Training loss: 0.0034\n",
      "Epoch 38, Epoch status: 0.76, Training loss: 0.0034\n",
      "Epoch 38, Epoch status: 0.89, Training loss: 0.0034\n",
      "evaluating epoch 38 ...\n",
      "MPII - epoch: 38, epoch status: 1.00, r@1: 0.214, r@5: 0.469, r@10: 0.601, mr: 6\n",
      "epoch: 38\n",
      "Epoch 39, Epoch status: 0.13, Training loss: 0.0033\n",
      "Epoch 39, Epoch status: 0.25, Training loss: 0.0032\n",
      "Epoch 39, Epoch status: 0.38, Training loss: 0.0033\n",
      "Epoch 39, Epoch status: 0.51, Training loss: 0.0033\n",
      "Epoch 39, Epoch status: 0.63, Training loss: 0.0033\n",
      "Epoch 39, Epoch status: 0.76, Training loss: 0.0033\n",
      "Epoch 39, Epoch status: 0.89, Training loss: 0.0034\n",
      "evaluating epoch 39 ...\n",
      "MPII - epoch: 39, epoch status: 1.00, r@1: 0.225, r@5: 0.483, r@10: 0.594, mr: 6\n",
      "epoch: 39\n",
      "Epoch 40, Epoch status: 0.13, Training loss: 0.0033\n",
      "Epoch 40, Epoch status: 0.25, Training loss: 0.0033\n",
      "Epoch 40, Epoch status: 0.38, Training loss: 0.0032\n",
      "Epoch 40, Epoch status: 0.51, Training loss: 0.0033\n",
      "Epoch 40, Epoch status: 0.63, Training loss: 0.0033\n",
      "Epoch 40, Epoch status: 0.76, Training loss: 0.0033\n",
      "Epoch 40, Epoch status: 0.89, Training loss: 0.0034\n",
      "evaluating epoch 40 ...\n",
      "MPII - epoch: 40, epoch status: 1.00, r@1: 0.213, r@5: 0.480, r@10: 0.599, mr: 6\n",
      "epoch: 40\n",
      "Epoch 41, Epoch status: 0.13, Training loss: 0.0032\n",
      "Epoch 41, Epoch status: 0.25, Training loss: 0.0032\n",
      "Epoch 41, Epoch status: 0.38, Training loss: 0.0032\n",
      "Epoch 41, Epoch status: 0.51, Training loss: 0.0032\n",
      "Epoch 41, Epoch status: 0.63, Training loss: 0.0033\n",
      "Epoch 41, Epoch status: 0.76, Training loss: 0.0033\n",
      "Epoch 41, Epoch status: 0.89, Training loss: 0.0032\n",
      "evaluating epoch 41 ...\n",
      "MPII - epoch: 41, epoch status: 1.00, r@1: 0.219, r@5: 0.475, r@10: 0.594, mr: 6\n",
      "epoch: 41\n",
      "Epoch 42, Epoch status: 0.13, Training loss: 0.0032\n",
      "Epoch 42, Epoch status: 0.25, Training loss: 0.0032\n",
      "Epoch 42, Epoch status: 0.38, Training loss: 0.0032\n",
      "Epoch 42, Epoch status: 0.51, Training loss: 0.0032\n",
      "Epoch 42, Epoch status: 0.63, Training loss: 0.0032\n",
      "Epoch 42, Epoch status: 0.76, Training loss: 0.0032\n",
      "Epoch 42, Epoch status: 0.89, Training loss: 0.0033\n",
      "evaluating epoch 42 ...\n",
      "MPII - epoch: 42, epoch status: 1.00, r@1: 0.214, r@5: 0.465, r@10: 0.587, mr: 6\n",
      "epoch: 42\n",
      "Epoch 43, Epoch status: 0.13, Training loss: 0.0031\n",
      "Epoch 43, Epoch status: 0.25, Training loss: 0.0032\n",
      "Epoch 43, Epoch status: 0.38, Training loss: 0.0031\n",
      "Epoch 43, Epoch status: 0.51, Training loss: 0.0032\n",
      "Epoch 43, Epoch status: 0.63, Training loss: 0.0032\n",
      "Epoch 43, Epoch status: 0.76, Training loss: 0.0032\n",
      "Epoch 43, Epoch status: 0.89, Training loss: 0.0033\n",
      "evaluating epoch 43 ...\n",
      "MPII - epoch: 43, epoch status: 1.00, r@1: 0.218, r@5: 0.459, r@10: 0.583, mr: 6\n",
      "epoch: 43\n",
      "Epoch 44, Epoch status: 0.13, Training loss: 0.0031\n",
      "Epoch 44, Epoch status: 0.25, Training loss: 0.0032\n",
      "Epoch 44, Epoch status: 0.38, Training loss: 0.0031\n",
      "Epoch 44, Epoch status: 0.51, Training loss: 0.0031\n",
      "Epoch 44, Epoch status: 0.63, Training loss: 0.0031\n",
      "Epoch 44, Epoch status: 0.76, Training loss: 0.0032\n",
      "Epoch 44, Epoch status: 0.89, Training loss: 0.0032\n",
      "evaluating epoch 44 ...\n",
      "MPII - epoch: 44, epoch status: 1.00, r@1: 0.222, r@5: 0.485, r@10: 0.590, mr: 6\n",
      "epoch: 44\n",
      "Epoch 45, Epoch status: 0.13, Training loss: 0.0031\n",
      "Epoch 45, Epoch status: 0.25, Training loss: 0.0031\n",
      "Epoch 45, Epoch status: 0.38, Training loss: 0.0031\n",
      "Epoch 45, Epoch status: 0.51, Training loss: 0.0031\n",
      "Epoch 45, Epoch status: 0.63, Training loss: 0.0031\n",
      "Epoch 45, Epoch status: 0.76, Training loss: 0.0032\n",
      "Epoch 45, Epoch status: 0.89, Training loss: 0.0032\n",
      "evaluating epoch 45 ...\n",
      "MPII - epoch: 45, epoch status: 1.00, r@1: 0.215, r@5: 0.468, r@10: 0.597, mr: 7\n",
      "epoch: 45\n",
      "Epoch 46, Epoch status: 0.13, Training loss: 0.0031\n",
      "Epoch 46, Epoch status: 0.25, Training loss: 0.0031\n",
      "Epoch 46, Epoch status: 0.38, Training loss: 0.0031\n",
      "Epoch 46, Epoch status: 0.51, Training loss: 0.0031\n",
      "Epoch 46, Epoch status: 0.63, Training loss: 0.0031\n",
      "Epoch 46, Epoch status: 0.76, Training loss: 0.0031\n",
      "Epoch 46, Epoch status: 0.89, Training loss: 0.0032\n",
      "evaluating epoch 46 ...\n",
      "MPII - epoch: 46, epoch status: 1.00, r@1: 0.209, r@5: 0.466, r@10: 0.590, mr: 7\n",
      "epoch: 46\n",
      "Epoch 47, Epoch status: 0.13, Training loss: 0.0030\n",
      "Epoch 47, Epoch status: 0.25, Training loss: 0.0030\n",
      "Epoch 47, Epoch status: 0.38, Training loss: 0.0031\n",
      "Epoch 47, Epoch status: 0.51, Training loss: 0.0031\n",
      "Epoch 47, Epoch status: 0.63, Training loss: 0.0031\n",
      "Epoch 47, Epoch status: 0.76, Training loss: 0.0031\n",
      "Epoch 47, Epoch status: 0.89, Training loss: 0.0031\n",
      "evaluating epoch 47 ...\n",
      "MPII - epoch: 47, epoch status: 1.00, r@1: 0.204, r@5: 0.455, r@10: 0.581, mr: 7\n",
      "epoch: 47\n",
      "Epoch 48, Epoch status: 0.13, Training loss: 0.0030\n",
      "Epoch 48, Epoch status: 0.25, Training loss: 0.0030\n",
      "Epoch 48, Epoch status: 0.38, Training loss: 0.0031\n",
      "Epoch 48, Epoch status: 0.51, Training loss: 0.0031\n",
      "Epoch 48, Epoch status: 0.63, Training loss: 0.0030\n",
      "Epoch 48, Epoch status: 0.76, Training loss: 0.0031\n",
      "Epoch 48, Epoch status: 0.89, Training loss: 0.0031\n",
      "evaluating epoch 48 ...\n",
      "MPII - epoch: 48, epoch status: 1.00, r@1: 0.207, r@5: 0.461, r@10: 0.591, mr: 7\n",
      "epoch: 48\n",
      "Epoch 49, Epoch status: 0.13, Training loss: 0.0030\n",
      "Epoch 49, Epoch status: 0.25, Training loss: 0.0030\n",
      "Epoch 49, Epoch status: 0.38, Training loss: 0.0030\n",
      "Epoch 49, Epoch status: 0.51, Training loss: 0.0030\n",
      "Epoch 49, Epoch status: 0.63, Training loss: 0.0030\n",
      "Epoch 49, Epoch status: 0.76, Training loss: 0.0031\n",
      "Epoch 49, Epoch status: 0.89, Training loss: 0.0031\n",
      "evaluating epoch 49 ...\n",
      "MPII - epoch: 49, epoch status: 1.00, r@1: 0.213, r@5: 0.465, r@10: 0.590, mr: 7\n",
      "epoch: 49\n",
      "Epoch 50, Epoch status: 0.13, Training loss: 0.0030\n",
      "Epoch 50, Epoch status: 0.25, Training loss: 0.0030\n",
      "Epoch 50, Epoch status: 0.38, Training loss: 0.0030\n",
      "Epoch 50, Epoch status: 0.51, Training loss: 0.0030\n",
      "Epoch 50, Epoch status: 0.63, Training loss: 0.0030\n",
      "Epoch 50, Epoch status: 0.76, Training loss: 0.0030\n",
      "Epoch 50, Epoch status: 0.89, Training loss: 0.0030\n",
      "evaluating epoch 50 ...\n",
      "MPII - epoch: 50, epoch status: 1.00, r@1: 0.208, r@5: 0.458, r@10: 0.592, mr: 7\n"
     ]
    }
   ],
   "source": [
    "import torch as th\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import LSMDC as LD2\n",
    "import MSRVTT as MSR\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "import argparse\n",
    "#from loss import MaxMarginRankingLoss\n",
    "from loss import MaxMarginRankingLoss_Distance  #Yang changed\n",
    "from model_yang import Net_Fuse\n",
    "from torch.autograd import Variable\n",
    "import os\n",
    "import random\n",
    "from qcm_sampler import QCMSampler\n",
    "from MSR_sampler import MSRSampler\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser(description='LSMDC2017')\n",
    "\n",
    "parser.add_argument('--coco', type=bool, default=False,\n",
    "                            help='add coco dataset')\n",
    "\n",
    "parser.add_argument('--lr', type=float, default=0.0001,\n",
    "                            help='initial learning rate')\n",
    "parser.add_argument('--epochs', type=int, default=50,\n",
    "                            help='upper epoch limit')\n",
    "parser.add_argument('--batch_size', type=int, default=128,\n",
    "                            help='batch size')\n",
    "parser.add_argument('--text_cluster_size', type=int, default=32,\n",
    "                            help='Text cluster size')\n",
    "parser.add_argument('--margin', type=float, default=0.2,\n",
    "                            help='MaxMargin margin value')\n",
    "parser.add_argument('--lr_decay', type=float, default=0.95,\n",
    "                            help='Learning rate exp epoch decay')\n",
    "parser.add_argument('--n_display', type=int, default=100,\n",
    "                            help='Information display frequence')\n",
    "parser.add_argument('--GPU', type=bool, default=True,\n",
    "                            help='Use of GPU')\n",
    "parser.add_argument('--n_cpu', type=int, default=4,\n",
    "                            help='Number of CPU')\n",
    "\n",
    "parser.add_argument('--model_name', type=str, default='test',\n",
    "                            help='Model name')\n",
    "parser.add_argument('--seed', type=int, default=1,\n",
    "                            help='Initial Random Seed')\n",
    "\n",
    "parser.add_argument('--optimizer', type=str, default='adam',\n",
    "                            help='optimizer')\n",
    "parser.add_argument('--momentum', type=float, default=0.9,\n",
    "                            help='Nesterov Momentum for SGD')\n",
    "\n",
    "\n",
    "parser.add_argument('--eval_qcm', type=bool, default=False,\n",
    "                            help='Eval or not QCM')\n",
    "\n",
    "parser.add_argument('--MSRVTT', type=bool, default=False,\n",
    "                            help='MSRVTT')\n",
    "\n",
    "parser.add_argument('--coco_sampling_rate', type=float, default=1.0,\n",
    "                            help='coco sampling rate')\n",
    "\n",
    "\n",
    "#args = parser.parse_args()\n",
    "args, unknown =  parser.parse_known_args()\n",
    "\n",
    "print (args)\n",
    "\n",
    "root_feat = 'data'\n",
    "\n",
    "mp_visual_path = os.path.join(root_feat,'X_resnet.npy')\n",
    "mp_flow_path = os.path.join(root_feat,'X_flow.npy')\n",
    "mp_face_path = os.path.join(root_feat,'X_face.npy')\n",
    "\n",
    "# predefining random initial seeds\n",
    "th.manual_seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "random.seed(args.seed)\n",
    "\n",
    "if args.eval_qcm and not(args.MSRVTT):\n",
    "    qcm_dataset = LD2.LSMDC_qcm(os.path.join(root_feat,'resnet-qcm.npy'),\n",
    "            os.path.join(root_feat,'w2v_LSMDC_qcm.npy'), os.path.join(root_feat,'X_audio_test.npy'),\n",
    "            os.path.join(root_feat,'flow-qcm.npy'),\n",
    "            os.path.join(root_feat,'face-qcm.npy')) \n",
    "    \n",
    "    qcm_sampler = QCMSampler(len(qcm_dataset))\n",
    "    qcm_dataloader = DataLoader(qcm_dataset, batch_size=500, sampler=qcm_sampler, num_workers=1)\n",
    "    qcm_gt_fn = os.path.join(root_feat,'multiple_choice_gt.txt')\n",
    "    qcm_gt = [line.rstrip('\\n') for line in open(qcm_gt_fn)]\n",
    "    qcm_gt = np.array(map(int,qcm_gt))\n",
    "\n",
    "def make_tensor(l, max_len):\n",
    "    tensor = np.zeros((len(l),max_len,l[0].shape[-1]))\n",
    "    for i in range(len(l)):\n",
    "        if len(l[i]):\n",
    "            tensor[i,:min(max_len,l[i].shape[0]),:] = l[i][:min(max_len,l[i].shape[0])]\n",
    "\n",
    "    return th.from_numpy(tensor).float()\n",
    "\n",
    "def verbose(epoch, status, metrics, name='TEST'):\n",
    "    print(name+' - epoch: %d, epoch status: %.2f, r@1: %.3f, r@5: %.3f, r@10: %.3f, mr: %d' % \n",
    "            (epoch + 1, status, \n",
    "                metrics['R1'], metrics['R5'], metrics['R10'],\n",
    "                metrics['MR']))\n",
    "\n",
    "\n",
    "def compute_metric(x):\n",
    "    sx = np.sort(-x, axis=1)\n",
    "    d = np.diag(-x)\n",
    "    d = d[:,np.newaxis]\n",
    "    ind = sx - d\n",
    "    ind = np.where(ind == 0)\n",
    "    ind = ind[1]\n",
    "\n",
    "    metrics = {}\n",
    "    metrics['R1'] = float(np.sum(ind == 0))/len(ind)\n",
    "    metrics['R5'] = float(np.sum(ind < 5))/len(ind)\n",
    "    metrics['R10'] = float(np.sum(ind < 10))/len(ind)\n",
    "    metrics['MR'] = np.median(ind) + 1\n",
    "\n",
    "    return metrics\n",
    "\n",
    "def compute_metric_yang(x):\n",
    "    sx = np.sort(x, axis=1)\n",
    "    d = np.diag(x) #(1000)\n",
    "    d = d[:,np.newaxis] #(1000,1)\n",
    "    ind = sx - d\n",
    "    ind = np.where(ind == 0)\n",
    "    ind = ind[1]\n",
    "\n",
    "    metrics = {}\n",
    "    metrics['R1'] = float(np.sum(ind == 0))/len(ind)  # for 1000 sample, howmany correct one is at the first\n",
    "    metrics['R5'] = float(np.sum(ind < 5))/len(ind)   # for 1000 sample, howmany correct one is among the first five\n",
    "    metrics['R10'] = float(np.sum(ind < 10))/len(ind) \n",
    "    metrics['MR'] = np.median(ind) + 1\n",
    "\n",
    "    return metrics\n",
    "\n",
    "\n",
    "\n",
    "print ('Pre-loading features ... This may takes several minutes ...')\n",
    "\n",
    "if args.MSRVTT:\n",
    "    print ('MSRVTT')\n",
    "    visual_feat_path = os.path.join(root_feat,'resnet_features.pickle')  \n",
    "    flow_feat_path = os.path.join(root_feat,'flow_features.pickle')\n",
    "    text_feat_path = os.path.join(root_feat,'w2v_MSRVTT.pickle')\n",
    "    audio_feat_path = os.path.join(root_feat,'audio_features.pickle')\n",
    "    face_feat_path = os.path.join(root_feat,'face_features.pickle')\n",
    "    train_list_path = os.path.join(root_feat,'train_list.txt')\n",
    "    test_list_path = os.path.join(root_feat,'test_list.txt')\n",
    "\n",
    "    dataset = MSR.MSRVTT(visual_feat_path, flow_feat_path, text_feat_path,\n",
    "            audio_feat_path, face_feat_path, train_list_path,test_list_path, coco=args.coco) \n",
    "    msr_sampler = MSRSampler(dataset.n_MSR, dataset.n_coco, args.coco_sampling_rate)\n",
    "    \n",
    "    if args.coco:\n",
    "        dataloader = DataLoader(dataset, batch_size=args.batch_size,\n",
    "                sampler=msr_sampler, num_workers=1,collate_fn=dataset.collate_data, drop_last=True)\n",
    "    else:\n",
    "        dataloader = DataLoader(dataset, batch_size=args.batch_size,\n",
    "                shuffle=True, num_workers=1,collate_fn=dataset.collate_data, drop_last=True)\n",
    "\n",
    "else:\n",
    "    path_to_text = os.path.join(root_feat,'w2v_LSMDC.npy')\n",
    "    path_to_audio = os.path.join(root_feat,'X_audio_train.npy')\n",
    "    path_to_coco_visual_path= os.path.join(root_feat,'X_train2014_resnet152.npy') #Yang add\n",
    "    path_to_coco_text_path=os.path.join(root_feat,'w2v_coco_train2014_1.npy')#Yang add\n",
    "    \n",
    "\n",
    "    #dataset = LD2.LSMDC(mp_visual_path, path_to_text,\n",
    "    #        path_to_audio, mp_flow_path, mp_face_path, coco=args.coco) \n",
    "    dataset = LD2.LSMDC(mp_visual_path, path_to_text,\n",
    "            path_to_audio, mp_flow_path, mp_face_path,path_to_coco_visual_path,path_to_coco_text_path, coco=args.coco) \n",
    "    dataloader = DataLoader(dataset, batch_size=args.batch_size,\n",
    "            shuffle=True, num_workers=1, drop_last=True)\n",
    "    print ('Done.')\n",
    "\n",
    "    print ('Reading test data ...')\n",
    "    resnet_features_path = os.path.join(root_feat,'resnet152-retrieval.npy.tensor.npy')\n",
    "    flow_features_path = os.path.join(root_feat,'flow-retrieval.npy.tensor.npy')\n",
    "    face_features_path = os.path.join(root_feat,'face-retrieval.npy.tensor.npy')\n",
    "    text_features_path = os.path.join(root_feat,'w2v_LSMDC_retrieval.npy')\n",
    "    audio_features_path = os.path.join(root_feat,'X_audio_retrieval.npy.tensor.npy')\n",
    "\n",
    "    vid_retrieval = np.load(resnet_features_path, encoding='latin1') # torch size [1000,2048]\n",
    "    flow_retrieval = np.load(flow_features_path, encoding='latin1') # torch size [1000,1024]\n",
    "    face_retrieval = np.load(face_features_path, encoding='latin1')# torch size [1000,128]\n",
    "    text_retrieval = np.load(text_features_path, encoding='latin1')# torch Size([1000, 29, 300]) \n",
    "                                                                   # left -->right, maximum word length 29, \n",
    "                                                                   # if not enough, fill all zeros\n",
    "    audio_retrieval = np.load(audio_features_path, encoding='latin1') #torch.Size([1000, 30, 128])\n",
    "\n",
    "    mm = max(map(len,text_retrieval)) # map(func, iterable)  =29\n",
    "\n",
    "    text_retrieval = make_tensor(text_retrieval,mm)\n",
    "\n",
    "    vid_retrieval = th.from_numpy(vid_retrieval).float() # torch size [1000,2048]\n",
    "    flow_retrieval = th.from_numpy(flow_retrieval).float() # torch size [1000,1024]\n",
    "    face_retrieval = th.from_numpy(face_retrieval).float() # torch size [1000,128]\n",
    "    audio_retrieval = th.from_numpy(audio_retrieval).float() #torch.Size([1000, 30, 128])\n",
    "\n",
    "    text_retrieval_val = text_retrieval\n",
    "    vid_retrieval_val = vid_retrieval\n",
    "    flow_retrieval_val = flow_retrieval\n",
    "    face_retrieval_val = face_retrieval\n",
    "    audio_retrieval_val = audio_retrieval\n",
    "\n",
    "\n",
    "    face_ind_test = np.load(os.path.join(root_feat,'no_face_ind_retrieval.npy'))\n",
    "    face_ind_test = 1 - face_ind_test # for no face data, all descriptors==0\n",
    "print ('Done.')\n",
    "\n",
    "# Model\n",
    "#video_modality_dim = {'face': (128,128), 'audio': (128*16,128),\n",
    "#'visual': (2048,2048), 'motion': (1024,1024)}\n",
    "\n",
    "video_modality_dim_fuse = {'face': (128,128,256), 'audio': (128*16,128,256),\n",
    "'visual': (2048,2048,256), 'motion': (1024,1024,256)}\n",
    "\n",
    "text_dim={'text':(300,128,256)}\n",
    "net = Net_Fuse(video_modality_dim_fuse,text_dim,\n",
    "        audio_cluster=16,text_cluster=args.text_cluster_size)\n",
    "net.train()\n",
    "\n",
    "if args.GPU:\n",
    "    net.cuda()\n",
    "\n",
    "# Optimizers + Loss\n",
    "#max_margin = MaxMarginRankingLoss(margin=args.margin) \n",
    "max_margin = MaxMarginRankingLoss_Distance(margin=args.margin)  #Yang changed\n",
    "\n",
    "\n",
    "if args.optimizer == 'adam':\n",
    "    optimizer = optim.Adam(net.parameters(), lr=args.lr)\n",
    "elif args.optimizer == 'sgd':\n",
    "    optimizer = optim.SGD(net.parameters(), lr=args.lr, momentum=args.momentum)\n",
    "\n",
    "if args.GPU:\n",
    "    max_margin.cuda()\n",
    "\n",
    "n_display = args.n_display\n",
    "dataset_size = len(dataset)\n",
    "lr_decay = args.lr_decay\n",
    "\n",
    "\n",
    "\n",
    "print ('Starting training loop ...')\n",
    "\n",
    "for epoch in range(args.epochs):\n",
    "    running_loss = 0.0\n",
    "    print ('epoch: %d'%epoch)\n",
    "\n",
    "    for i_batch, sample_batched in enumerate(dataloader):\n",
    "\n",
    "        if args.MSRVTT:\n",
    "            captions = sample_batched['text']\n",
    "            audio = sample_batched['audio']\n",
    "        else:\n",
    "            captions = dataset.shorteningTextTensor(sample_batched['text'],\n",
    "                    sample_batched['text_size']) #[128,30,300] ==>[128,28,300] , max length 30\n",
    "                                                 # the longest text length is 28 in this batch\n",
    "            \n",
    "            audio = dataset.shorteningTextTensor(sample_batched['audio'],\n",
    "                    sample_batched['audio_size']) #[128, 183, 128]==> [128, 16, 128], max length 183\n",
    "                                                  # the longest text length is 16 in this batch\n",
    "       \n",
    "\n",
    "        face = sample_batched['face'] #[128,128]\n",
    "        video = sample_batched['video']  #[128,2048]\n",
    "        flow = sample_batched['flow'] #[128,1024]\n",
    "        coco_ind = sample_batched['coco_ind'] #[128], 0 indicates not coco image\n",
    "        face_ind = sample_batched['face_ind'] #[128], 0 indicates no face descriptor\n",
    "\n",
    "        ind = {}\n",
    "        ind['face'] = face_ind  # [128]\n",
    "        ind['visual'] = np.ones((len(face_ind))) # all one mask for 'visual'\n",
    "        ind['motion'] = 1 - coco_ind # lsmdb has one for motion and audio,\n",
    "        ind['audio'] = 1 - coco_ind # coco is 0 since no motion/audio\n",
    "\n",
    "        if args.GPU:\n",
    "            captions, video = Variable(captions.cuda()), Variable(video.cuda())\n",
    "            audio, flow  =  Variable(audio.cuda()), Variable(flow.cuda())\n",
    "            face = Variable(face.cuda())\n",
    "\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        confusion_matrix = net(captions,\n",
    "                {'face': face, 'audio': audio, 'visual': video, 'motion': flow}, ind, True)\n",
    "        loss = max_margin(confusion_matrix)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        running_loss += loss.data[0]\n",
    "        \n",
    "        #if i_batch==2:\n",
    "        #    break\n",
    "        \n",
    "        if (i_batch+1) % n_display == 0:\n",
    "            print ('Epoch %d, Epoch status: %.2f, Training loss: %.4f'%(epoch + 1,\n",
    "                    args.batch_size*float(i_batch)/dataset_size,running_loss/n_display))\n",
    "            running_loss = 0.0\n",
    "\n",
    "    print ('evaluating epoch %d ...'%(epoch+1))\n",
    "    net.eval()  \n",
    "\n",
    "    if args.MSRVTT:\n",
    "        retrieval_samples = dataset.getRetrievalSamples()\n",
    "\n",
    "        video = Variable(retrieval_samples['video'].cuda(), volatile=True)\n",
    "        captions = Variable(retrieval_samples['text'].cuda(), volatile=True)\n",
    "        audio = Variable(retrieval_samples['audio'].cuda(), volatile=True)\n",
    "        flow = Variable(retrieval_samples['flow'].cuda(), volatile=True)\n",
    "        face = Variable(retrieval_samples['face'].cuda(), volatile=True)\n",
    "        face_ind = retrieval_samples['face_ind']\n",
    "\n",
    "        ind = {}\n",
    "        ind['face'] = face_ind\n",
    "        ind['visual'] = np.ones((len(face_ind)))\n",
    "        ind['motion'] = np.ones((len(face_ind)))\n",
    "        ind['audio'] = np.ones((len(face_ind)))\n",
    "\n",
    "        conf = net(captions,\n",
    "                {'face': face, 'audio': audio, 'visual': video, 'motion': flow}, ind, True)\n",
    "        confusion_matrix = conf.data.cpu().float().numpy() #[1000,1000] ==>change tensor to numpy\n",
    "        metrics = compute_metric_yang(confusion_matrix)\n",
    "        verbose(epoch, args.batch_size*float(i_batch)/dataset_size, metrics, name='MSRVTT')\n",
    "\n",
    "    else:\n",
    "        video = Variable(vid_retrieval_val.cuda(), volatile=True)\n",
    "        captions = Variable(text_retrieval_val.cuda(), volatile=True)\n",
    "        audio = Variable(audio_retrieval_val.cuda(), volatile=True)\n",
    "        flow = Variable(flow_retrieval_val.cuda(), volatile=True)\n",
    "        face = Variable(face_retrieval_val.cuda(), volatile=True)\n",
    "\n",
    "        ind = {}\n",
    "        ind['face'] = face_ind_test\n",
    "        ind['visual'] = np.ones((len(face_ind_test)))\n",
    "        ind['motion'] = np.ones((len(face_ind_test)))\n",
    "        ind['audio'] = np.ones((len(face_ind_test)))\n",
    "\n",
    "        conf = net(captions,\n",
    "                {'face': face, 'audio': audio, 'visual': video, 'motion': flow}, ind, True)\n",
    "        confusion_matrix = conf.data.cpu().float().numpy()\n",
    "        metrics = compute_metric_yang(confusion_matrix)\n",
    "        verbose(epoch, args.batch_size*float(i_batch)/dataset_size, metrics, name='MPII')\n",
    "\n",
    "\n",
    "\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] *= lr_decay\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch]",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as th\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import LSMDC as LD2\n",
    "import MSRVTT as MSR\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "import argparse\n",
    "#from loss import MaxMarginRankingLoss\n",
    "from loss import MaxMarginRankingLoss_Distance  #Yang changed\n",
    "from model_yang import Net_Fuse\n",
    "from torch.autograd import Variable\n",
    "import os\n",
    "import random\n",
    "from qcm_sampler import QCMSampler\n",
    "from MSR_sampler import MSRSampler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(GPU=True, MSRVTT=False, batch_size=128, coco=False, coco_sampling_rate=1.0, epochs=50, eval_qcm=False, lr=0.0001, lr_decay=0.95, margin=0.2, model_name='test', momentum=0.9, n_cpu=1, n_display=100, optimizer='adam', seed=1, text_cluster_size=32)\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser(description='LSMDC2017')\n",
    "\n",
    "parser.add_argument('--coco', type=bool, default=False,\n",
    "                            help='add coco dataset')\n",
    "\n",
    "parser.add_argument('--lr', type=float, default=0.0001,\n",
    "                            help='initial learning rate')\n",
    "parser.add_argument('--epochs', type=int, default=50,\n",
    "                            help='upper epoch limit')\n",
    "parser.add_argument('--batch_size', type=int, default=128,\n",
    "                            help='batch size')\n",
    "parser.add_argument('--text_cluster_size', type=int, default=32,\n",
    "                            help='Text cluster size')\n",
    "parser.add_argument('--margin', type=float, default=0.2,\n",
    "                            help='MaxMargin margin value')\n",
    "parser.add_argument('--lr_decay', type=float, default=0.95,\n",
    "                            help='Learning rate exp epoch decay')\n",
    "parser.add_argument('--n_display', type=int, default=100,\n",
    "                            help='Information display frequence')\n",
    "parser.add_argument('--GPU', type=bool, default=True,\n",
    "                            help='Use of GPU')\n",
    "parser.add_argument('--n_cpu', type=int, default=1,\n",
    "                            help='Number of CPU')\n",
    "\n",
    "parser.add_argument('--model_name', type=str, default='test',\n",
    "                            help='Model name')\n",
    "parser.add_argument('--seed', type=int, default=1,\n",
    "                            help='Initial Random Seed')\n",
    "\n",
    "parser.add_argument('--optimizer', type=str, default='adam',\n",
    "                            help='optimizer')\n",
    "parser.add_argument('--momentum', type=float, default=0.9,\n",
    "                            help='Nesterov Momentum for SGD')\n",
    "\n",
    "\n",
    "parser.add_argument('--eval_qcm', type=bool, default=False,\n",
    "                            help='Eval or not QCM')\n",
    "\n",
    "parser.add_argument('--MSRVTT', type=bool, default=False,\n",
    "                            help='MSRVTT')\n",
    "\n",
    "parser.add_argument('--coco_sampling_rate', type=float, default=1.0,\n",
    "                            help='coco sampling rate')\n",
    "\n",
    "\n",
    "#args = parser.parse_args()\n",
    "args, unknown =  parser.parse_known_args()\n",
    "\n",
    "print (args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_feat = 'data'\n",
    "\n",
    "mp_visual_path = os.path.join(root_feat,'X_resnet.npy')\n",
    "mp_flow_path = os.path.join(root_feat,'X_flow.npy')\n",
    "mp_face_path = os.path.join(root_feat,'X_face.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predefining random initial seeds\n",
    "th.manual_seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "random.seed(args.seed)\n",
    "\n",
    "if args.eval_qcm and not(args.MSRVTT):\n",
    "    print ('sha')\n",
    "    qcm_dataset = LD2.LSMDC_qcm(os.path.join(root_feat,'resnet-qcm.npy'),\n",
    "            os.path.join(root_feat,'w2v_LSMDC_qcm.npy'), os.path.join(root_feat,'X_audio_test.npy'),\n",
    "            os.path.join(root_feat,'flow-qcm.npy'),\n",
    "            os.path.join(root_feat,'face-qcm.npy')) \n",
    "    \n",
    "    qcm_sampler = QCMSampler(len(qcm_dataset))\n",
    "    qcm_dataloader = DataLoader(qcm_dataset, batch_size=500, sampler=qcm_sampler, num_workers=1)\n",
    "    qcm_gt_fn = os.path.join(root_feat,'multiple_choice_gt.txt')\n",
    "    qcm_gt = [line.rstrip('\\n') for line in open(qcm_gt_fn)]\n",
    "    qcm_gt = np.array(map(int,qcm_gt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_tensor(l, max_len):\n",
    "    tensor = np.zeros((len(l),max_len,l[0].shape[-1]))\n",
    "    for i in range(len(l)):\n",
    "        if len(l[i]):\n",
    "            tensor[i,:min(max_len,l[i].shape[0]),:] = l[i][:min(max_len,l[i].shape[0])]\n",
    "\n",
    "    return th.from_numpy(tensor).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verbose(epoch, status, metrics, name='TEST'):\n",
    "    print(name+' - epoch: %d, epoch status: %.2f, r@1: %.3f, r@5: %.3f, r@10: %.3f, mr: %d' % \n",
    "            (epoch + 1, status, \n",
    "                metrics['R1'], metrics['R5'], metrics['R10'],\n",
    "                metrics['MR']))\n",
    "\n",
    "\n",
    "def compute_metric(x):\n",
    "    sx = np.sort(-x, axis=1)\n",
    "    d = np.diag(-x)\n",
    "    d = d[:,np.newaxis]\n",
    "    ind = sx - d\n",
    "    ind = np.where(ind == 0)\n",
    "    ind = ind[1]\n",
    "\n",
    "    metrics = {}\n",
    "    metrics['R1'] = float(np.sum(ind == 0))/len(ind)\n",
    "    metrics['R5'] = float(np.sum(ind < 5))/len(ind)\n",
    "    metrics['R10'] = float(np.sum(ind < 10))/len(ind)\n",
    "    metrics['MR'] = np.median(ind) + 1\n",
    "\n",
    "    return metrics\n",
    "\n",
    "def compute_metric_yang(x):\n",
    "    sx = np.sort(x, axis=1)\n",
    "    d = np.diag(x) #(1000)\n",
    "    d = d[:,np.newaxis] #(1000,1)\n",
    "    ind = sx - d\n",
    "    ind = np.where(ind == 0)\n",
    "    ind = ind[1]\n",
    "\n",
    "    metrics = {}\n",
    "    metrics['R1'] = float(np.sum(ind == 0))/len(ind)  # for 1000 sample, howmany correct one is at the first\n",
    "    metrics['R5'] = float(np.sum(ind < 5))/len(ind)   # for 1000 sample, howmany correct one is among the first five\n",
    "    metrics['R10'] = float(np.sum(ind < 10))/len(ind) \n",
    "    metrics['MR'] = np.median(ind) + 1\n",
    "\n",
    "    return metrics\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-loading features ... This may takes several minutes ...\n",
      "Done.\n",
      "Reading test data ...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "print ('Pre-loading features ... This may takes several minutes ...')\n",
    "\n",
    "if args.MSRVTT:\n",
    "    print ('MSRVTT')\n",
    "    visual_feat_path = os.path.join(root_feat,'resnet_features.pickle')  \n",
    "    flow_feat_path = os.path.join(root_feat,'flow_features.pickle')\n",
    "    text_feat_path = os.path.join(root_feat,'w2v_MSRVTT.pickle')\n",
    "    audio_feat_path = os.path.join(root_feat,'audio_features.pickle')\n",
    "    face_feat_path = os.path.join(root_feat,'face_features.pickle')\n",
    "    train_list_path = os.path.join(root_feat,'train_list.txt')\n",
    "    test_list_path = os.path.join(root_feat,'test_list.txt')\n",
    "\n",
    "    dataset = MSR.MSRVTT(visual_feat_path, flow_feat_path, text_feat_path,\n",
    "            audio_feat_path, face_feat_path, train_list_path,test_list_path, coco=args.coco) \n",
    "    msr_sampler = MSRSampler(dataset.n_MSR, dataset.n_coco, args.coco_sampling_rate)\n",
    "    \n",
    "    if args.coco:\n",
    "        dataloader = DataLoader(dataset, batch_size=args.batch_size,\n",
    "                sampler=msr_sampler, num_workers=1,collate_fn=dataset.collate_data, drop_last=True)\n",
    "    else:\n",
    "        dataloader = DataLoader(dataset, batch_size=args.batch_size,\n",
    "                shuffle=True, num_workers=1,collate_fn=dataset.collate_data, drop_last=True)\n",
    "\n",
    "else:\n",
    "    path_to_text = os.path.join(root_feat,'w2v_LSMDC.npy')\n",
    "    path_to_audio = os.path.join(root_feat,'X_audio_train.npy')\n",
    "    path_to_coco_visual_path= os.path.join(root_feat,'X_train2014_resnet152.npy') #Yang add\n",
    "    path_to_coco_text_path=os.path.join(root_feat,'w2v_coco_train2014_1.npy')#Yang add\n",
    "    \n",
    "\n",
    "    #dataset = LD2.LSMDC(mp_visual_path, path_to_text,\n",
    "    #        path_to_audio, mp_flow_path, mp_face_path, coco=args.coco) \n",
    "    #coco=args.coco\n",
    "    dataset = LD2.LSMDC(mp_visual_path, path_to_text,\n",
    "            path_to_audio, mp_flow_path, mp_face_path,path_to_coco_visual_path,path_to_coco_text_path, args.coco) \n",
    "    dataloader = DataLoader(dataset, batch_size=args.batch_size,\n",
    "            shuffle=True, num_workers=1, drop_last=True)\n",
    "    print ('Done.')\n",
    "\n",
    "    print ('Reading test data ...')\n",
    "    resnet_features_path = os.path.join(root_feat,'resnet152-retrieval.npy.tensor.npy')\n",
    "    flow_features_path = os.path.join(root_feat,'flow-retrieval.npy.tensor.npy')\n",
    "    face_features_path = os.path.join(root_feat,'face-retrieval.npy.tensor.npy')\n",
    "    text_features_path = os.path.join(root_feat,'w2v_LSMDC_retrieval.npy')\n",
    "    audio_features_path = os.path.join(root_feat,'X_audio_retrieval.npy.tensor.npy')\n",
    "\n",
    "    vid_retrieval = np.load(resnet_features_path, encoding='latin1') # torch size [1000,2048]\n",
    "    flow_retrieval = np.load(flow_features_path, encoding='latin1') # torch size [1000,1024]\n",
    "    face_retrieval = np.load(face_features_path, encoding='latin1')# torch size [1000,128]\n",
    "    text_retrieval = np.load(text_features_path, encoding='latin1')# torch Size([1000, 29, 300]) \n",
    "                                                                   # left -->right, maximum word length 29, \n",
    "                                                                   # if not enough, fill all zeros\n",
    "    audio_retrieval = np.load(audio_features_path, encoding='latin1') #torch.Size([1000, 30, 128])\n",
    "\n",
    "    mm = max(map(len,text_retrieval)) # map(func, iterable)  =29\n",
    "\n",
    "    text_retrieval = make_tensor(text_retrieval,mm)\n",
    "\n",
    "    vid_retrieval = th.from_numpy(vid_retrieval).float() # torch size [1000,2048]\n",
    "    flow_retrieval = th.from_numpy(flow_retrieval).float() # torch size [1000,1024]\n",
    "    face_retrieval = th.from_numpy(face_retrieval).float() # torch size [1000,128]\n",
    "    audio_retrieval = th.from_numpy(audio_retrieval).float() #torch.Size([1000, 30, 128])\n",
    "\n",
    "    text_retrieval_val = text_retrieval\n",
    "    vid_retrieval_val = vid_retrieval\n",
    "    flow_retrieval_val = flow_retrieval\n",
    "    face_retrieval_val = face_retrieval\n",
    "    audio_retrieval_val = audio_retrieval\n",
    "\n",
    "\n",
    "    face_ind_test = np.load(os.path.join(root_feat,'no_face_ind_retrieval.npy'))\n",
    "    face_ind_test = 1 - face_ind_test # for no face data, all descriptors==0\n",
    "print ('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([101074, 30, 300])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.text_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2048\n",
      "9600\n"
     ]
    }
   ],
   "source": [
    "# Model\n",
    "#video_modality_dim = {'face': (128,128), 'audio': (128*16,128),\n",
    "#'visual': (2048,2048), 'motion': (1024,1024)}\n",
    "\n",
    "video_modality_dim_fuse = {'face': (128,128,256), 'audio': (128*16,128,256),\n",
    "'visual': (2048,2048,256), 'motion': (1024,1024,256)}\n",
    "\n",
    "text_dim={'text':(300,128,256)}\n",
    "net = Net_Fuse(video_modality_dim_fuse,text_dim,\n",
    "        audio_cluster=16,text_cluster=args.text_cluster_size)\n",
    "#net.train()\n",
    "\n",
    "if args.GPU:\n",
    "    net.cuda()\n",
    "\n",
    "# Optimizers + Loss\n",
    "#max_margin = MaxMarginRankingLoss(margin=args.margin) \n",
    "max_margin = MaxMarginRankingLoss_Distance(margin=args.margin)  #Yang changed\n",
    "\n",
    "\n",
    "if args.optimizer == 'adam':\n",
    "    optimizer = optim.Adam(net.parameters(), lr=args.lr)\n",
    "elif args.optimizer == 'sgd':\n",
    "    optimizer = optim.SGD(net.parameters(), lr=args.lr, momentum=args.momentum)\n",
    "\n",
    "if args.GPU:\n",
    "    max_margin.cuda()\n",
    "\n",
    "n_display = args.n_display\n",
    "dataset_size = len(dataset)\n",
    "lr_decay = args.lr_decay\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training loop ...\n",
      "epoch: 0\n",
      "True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/yangl/external/anaconda3/envs/pytorch/lib/python3.7/site-packages/ipykernel/__main__.py:57: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Epoch status: 0.13, Training loss: 0.1474\n",
      "Epoch 1, Epoch status: 0.25, Training loss: 0.0967\n",
      "Epoch 1, Epoch status: 0.38, Training loss: 0.0615\n",
      "Epoch 1, Epoch status: 0.51, Training loss: 0.0483\n",
      "Epoch 1, Epoch status: 0.63, Training loss: 0.0423\n",
      "Epoch 1, Epoch status: 0.76, Training loss: 0.0409\n",
      "Epoch 1, Epoch status: 0.89, Training loss: 0.0375\n",
      "evaluating epoch 1 ...\n",
      "False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/yangl/external/anaconda3/envs/pytorch/lib/python3.7/site-packages/ipykernel/__main__.py:97: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "/users/yangl/external/anaconda3/envs/pytorch/lib/python3.7/site-packages/ipykernel/__main__.py:98: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "/users/yangl/external/anaconda3/envs/pytorch/lib/python3.7/site-packages/ipykernel/__main__.py:99: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "/users/yangl/external/anaconda3/envs/pytorch/lib/python3.7/site-packages/ipykernel/__main__.py:100: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "/users/yangl/external/anaconda3/envs/pytorch/lib/python3.7/site-packages/ipykernel/__main__.py:101: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPII - epoch: 1, epoch status: 0.00, r@1: 0.026, r@5: 0.094, r@10: 0.160, mr: 66\n",
      "epoch: 1\n",
      "True\n",
      "Epoch 2, Epoch status: 0.13, Training loss: 0.0332\n",
      "Epoch 2, Epoch status: 0.25, Training loss: 0.0316\n",
      "Epoch 2, Epoch status: 0.38, Training loss: 0.0321\n",
      "Epoch 2, Epoch status: 0.51, Training loss: 0.0300\n",
      "Epoch 2, Epoch status: 0.63, Training loss: 0.0289\n",
      "Epoch 2, Epoch status: 0.76, Training loss: 0.0274\n",
      "Epoch 2, Epoch status: 0.89, Training loss: 0.0273\n",
      "evaluating epoch 2 ...\n",
      "False\n",
      "MPII - epoch: 2, epoch status: 0.00, r@1: 0.052, r@5: 0.127, r@10: 0.204, mr: 52\n",
      "epoch: 2\n",
      "True\n",
      "Epoch 3, Epoch status: 0.13, Training loss: 0.0245\n",
      "Epoch 3, Epoch status: 0.25, Training loss: 0.0237\n",
      "Epoch 3, Epoch status: 0.38, Training loss: 0.0238\n",
      "Epoch 3, Epoch status: 0.51, Training loss: 0.0238\n",
      "Epoch 3, Epoch status: 0.63, Training loss: 0.0233\n",
      "Epoch 3, Epoch status: 0.76, Training loss: 0.0229\n",
      "Epoch 3, Epoch status: 0.89, Training loss: 0.0225\n",
      "evaluating epoch 3 ...\n",
      "False\n",
      "MPII - epoch: 3, epoch status: 0.00, r@1: 0.058, r@5: 0.185, r@10: 0.273, mr: 39\n",
      "epoch: 3\n",
      "True\n",
      "Epoch 4, Epoch status: 0.13, Training loss: 0.0210\n",
      "Epoch 4, Epoch status: 0.25, Training loss: 0.0207\n",
      "Epoch 4, Epoch status: 0.38, Training loss: 0.0205\n",
      "Epoch 4, Epoch status: 0.51, Training loss: 0.0208\n",
      "Epoch 4, Epoch status: 0.63, Training loss: 0.0203\n",
      "Epoch 4, Epoch status: 0.76, Training loss: 0.0199\n",
      "Epoch 4, Epoch status: 0.89, Training loss: 0.0196\n",
      "evaluating epoch 4 ...\n",
      "False\n",
      "MPII - epoch: 4, epoch status: 0.00, r@1: 0.057, r@5: 0.175, r@10: 0.257, mr: 37\n",
      "epoch: 4\n",
      "True\n",
      "Epoch 5, Epoch status: 0.13, Training loss: 0.0183\n",
      "Epoch 5, Epoch status: 0.25, Training loss: 0.0179\n",
      "Epoch 5, Epoch status: 0.38, Training loss: 0.0181\n",
      "Epoch 5, Epoch status: 0.51, Training loss: 0.0182\n",
      "Epoch 5, Epoch status: 0.63, Training loss: 0.0183\n",
      "Epoch 5, Epoch status: 0.76, Training loss: 0.0178\n",
      "Epoch 5, Epoch status: 0.89, Training loss: 0.0182\n",
      "evaluating epoch 5 ...\n",
      "False\n",
      "MPII - epoch: 5, epoch status: 0.00, r@1: 0.066, r@5: 0.178, r@10: 0.259, mr: 35\n",
      "epoch: 5\n",
      "True\n",
      "Epoch 6, Epoch status: 0.13, Training loss: 0.0162\n",
      "Epoch 6, Epoch status: 0.25, Training loss: 0.0165\n",
      "Epoch 6, Epoch status: 0.38, Training loss: 0.0165\n",
      "Epoch 6, Epoch status: 0.51, Training loss: 0.0166\n",
      "Epoch 6, Epoch status: 0.63, Training loss: 0.0162\n",
      "Epoch 6, Epoch status: 0.76, Training loss: 0.0162\n",
      "Epoch 6, Epoch status: 0.89, Training loss: 0.0164\n",
      "evaluating epoch 6 ...\n",
      "False\n",
      "MPII - epoch: 6, epoch status: 0.00, r@1: 0.062, r@5: 0.179, r@10: 0.264, mr: 37\n",
      "epoch: 6\n",
      "True\n",
      "Epoch 7, Epoch status: 0.13, Training loss: 0.0147\n",
      "Epoch 7, Epoch status: 0.25, Training loss: 0.0148\n",
      "Epoch 7, Epoch status: 0.38, Training loss: 0.0153\n",
      "Epoch 7, Epoch status: 0.51, Training loss: 0.0148\n",
      "Epoch 7, Epoch status: 0.63, Training loss: 0.0151\n",
      "Epoch 7, Epoch status: 0.76, Training loss: 0.0149\n",
      "Epoch 7, Epoch status: 0.89, Training loss: 0.0152\n",
      "evaluating epoch 7 ...\n",
      "False\n",
      "MPII - epoch: 7, epoch status: 0.00, r@1: 0.063, r@5: 0.191, r@10: 0.272, mr: 35\n",
      "epoch: 7\n",
      "True\n",
      "Epoch 8, Epoch status: 0.13, Training loss: 0.0137\n",
      "Epoch 8, Epoch status: 0.25, Training loss: 0.0140\n",
      "Epoch 8, Epoch status: 0.38, Training loss: 0.0135\n",
      "Epoch 8, Epoch status: 0.51, Training loss: 0.0139\n",
      "Epoch 8, Epoch status: 0.63, Training loss: 0.0141\n",
      "Epoch 8, Epoch status: 0.76, Training loss: 0.0138\n",
      "Epoch 8, Epoch status: 0.89, Training loss: 0.0142\n",
      "evaluating epoch 8 ...\n",
      "False\n",
      "MPII - epoch: 8, epoch status: 0.00, r@1: 0.062, r@5: 0.178, r@10: 0.289, mr: 36\n",
      "epoch: 8\n",
      "True\n",
      "Epoch 9, Epoch status: 0.13, Training loss: 0.0128\n",
      "Epoch 9, Epoch status: 0.25, Training loss: 0.0129\n",
      "Epoch 9, Epoch status: 0.38, Training loss: 0.0129\n",
      "Epoch 9, Epoch status: 0.51, Training loss: 0.0132\n",
      "Epoch 9, Epoch status: 0.63, Training loss: 0.0131\n",
      "Epoch 9, Epoch status: 0.76, Training loss: 0.0131\n",
      "Epoch 9, Epoch status: 0.89, Training loss: 0.0127\n",
      "evaluating epoch 9 ...\n",
      "False\n",
      "MPII - epoch: 9, epoch status: 0.00, r@1: 0.062, r@5: 0.198, r@10: 0.295, mr: 33\n",
      "epoch: 9\n",
      "True\n",
      "Epoch 10, Epoch status: 0.13, Training loss: 0.0118\n",
      "Epoch 10, Epoch status: 0.25, Training loss: 0.0120\n",
      "Epoch 10, Epoch status: 0.38, Training loss: 0.0120\n",
      "Epoch 10, Epoch status: 0.51, Training loss: 0.0122\n",
      "Epoch 10, Epoch status: 0.63, Training loss: 0.0118\n",
      "Epoch 10, Epoch status: 0.76, Training loss: 0.0122\n",
      "Epoch 10, Epoch status: 0.89, Training loss: 0.0124\n",
      "evaluating epoch 10 ...\n",
      "False\n",
      "MPII - epoch: 10, epoch status: 0.00, r@1: 0.061, r@5: 0.214, r@10: 0.307, mr: 34\n",
      "epoch: 10\n",
      "True\n",
      "Epoch 11, Epoch status: 0.13, Training loss: 0.0111\n",
      "Epoch 11, Epoch status: 0.25, Training loss: 0.0113\n",
      "Epoch 11, Epoch status: 0.38, Training loss: 0.0113\n",
      "Epoch 11, Epoch status: 0.51, Training loss: 0.0117\n",
      "Epoch 11, Epoch status: 0.63, Training loss: 0.0117\n",
      "Epoch 11, Epoch status: 0.76, Training loss: 0.0117\n",
      "Epoch 11, Epoch status: 0.89, Training loss: 0.0116\n",
      "evaluating epoch 11 ...\n",
      "False\n",
      "MPII - epoch: 11, epoch status: 0.00, r@1: 0.068, r@5: 0.193, r@10: 0.293, mr: 34\n",
      "epoch: 11\n",
      "True\n",
      "Epoch 12, Epoch status: 0.13, Training loss: 0.0105\n",
      "Epoch 12, Epoch status: 0.25, Training loss: 0.0105\n",
      "Epoch 12, Epoch status: 0.38, Training loss: 0.0108\n",
      "Epoch 12, Epoch status: 0.51, Training loss: 0.0107\n",
      "Epoch 12, Epoch status: 0.63, Training loss: 0.0111\n",
      "Epoch 12, Epoch status: 0.76, Training loss: 0.0111\n",
      "Epoch 12, Epoch status: 0.89, Training loss: 0.0110\n",
      "evaluating epoch 12 ...\n",
      "False\n",
      "MPII - epoch: 12, epoch status: 0.00, r@1: 0.068, r@5: 0.193, r@10: 0.293, mr: 36\n",
      "epoch: 12\n",
      "True\n",
      "Epoch 13, Epoch status: 0.13, Training loss: 0.0098\n",
      "Epoch 13, Epoch status: 0.25, Training loss: 0.0102\n",
      "Epoch 13, Epoch status: 0.38, Training loss: 0.0101\n",
      "Epoch 13, Epoch status: 0.51, Training loss: 0.0103\n",
      "Epoch 13, Epoch status: 0.63, Training loss: 0.0103\n",
      "Epoch 13, Epoch status: 0.76, Training loss: 0.0105\n",
      "Epoch 13, Epoch status: 0.89, Training loss: 0.0105\n",
      "evaluating epoch 13 ...\n",
      "False\n",
      "MPII - epoch: 13, epoch status: 0.00, r@1: 0.071, r@5: 0.199, r@10: 0.282, mr: 39\n",
      "epoch: 13\n",
      "True\n",
      "Epoch 14, Epoch status: 0.13, Training loss: 0.0096\n",
      "Epoch 14, Epoch status: 0.25, Training loss: 0.0094\n",
      "Epoch 14, Epoch status: 0.38, Training loss: 0.0098\n",
      "Epoch 14, Epoch status: 0.51, Training loss: 0.0099\n",
      "Epoch 14, Epoch status: 0.63, Training loss: 0.0099\n",
      "Epoch 14, Epoch status: 0.76, Training loss: 0.0098\n",
      "Epoch 14, Epoch status: 0.89, Training loss: 0.0099\n",
      "evaluating epoch 14 ...\n",
      "False\n",
      "MPII - epoch: 14, epoch status: 0.00, r@1: 0.073, r@5: 0.204, r@10: 0.285, mr: 34\n",
      "epoch: 14\n",
      "True\n",
      "Epoch 15, Epoch status: 0.13, Training loss: 0.0089\n",
      "Epoch 15, Epoch status: 0.25, Training loss: 0.0091\n",
      "Epoch 15, Epoch status: 0.38, Training loss: 0.0092\n",
      "Epoch 15, Epoch status: 0.51, Training loss: 0.0093\n",
      "Epoch 15, Epoch status: 0.63, Training loss: 0.0094\n",
      "Epoch 15, Epoch status: 0.76, Training loss: 0.0096\n",
      "Epoch 15, Epoch status: 0.89, Training loss: 0.0095\n",
      "evaluating epoch 15 ...\n",
      "False\n",
      "MPII - epoch: 15, epoch status: 0.00, r@1: 0.064, r@5: 0.206, r@10: 0.286, mr: 38\n",
      "epoch: 15\n",
      "True\n",
      "Epoch 16, Epoch status: 0.13, Training loss: 0.0085\n",
      "Epoch 16, Epoch status: 0.25, Training loss: 0.0087\n",
      "Epoch 16, Epoch status: 0.38, Training loss: 0.0091\n",
      "Epoch 16, Epoch status: 0.51, Training loss: 0.0091\n",
      "Epoch 16, Epoch status: 0.63, Training loss: 0.0090\n",
      "Epoch 16, Epoch status: 0.76, Training loss: 0.0090\n",
      "Epoch 16, Epoch status: 0.89, Training loss: 0.0090\n",
      "evaluating epoch 16 ...\n",
      "False\n",
      "MPII - epoch: 16, epoch status: 0.00, r@1: 0.079, r@5: 0.204, r@10: 0.295, mr: 36\n",
      "epoch: 16\n",
      "True\n",
      "Epoch 17, Epoch status: 0.13, Training loss: 0.0083\n",
      "Epoch 17, Epoch status: 0.25, Training loss: 0.0084\n",
      "Epoch 17, Epoch status: 0.38, Training loss: 0.0084\n",
      "Epoch 17, Epoch status: 0.51, Training loss: 0.0085\n",
      "Epoch 17, Epoch status: 0.63, Training loss: 0.0087\n",
      "Epoch 17, Epoch status: 0.76, Training loss: 0.0086\n",
      "Epoch 17, Epoch status: 0.89, Training loss: 0.0087\n",
      "evaluating epoch 17 ...\n",
      "False\n",
      "MPII - epoch: 17, epoch status: 0.00, r@1: 0.068, r@5: 0.210, r@10: 0.294, mr: 35\n",
      "epoch: 17\n",
      "True\n",
      "Epoch 18, Epoch status: 0.13, Training loss: 0.0079\n",
      "Epoch 18, Epoch status: 0.25, Training loss: 0.0080\n",
      "Epoch 18, Epoch status: 0.38, Training loss: 0.0082\n",
      "Epoch 18, Epoch status: 0.51, Training loss: 0.0082\n",
      "Epoch 18, Epoch status: 0.63, Training loss: 0.0082\n",
      "Epoch 18, Epoch status: 0.76, Training loss: 0.0083\n",
      "Epoch 18, Epoch status: 0.89, Training loss: 0.0084\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating epoch 18 ...\n",
      "False\n",
      "MPII - epoch: 18, epoch status: 0.00, r@1: 0.070, r@5: 0.198, r@10: 0.280, mr: 35\n",
      "epoch: 18\n",
      "True\n",
      "Epoch 19, Epoch status: 0.13, Training loss: 0.0076\n",
      "Epoch 19, Epoch status: 0.25, Training loss: 0.0077\n",
      "Epoch 19, Epoch status: 0.38, Training loss: 0.0078\n",
      "Epoch 19, Epoch status: 0.51, Training loss: 0.0080\n",
      "Epoch 19, Epoch status: 0.63, Training loss: 0.0080\n",
      "Epoch 19, Epoch status: 0.76, Training loss: 0.0081\n",
      "Epoch 19, Epoch status: 0.89, Training loss: 0.0080\n",
      "evaluating epoch 19 ...\n",
      "False\n",
      "MPII - epoch: 19, epoch status: 0.00, r@1: 0.073, r@5: 0.200, r@10: 0.297, mr: 36\n",
      "epoch: 19\n",
      "True\n",
      "Epoch 20, Epoch status: 0.13, Training loss: 0.0074\n",
      "Epoch 20, Epoch status: 0.25, Training loss: 0.0075\n",
      "Epoch 20, Epoch status: 0.38, Training loss: 0.0076\n",
      "Epoch 20, Epoch status: 0.51, Training loss: 0.0076\n",
      "Epoch 20, Epoch status: 0.63, Training loss: 0.0076\n",
      "Epoch 20, Epoch status: 0.76, Training loss: 0.0078\n",
      "Epoch 20, Epoch status: 0.89, Training loss: 0.0078\n",
      "evaluating epoch 20 ...\n",
      "False\n",
      "MPII - epoch: 20, epoch status: 0.00, r@1: 0.066, r@5: 0.201, r@10: 0.278, mr: 34\n",
      "epoch: 20\n",
      "True\n",
      "Epoch 21, Epoch status: 0.13, Training loss: 0.0070\n",
      "Epoch 21, Epoch status: 0.25, Training loss: 0.0074\n",
      "Epoch 21, Epoch status: 0.38, Training loss: 0.0073\n",
      "Epoch 21, Epoch status: 0.51, Training loss: 0.0074\n",
      "Epoch 21, Epoch status: 0.63, Training loss: 0.0075\n",
      "Epoch 21, Epoch status: 0.76, Training loss: 0.0074\n",
      "Epoch 21, Epoch status: 0.89, Training loss: 0.0076\n",
      "evaluating epoch 21 ...\n",
      "False\n",
      "MPII - epoch: 21, epoch status: 0.00, r@1: 0.076, r@5: 0.207, r@10: 0.289, mr: 35\n",
      "epoch: 21\n",
      "True\n",
      "Epoch 22, Epoch status: 0.13, Training loss: 0.0069\n",
      "Epoch 22, Epoch status: 0.25, Training loss: 0.0069\n",
      "Epoch 22, Epoch status: 0.38, Training loss: 0.0070\n",
      "Epoch 22, Epoch status: 0.51, Training loss: 0.0072\n",
      "Epoch 22, Epoch status: 0.63, Training loss: 0.0074\n",
      "Epoch 22, Epoch status: 0.76, Training loss: 0.0072\n",
      "Epoch 22, Epoch status: 0.89, Training loss: 0.0073\n",
      "evaluating epoch 22 ...\n",
      "False\n",
      "MPII - epoch: 22, epoch status: 0.00, r@1: 0.077, r@5: 0.194, r@10: 0.278, mr: 34\n",
      "epoch: 22\n",
      "True\n",
      "Epoch 23, Epoch status: 0.13, Training loss: 0.0066\n",
      "Epoch 23, Epoch status: 0.25, Training loss: 0.0068\n",
      "Epoch 23, Epoch status: 0.38, Training loss: 0.0068\n",
      "Epoch 23, Epoch status: 0.51, Training loss: 0.0069\n",
      "Epoch 23, Epoch status: 0.63, Training loss: 0.0071\n",
      "Epoch 23, Epoch status: 0.76, Training loss: 0.0070\n",
      "Epoch 23, Epoch status: 0.89, Training loss: 0.0069\n",
      "evaluating epoch 23 ...\n",
      "False\n",
      "MPII - epoch: 23, epoch status: 0.00, r@1: 0.075, r@5: 0.198, r@10: 0.286, mr: 37\n",
      "epoch: 23\n",
      "True\n",
      "Epoch 24, Epoch status: 0.13, Training loss: 0.0065\n",
      "Epoch 24, Epoch status: 0.25, Training loss: 0.0065\n",
      "Epoch 24, Epoch status: 0.38, Training loss: 0.0068\n",
      "Epoch 24, Epoch status: 0.51, Training loss: 0.0068\n",
      "Epoch 24, Epoch status: 0.63, Training loss: 0.0068\n",
      "Epoch 24, Epoch status: 0.76, Training loss: 0.0068\n",
      "Epoch 24, Epoch status: 0.89, Training loss: 0.0069\n",
      "evaluating epoch 24 ...\n",
      "False\n",
      "MPII - epoch: 24, epoch status: 0.00, r@1: 0.067, r@5: 0.195, r@10: 0.287, mr: 33\n",
      "epoch: 24\n",
      "True\n",
      "Epoch 25, Epoch status: 0.13, Training loss: 0.0063\n",
      "Epoch 25, Epoch status: 0.25, Training loss: 0.0063\n",
      "Epoch 25, Epoch status: 0.38, Training loss: 0.0065\n",
      "Epoch 25, Epoch status: 0.51, Training loss: 0.0064\n",
      "Epoch 25, Epoch status: 0.63, Training loss: 0.0066\n",
      "Epoch 25, Epoch status: 0.76, Training loss: 0.0067\n",
      "Epoch 25, Epoch status: 0.89, Training loss: 0.0067\n",
      "evaluating epoch 25 ...\n",
      "False\n",
      "MPII - epoch: 25, epoch status: 0.00, r@1: 0.064, r@5: 0.194, r@10: 0.287, mr: 34\n",
      "epoch: 25\n",
      "True\n",
      "Epoch 26, Epoch status: 0.13, Training loss: 0.0061\n",
      "Epoch 26, Epoch status: 0.25, Training loss: 0.0061\n",
      "Epoch 26, Epoch status: 0.38, Training loss: 0.0064\n",
      "Epoch 26, Epoch status: 0.51, Training loss: 0.0063\n",
      "Epoch 26, Epoch status: 0.63, Training loss: 0.0064\n",
      "Epoch 26, Epoch status: 0.76, Training loss: 0.0064\n",
      "Epoch 26, Epoch status: 0.89, Training loss: 0.0065\n",
      "evaluating epoch 26 ...\n",
      "False\n",
      "MPII - epoch: 26, epoch status: 0.00, r@1: 0.070, r@5: 0.203, r@10: 0.281, mr: 36\n",
      "epoch: 26\n",
      "True\n",
      "Epoch 27, Epoch status: 0.13, Training loss: 0.0059\n",
      "Epoch 27, Epoch status: 0.25, Training loss: 0.0060\n",
      "Epoch 27, Epoch status: 0.38, Training loss: 0.0061\n",
      "Epoch 27, Epoch status: 0.51, Training loss: 0.0061\n",
      "Epoch 27, Epoch status: 0.63, Training loss: 0.0062\n",
      "Epoch 27, Epoch status: 0.76, Training loss: 0.0063\n",
      "Epoch 27, Epoch status: 0.89, Training loss: 0.0064\n",
      "evaluating epoch 27 ...\n",
      "False\n",
      "MPII - epoch: 27, epoch status: 0.00, r@1: 0.069, r@5: 0.194, r@10: 0.276, mr: 36\n",
      "epoch: 27\n",
      "True\n",
      "Epoch 28, Epoch status: 0.13, Training loss: 0.0059\n",
      "Epoch 28, Epoch status: 0.25, Training loss: 0.0060\n",
      "Epoch 28, Epoch status: 0.38, Training loss: 0.0060\n",
      "Epoch 28, Epoch status: 0.51, Training loss: 0.0061\n",
      "Epoch 28, Epoch status: 0.63, Training loss: 0.0061\n",
      "Epoch 28, Epoch status: 0.76, Training loss: 0.0062\n",
      "Epoch 28, Epoch status: 0.89, Training loss: 0.0061\n",
      "evaluating epoch 28 ...\n",
      "False\n",
      "MPII - epoch: 28, epoch status: 0.00, r@1: 0.075, r@5: 0.194, r@10: 0.273, mr: 37\n",
      "epoch: 28\n",
      "True\n",
      "Epoch 29, Epoch status: 0.13, Training loss: 0.0056\n",
      "Epoch 29, Epoch status: 0.25, Training loss: 0.0058\n",
      "Epoch 29, Epoch status: 0.38, Training loss: 0.0058\n",
      "Epoch 29, Epoch status: 0.51, Training loss: 0.0058\n",
      "Epoch 29, Epoch status: 0.63, Training loss: 0.0059\n",
      "Epoch 29, Epoch status: 0.76, Training loss: 0.0060\n",
      "Epoch 29, Epoch status: 0.89, Training loss: 0.0060\n",
      "evaluating epoch 29 ...\n",
      "False\n",
      "MPII - epoch: 29, epoch status: 0.00, r@1: 0.073, r@5: 0.200, r@10: 0.265, mr: 36\n",
      "epoch: 29\n",
      "True\n",
      "Epoch 30, Epoch status: 0.13, Training loss: 0.0055\n",
      "Epoch 30, Epoch status: 0.25, Training loss: 0.0058\n",
      "Epoch 30, Epoch status: 0.38, Training loss: 0.0057\n",
      "Epoch 30, Epoch status: 0.51, Training loss: 0.0057\n",
      "Epoch 30, Epoch status: 0.63, Training loss: 0.0058\n",
      "Epoch 30, Epoch status: 0.76, Training loss: 0.0059\n",
      "Epoch 30, Epoch status: 0.89, Training loss: 0.0059\n",
      "evaluating epoch 30 ...\n",
      "False\n",
      "MPII - epoch: 30, epoch status: 0.00, r@1: 0.077, r@5: 0.195, r@10: 0.278, mr: 36\n",
      "epoch: 30\n",
      "True\n",
      "Epoch 31, Epoch status: 0.13, Training loss: 0.0055\n",
      "Epoch 31, Epoch status: 0.25, Training loss: 0.0055\n",
      "Epoch 31, Epoch status: 0.38, Training loss: 0.0056\n",
      "Epoch 31, Epoch status: 0.51, Training loss: 0.0056\n",
      "Epoch 31, Epoch status: 0.63, Training loss: 0.0056\n",
      "Epoch 31, Epoch status: 0.76, Training loss: 0.0057\n",
      "Epoch 31, Epoch status: 0.89, Training loss: 0.0057\n",
      "evaluating epoch 31 ...\n",
      "False\n",
      "MPII - epoch: 31, epoch status: 0.00, r@1: 0.070, r@5: 0.190, r@10: 0.275, mr: 37\n",
      "epoch: 31\n",
      "True\n",
      "Epoch 32, Epoch status: 0.13, Training loss: 0.0053\n",
      "Epoch 32, Epoch status: 0.25, Training loss: 0.0054\n",
      "Epoch 32, Epoch status: 0.38, Training loss: 0.0056\n",
      "Epoch 32, Epoch status: 0.51, Training loss: 0.0056\n",
      "Epoch 32, Epoch status: 0.63, Training loss: 0.0055\n",
      "Epoch 32, Epoch status: 0.76, Training loss: 0.0056\n",
      "Epoch 32, Epoch status: 0.89, Training loss: 0.0056\n",
      "evaluating epoch 32 ...\n",
      "False\n",
      "MPII - epoch: 32, epoch status: 0.00, r@1: 0.074, r@5: 0.197, r@10: 0.286, mr: 35\n",
      "epoch: 32\n",
      "True\n",
      "Epoch 33, Epoch status: 0.13, Training loss: 0.0053\n",
      "Epoch 33, Epoch status: 0.25, Training loss: 0.0053\n",
      "Epoch 33, Epoch status: 0.38, Training loss: 0.0055\n",
      "Epoch 33, Epoch status: 0.51, Training loss: 0.0055\n",
      "Epoch 33, Epoch status: 0.63, Training loss: 0.0055\n",
      "Epoch 33, Epoch status: 0.76, Training loss: 0.0054\n",
      "Epoch 33, Epoch status: 0.89, Training loss: 0.0055\n",
      "evaluating epoch 33 ...\n",
      "False\n",
      "MPII - epoch: 33, epoch status: 0.00, r@1: 0.070, r@5: 0.195, r@10: 0.282, mr: 36\n",
      "epoch: 33\n",
      "True\n",
      "Epoch 34, Epoch status: 0.13, Training loss: 0.0051\n",
      "Epoch 34, Epoch status: 0.25, Training loss: 0.0052\n",
      "Epoch 34, Epoch status: 0.38, Training loss: 0.0053\n",
      "Epoch 34, Epoch status: 0.51, Training loss: 0.0053\n",
      "Epoch 34, Epoch status: 0.63, Training loss: 0.0054\n",
      "Epoch 34, Epoch status: 0.76, Training loss: 0.0053\n",
      "Epoch 34, Epoch status: 0.89, Training loss: 0.0055\n",
      "evaluating epoch 34 ...\n",
      "False\n",
      "MPII - epoch: 34, epoch status: 0.00, r@1: 0.073, r@5: 0.192, r@10: 0.280, mr: 37\n",
      "epoch: 34\n",
      "True\n",
      "Epoch 35, Epoch status: 0.13, Training loss: 0.0051\n",
      "Epoch 35, Epoch status: 0.25, Training loss: 0.0051\n",
      "Epoch 35, Epoch status: 0.38, Training loss: 0.0052\n"
     ]
    }
   ],
   "source": [
    "print ('Starting training loop ...')\n",
    "\n",
    "#for epoch in range(args.epochs):\n",
    "    \n",
    "for epoch in range(args.epochs):\n",
    "#     net.train()\n",
    "#     #net.train(False)\n",
    "    net.train()\n",
    "    running_loss = 0.0\n",
    "    print ('epoch: %d'%epoch)\n",
    "    print (net.training)\n",
    "\n",
    "    for i_batch, sample_batched in enumerate(dataloader):\n",
    "\n",
    "        if args.MSRVTT:\n",
    "            captions = sample_batched['text']\n",
    "            audio = sample_batched['audio']\n",
    "        else:\n",
    "            captions = dataset.shorteningTextTensor(sample_batched['text'],\n",
    "                    sample_batched['text_size']) #[128,30,300] ==>[128,28,300] , max length 30\n",
    "                                                 # the longest text length is 28 in this batch\n",
    "            \n",
    "            audio = dataset.shorteningTextTensor(sample_batched['audio'],\n",
    "                    sample_batched['audio_size']) #[128, 183, 128]==> [128, 16, 128], max length 183\n",
    "                                                  # the longest text length is 16 in this batch\n",
    "       \n",
    "\n",
    "        face = sample_batched['face'] #[128,128]\n",
    "        video = sample_batched['video']  #[128,2048]\n",
    "        flow = sample_batched['flow'] #[128,1024]\n",
    "        coco_ind = sample_batched['coco_ind'] #[128], 0 indicates not coco image\n",
    "        face_ind = sample_batched['face_ind'] #[128], 0 indicates no face descriptor\n",
    "\n",
    "        ind = {}\n",
    "        ind['face'] = face_ind  # [128]\n",
    "        #ind['face'] = np.zeros((len(face_ind))) # [128]\n",
    "        ind['visual'] = np.ones((len(face_ind))) # all one mask for 'visual'\n",
    "        #ind['visual'] = np.zeros((len(face_ind))) # all one mask for 'visual'\n",
    "        #ind['motion'] = 1 - coco_ind # lsmdb has one for motion and audio,\n",
    "        ind['motion'] = coco_ind # lsmdb has one for motion and audio,\n",
    "        ind['audio'] = 1 - coco_ind # coco is 0 since no motion/audio\n",
    "        #ind['audio'] = coco_ind # coco is 0 since no motion/audio\n",
    "\n",
    "        if args.GPU:\n",
    "            captions, video = Variable(captions.cuda()), Variable(video.cuda())\n",
    "            audio, flow  =  Variable(audio.cuda()), Variable(flow.cuda())\n",
    "            face = Variable(face.cuda())\n",
    "\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        confusion_matrix = net(captions,\n",
    "                {'face': face, 'audio': audio, 'visual': video, 'motion': flow}, ind, True)\n",
    "        loss = max_margin(confusion_matrix)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        running_loss += loss.data[0]\n",
    "        #print(loss.data[0])\n",
    "        #if i_batch==2:\n",
    "        #    break\n",
    "        \n",
    "        if (i_batch+1) % n_display == 0:\n",
    "            print ('Epoch %d, Epoch status: %.2f, Training loss: %.4f'%(epoch + 1,\n",
    "                    args.batch_size*float(i_batch)/dataset_size,running_loss/n_display))\n",
    "            running_loss = 0.0\n",
    "\n",
    "    print ('evaluating epoch %d ...'%(epoch+1))\n",
    "    net.eval()  \n",
    "    with torch.no_grad():\n",
    "    #if True:\n",
    "        if args.MSRVTT:\n",
    "            retrieval_samples = dataset.getRetrievalSamples()\n",
    "\n",
    "            video = Variable(retrieval_samples['video'].cuda(), volatile=True)\n",
    "            captions = Variable(retrieval_samples['text'].cuda(), volatile=True)\n",
    "            audio = Variable(retrieval_samples['audio'].cuda(), volatile=True)\n",
    "            flow = Variable(retrieval_samples['flow'].cuda(), volatile=True)\n",
    "            face = Variable(retrieval_samples['face'].cuda(), volatile=True)\n",
    "            face_ind = retrieval_samples['face_ind']\n",
    "\n",
    "            ind = {}\n",
    "            ind['face'] = face_ind\n",
    "            ind['visual'] = np.ones((len(face_ind)))\n",
    "            ind['motion'] = np.ones((len(face_ind)))\n",
    "            ind['audio'] = np.ones((len(face_ind)))\n",
    "\n",
    "\n",
    "            conf = net(captions,\n",
    "                    {'face': face, 'audio': audio, 'visual': video, 'motion': flow}, ind, True)\n",
    "            confusion_matrix = conf.data.cpu().float().numpy() #[1000,1000] ==>change tensor to numpy\n",
    "            metrics = compute_metric_yang(confusion_matrix)\n",
    "            verbose(epoch, args.batch_size*float(i_batch)/dataset_size, metrics, name='MSRVTT')\n",
    "\n",
    "        else:\n",
    "            i_batch=1\n",
    "            print (net.training)\n",
    "            video = Variable(vid_retrieval_val.cuda(), volatile=True)\n",
    "            captions = Variable(text_retrieval_val.cuda(), volatile=True)\n",
    "            audio = Variable(audio_retrieval_val.cuda(), volatile=True)\n",
    "            flow = Variable(flow_retrieval_val.cuda(), volatile=True)\n",
    "            face = Variable(face_retrieval_val.cuda(), volatile=True)\n",
    "\n",
    "            ind = {}\n",
    "            ind['face'] = face_ind_test\n",
    "            #ind['face'] = np.zeros((len(face_ind_test)))\n",
    "            ind['visual'] = np.ones((len(face_ind_test)))\n",
    "            ind['motion'] = np.zeros((len(face_ind_test)))\n",
    "            #ind['motion'] = np.ones((len(face_ind_test)))\n",
    "            #ind['audio'] = np.ones((len(face_ind_test)))\n",
    "            ind['audio'] = np.zeros((len(face_ind_test)))\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            conf = net(captions,\n",
    "                    {'face': face, 'audio': audio, 'visual': video, 'motion': flow}, ind, True)\n",
    "            confusion_matrix = conf.data.cpu().float().numpy()\n",
    "            metrics = compute_metric_yang(confusion_matrix)\n",
    "            #verbose(epoch, args.batch_size*float(i_batch)/dataset_size, metrics, name='MPII')\n",
    "            verbose(epoch, args.batch_size*float(i_batch)/dataset_size, metrics, name='MPII')\n",
    "\n",
    "\n",
    "    #net.train()\n",
    "    \n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] *= lr_decay\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on built-in function sum:\n",
      "\n",
      "sum(...)\n",
      "    .. function:: sum(input, dtype=None) -> Tensor\n",
      "    \n",
      "    Returns the sum of all elements in the :attr:`input` tensor.\n",
      "    \n",
      "    Args:\n",
      "        input (Tensor): the input tensor\n",
      "        dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor.\n",
      "            If specified, the input tensor is casted to :attr:`dtype` before the operation\n",
      "            is performed. This is useful for preventing data type overflows. Default: None.\n",
      "    \n",
      "    Example::\n",
      "    \n",
      "        >>> a = torch.randn(1, 3)\n",
      "        >>> a\n",
      "        tensor([[ 0.1133, -0.9567,  0.2958]])\n",
      "        >>> torch.sum(a)\n",
      "        tensor(-0.5475)\n",
      "    \n",
      "    .. function:: sum(input, dim, keepdim=False, dtype=None) -> Tensor\n",
      "    \n",
      "    Returns the sum of each row of the :attr:`input` tensor in the given\n",
      "    dimension :attr:`dim`. If :attr::`dim` is a list of dimensions,\n",
      "    reduce over all of them.\n",
      "    \n",
      "    If :attr:`keepdim` is ``True``, the output tensor is of the same size\n",
      "    as :attr:`input` except in the dimension :attr:`dim` where it is of size 1.\n",
      "    Otherwise, :attr:`dim` is squeezed (see :func:`torch.squeeze`), resulting in\n",
      "    the output tensor having 1 fewer dimension than :attr:`input`.\n",
      "    \n",
      "    Args:\n",
      "        input (Tensor): the input tensor\n",
      "        dim (int or tuple of ints): the dimension or dimensions to reduce\n",
      "        keepdim (bool): whether the output tensor has :attr:`dim` retained or not\n",
      "        dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor.\n",
      "            If specified, the input tensor is casted to :attr:`dtype` before the operation\n",
      "            is performed. This is useful for preventing data type overflows. Default: None.\n",
      "    \n",
      "    Example::\n",
      "    \n",
      "        >>> a = torch.randn(4, 4)\n",
      "        >>> a\n",
      "        tensor([[ 0.0569, -0.2475,  0.0737, -0.3429],\n",
      "                [-0.2993,  0.9138,  0.9337, -1.6864],\n",
      "                [ 0.1132,  0.7892, -0.1003,  0.5688],\n",
      "                [ 0.3637, -0.9906, -0.4752, -1.5197]])\n",
      "        >>> torch.sum(a, 1)\n",
      "        tensor([-0.4598, -0.1381,  1.3708, -2.6217])\n",
      "        >>> b = torch.arange(4 * 5 * 6).view(4, 5, 6)\n",
      "        >>> torch.sum(b, (2, 1))\n",
      "        tensor([  435.,  1335.,  2235.,  3135.])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(th.sum)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aa=confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sx = np.sort(aa, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = np.diag(aa) #(1000)\n",
    "d = d[:,np.newaxis] #(1000,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = sx - d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_pos = np.where(ind == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_pos[1][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metric_yang(x):\n",
    "    sx = np.sort(x, axis=1)\n",
    "    d = np.diag(x) #(1000)\n",
    "    d = d[:,np.newaxis] #(1000,1)\n",
    "    ind = sx - d\n",
    "    ind = np.where(ind == 0)\n",
    "    ind = ind[1]\n",
    "\n",
    "    metrics = {}\n",
    "    metrics['R1'] = float(np.sum(ind == 0))/len(ind)\n",
    "    metrics['R5'] = float(np.sum(ind < 5))/len(ind)\n",
    "    metrics['R10'] = float(np.sum(ind < 10))/len(ind)\n",
    "    metrics['MR'] = np.median(ind) + 1\n",
    "\n",
    "    return metrics\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " metrics_yang = compute_metric_distance(confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(GPU=True, MSRVTT=False, batch_size=128, coco=False, coco_sampling_rate=1.0, epochs=50, eval_qcm=False, lr=0.0001, lr_decay=0.95, margin=0.2, model_name='test', momentum=0.9, n_cpu=4, n_display=100, optimizer='adam', seed=1, text_cluster_size=32)\n",
      "Pre-loading features ... This may takes several minutes ...\n",
      "Done.\n",
      "Reading test data ...\n",
      "Done.\n",
      "Starting training loop ...\n",
      "epoch: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/shared/nfs1/yangl/code/VideoText/VideoRetrival/train.py:275: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "  running_loss += loss.data[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Epoch status: 0.13, Training loss: 0.0950\n",
      "Epoch 1, Epoch status: 0.25, Training loss: 0.0624\n",
      "Epoch 1, Epoch status: 0.38, Training loss: 0.0544\n",
      "Epoch 1, Epoch status: 0.51, Training loss: 0.0516\n",
      "Epoch 1, Epoch status: 0.63, Training loss: 0.0484\n",
      "Epoch 1, Epoch status: 0.76, Training loss: 0.0474\n",
      "Epoch 1, Epoch status: 0.89, Training loss: 0.0458\n",
      "evaluating epoch 1 ...\n",
      "MPII - epoch: 1, epoch status: 1.00, r@1: 0.079, r@5: 0.196, r@10: 0.276, mr: 37\n",
      "epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/shared/nfs1/yangl/code/VideoText/VideoRetrival/train.py:308: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  video = Variable(vid_retrieval_val.cuda(), volatile=True)\n",
      "/scratch/shared/nfs1/yangl/code/VideoText/VideoRetrival/train.py:309: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  captions = Variable(text_retrieval_val.cuda(), volatile=True)\n",
      "/scratch/shared/nfs1/yangl/code/VideoText/VideoRetrival/train.py:310: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  audio = Variable(audio_retrieval_val.cuda(), volatile=True)\n",
      "/scratch/shared/nfs1/yangl/code/VideoText/VideoRetrival/train.py:311: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  flow = Variable(flow_retrieval_val.cuda(), volatile=True)\n",
      "/scratch/shared/nfs1/yangl/code/VideoText/VideoRetrival/train.py:312: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  face = Variable(face_retrieval_val.cuda(), volatile=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Epoch status: 0.13, Training loss: 0.0391\n",
      "Epoch 2, Epoch status: 0.25, Training loss: 0.0389\n",
      "Epoch 2, Epoch status: 0.38, Training loss: 0.0383\n",
      "Epoch 2, Epoch status: 0.51, Training loss: 0.0383\n",
      "Epoch 2, Epoch status: 0.63, Training loss: 0.0371\n",
      "Epoch 2, Epoch status: 0.76, Training loss: 0.0382\n",
      "Epoch 2, Epoch status: 0.89, Training loss: 0.0386\n",
      "evaluating epoch 2 ...\n",
      "MPII - epoch: 2, epoch status: 1.00, r@1: 0.092, r@5: 0.236, r@10: 0.323, mr: 32\n",
      "epoch: 2\n",
      "Epoch 3, Epoch status: 0.13, Training loss: 0.0326\n",
      "Epoch 3, Epoch status: 0.25, Training loss: 0.0328\n",
      "Epoch 3, Epoch status: 0.38, Training loss: 0.0327\n",
      "Epoch 3, Epoch status: 0.51, Training loss: 0.0328\n",
      "Epoch 3, Epoch status: 0.63, Training loss: 0.0324\n",
      "Epoch 3, Epoch status: 0.76, Training loss: 0.0325\n",
      "Epoch 3, Epoch status: 0.89, Training loss: 0.0326\n",
      "evaluating epoch 3 ...\n",
      "MPII - epoch: 3, epoch status: 1.00, r@1: 0.095, r@5: 0.257, r@10: 0.343, mr: 28\n",
      "epoch: 3\n",
      "Epoch 4, Epoch status: 0.13, Training loss: 0.0280\n",
      "Epoch 4, Epoch status: 0.25, Training loss: 0.0278\n",
      "Epoch 4, Epoch status: 0.38, Training loss: 0.0287\n",
      "Epoch 4, Epoch status: 0.51, Training loss: 0.0291\n",
      "Epoch 4, Epoch status: 0.63, Training loss: 0.0290\n",
      "Epoch 4, Epoch status: 0.76, Training loss: 0.0292\n",
      "Epoch 4, Epoch status: 0.89, Training loss: 0.0289\n",
      "evaluating epoch 4 ...\n",
      "MPII - epoch: 4, epoch status: 1.00, r@1: 0.110, r@5: 0.264, r@10: 0.362, mr: 24\n",
      "epoch: 4\n",
      "Epoch 5, Epoch status: 0.13, Training loss: 0.0248\n",
      "Epoch 5, Epoch status: 0.25, Training loss: 0.0249\n",
      "Epoch 5, Epoch status: 0.38, Training loss: 0.0253\n",
      "Epoch 5, Epoch status: 0.51, Training loss: 0.0259\n",
      "Epoch 5, Epoch status: 0.63, Training loss: 0.0254\n",
      "Epoch 5, Epoch status: 0.76, Training loss: 0.0264\n",
      "Epoch 5, Epoch status: 0.89, Training loss: 0.0263\n",
      "evaluating epoch 5 ...\n",
      "MPII - epoch: 5, epoch status: 1.00, r@1: 0.108, r@5: 0.277, r@10: 0.357, mr: 26\n",
      "epoch: 5\n",
      "Epoch 6, Epoch status: 0.13, Training loss: 0.0217\n",
      "Epoch 6, Epoch status: 0.25, Training loss: 0.0224\n",
      "Epoch 6, Epoch status: 0.38, Training loss: 0.0227\n",
      "Epoch 6, Epoch status: 0.51, Training loss: 0.0226\n",
      "Epoch 6, Epoch status: 0.63, Training loss: 0.0237\n",
      "Epoch 6, Epoch status: 0.76, Training loss: 0.0233\n",
      "Epoch 6, Epoch status: 0.89, Training loss: 0.0231\n",
      "evaluating epoch 6 ...\n",
      "MPII - epoch: 6, epoch status: 1.00, r@1: 0.118, r@5: 0.272, r@10: 0.371, mr: 22\n",
      "epoch: 6\n",
      "Epoch 7, Epoch status: 0.13, Training loss: 0.0194\n",
      "Epoch 7, Epoch status: 0.25, Training loss: 0.0201\n",
      "Epoch 7, Epoch status: 0.38, Training loss: 0.0204\n",
      "Epoch 7, Epoch status: 0.51, Training loss: 0.0205\n",
      "Epoch 7, Epoch status: 0.63, Training loss: 0.0208\n",
      "Epoch 7, Epoch status: 0.76, Training loss: 0.0211\n",
      "Epoch 7, Epoch status: 0.89, Training loss: 0.0215\n",
      "evaluating epoch 7 ...\n",
      "MPII - epoch: 7, epoch status: 1.00, r@1: 0.125, r@5: 0.267, r@10: 0.380, mr: 22\n",
      "epoch: 7\n",
      "Epoch 8, Epoch status: 0.13, Training loss: 0.0178\n",
      "Epoch 8, Epoch status: 0.25, Training loss: 0.0182\n",
      "Epoch 8, Epoch status: 0.38, Training loss: 0.0184\n",
      "Epoch 8, Epoch status: 0.51, Training loss: 0.0185\n",
      "Epoch 8, Epoch status: 0.63, Training loss: 0.0187\n",
      "Epoch 8, Epoch status: 0.76, Training loss: 0.0189\n",
      "Epoch 8, Epoch status: 0.89, Training loss: 0.0192\n",
      "evaluating epoch 8 ...\n",
      "MPII - epoch: 8, epoch status: 1.00, r@1: 0.124, r@5: 0.281, r@10: 0.372, mr: 22\n",
      "epoch: 8\n",
      "Epoch 9, Epoch status: 0.13, Training loss: 0.0157\n",
      "Epoch 9, Epoch status: 0.25, Training loss: 0.0160\n",
      "Epoch 9, Epoch status: 0.38, Training loss: 0.0165\n",
      "Epoch 9, Epoch status: 0.51, Training loss: 0.0169\n",
      "Epoch 9, Epoch status: 0.63, Training loss: 0.0172\n",
      "Epoch 9, Epoch status: 0.76, Training loss: 0.0173\n",
      "Epoch 9, Epoch status: 0.89, Training loss: 0.0172\n",
      "evaluating epoch 9 ...\n",
      "MPII - epoch: 9, epoch status: 1.00, r@1: 0.122, r@5: 0.274, r@10: 0.369, mr: 22\n",
      "epoch: 9\n",
      "Epoch 10, Epoch status: 0.13, Training loss: 0.0142\n",
      "Epoch 10, Epoch status: 0.25, Training loss: 0.0145\n",
      "Epoch 10, Epoch status: 0.38, Training loss: 0.0148\n",
      "Epoch 10, Epoch status: 0.51, Training loss: 0.0153\n",
      "Epoch 10, Epoch status: 0.63, Training loss: 0.0159\n",
      "Epoch 10, Epoch status: 0.76, Training loss: 0.0159\n",
      "Epoch 10, Epoch status: 0.89, Training loss: 0.0160\n",
      "evaluating epoch 10 ...\n",
      "MPII - epoch: 10, epoch status: 1.00, r@1: 0.131, r@5: 0.288, r@10: 0.375, mr: 23\n",
      "epoch: 10\n",
      "Epoch 11, Epoch status: 0.13, Training loss: 0.0127\n",
      "Epoch 11, Epoch status: 0.25, Training loss: 0.0133\n",
      "Epoch 11, Epoch status: 0.38, Training loss: 0.0136\n",
      "Epoch 11, Epoch status: 0.51, Training loss: 0.0141\n",
      "Epoch 11, Epoch status: 0.63, Training loss: 0.0138\n",
      "Epoch 11, Epoch status: 0.76, Training loss: 0.0143\n",
      "Epoch 11, Epoch status: 0.89, Training loss: 0.0147\n",
      "evaluating epoch 11 ...\n",
      "MPII - epoch: 11, epoch status: 1.00, r@1: 0.113, r@5: 0.286, r@10: 0.387, mr: 22\n",
      "epoch: 11\n",
      "Epoch 12, Epoch status: 0.13, Training loss: 0.0118\n",
      "Epoch 12, Epoch status: 0.25, Training loss: 0.0119\n",
      "Epoch 12, Epoch status: 0.38, Training loss: 0.0127\n",
      "Epoch 12, Epoch status: 0.51, Training loss: 0.0126\n",
      "Epoch 12, Epoch status: 0.63, Training loss: 0.0127\n",
      "Epoch 12, Epoch status: 0.76, Training loss: 0.0132\n",
      "Epoch 12, Epoch status: 0.89, Training loss: 0.0134\n",
      "evaluating epoch 12 ...\n",
      "MPII - epoch: 12, epoch status: 1.00, r@1: 0.108, r@5: 0.280, r@10: 0.373, mr: 22\n",
      "epoch: 12\n",
      "Epoch 13, Epoch status: 0.13, Training loss: 0.0107\n",
      "Epoch 13, Epoch status: 0.25, Training loss: 0.0112\n",
      "Epoch 13, Epoch status: 0.38, Training loss: 0.0113\n",
      "Epoch 13, Epoch status: 0.51, Training loss: 0.0117\n",
      "Epoch 13, Epoch status: 0.63, Training loss: 0.0117\n",
      "Epoch 13, Epoch status: 0.76, Training loss: 0.0119\n",
      "Epoch 13, Epoch status: 0.89, Training loss: 0.0124\n",
      "evaluating epoch 13 ...\n",
      "MPII - epoch: 13, epoch status: 1.00, r@1: 0.115, r@5: 0.275, r@10: 0.388, mr: 21\n",
      "epoch: 13\n",
      "Epoch 14, Epoch status: 0.13, Training loss: 0.0099\n",
      "Epoch 14, Epoch status: 0.25, Training loss: 0.0101\n",
      "Epoch 14, Epoch status: 0.38, Training loss: 0.0106\n",
      "Epoch 14, Epoch status: 0.51, Training loss: 0.0108\n",
      "Epoch 14, Epoch status: 0.63, Training loss: 0.0108\n",
      "Epoch 14, Epoch status: 0.76, Training loss: 0.0111\n",
      "Epoch 14, Epoch status: 0.89, Training loss: 0.0113\n",
      "evaluating epoch 14 ...\n",
      "MPII - epoch: 14, epoch status: 1.00, r@1: 0.109, r@5: 0.285, r@10: 0.377, mr: 20\n",
      "epoch: 14\n",
      "Epoch 15, Epoch status: 0.13, Training loss: 0.0091\n",
      "Epoch 15, Epoch status: 0.25, Training loss: 0.0094\n",
      "Epoch 15, Epoch status: 0.38, Training loss: 0.0095\n",
      "Epoch 15, Epoch status: 0.51, Training loss: 0.0100\n",
      "Epoch 15, Epoch status: 0.63, Training loss: 0.0102\n",
      "Epoch 15, Epoch status: 0.76, Training loss: 0.0105\n",
      "Epoch 15, Epoch status: 0.89, Training loss: 0.0103\n",
      "evaluating epoch 15 ...\n",
      "MPII - epoch: 15, epoch status: 1.00, r@1: 0.112, r@5: 0.290, r@10: 0.386, mr: 22\n",
      "epoch: 15\n",
      "Epoch 16, Epoch status: 0.13, Training loss: 0.0084\n",
      "Epoch 16, Epoch status: 0.25, Training loss: 0.0087\n",
      "Epoch 16, Epoch status: 0.38, Training loss: 0.0088\n",
      "Epoch 16, Epoch status: 0.51, Training loss: 0.0092\n",
      "Epoch 16, Epoch status: 0.63, Training loss: 0.0093\n",
      "Epoch 16, Epoch status: 0.76, Training loss: 0.0094\n",
      "Epoch 16, Epoch status: 0.89, Training loss: 0.0095\n",
      "evaluating epoch 16 ...\n",
      "MPII - epoch: 16, epoch status: 1.00, r@1: 0.103, r@5: 0.282, r@10: 0.386, mr: 22\n",
      "epoch: 16\n",
      "Epoch 17, Epoch status: 0.13, Training loss: 0.0078\n",
      "Epoch 17, Epoch status: 0.25, Training loss: 0.0080\n",
      "Epoch 17, Epoch status: 0.38, Training loss: 0.0082\n",
      "Epoch 17, Epoch status: 0.51, Training loss: 0.0086\n",
      "Epoch 17, Epoch status: 0.63, Training loss: 0.0085\n",
      "Epoch 17, Epoch status: 0.76, Training loss: 0.0089\n",
      "Epoch 17, Epoch status: 0.89, Training loss: 0.0090\n",
      "evaluating epoch 17 ...\n",
      "MPII - epoch: 17, epoch status: 1.00, r@1: 0.113, r@5: 0.285, r@10: 0.381, mr: 23\n",
      "epoch: 17\n",
      "Epoch 18, Epoch status: 0.13, Training loss: 0.0073\n",
      "Epoch 18, Epoch status: 0.25, Training loss: 0.0076\n",
      "Epoch 18, Epoch status: 0.38, Training loss: 0.0077\n",
      "Epoch 18, Epoch status: 0.51, Training loss: 0.0078\n",
      "Epoch 18, Epoch status: 0.63, Training loss: 0.0079\n",
      "Epoch 18, Epoch status: 0.76, Training loss: 0.0081\n",
      "Epoch 18, Epoch status: 0.89, Training loss: 0.0084\n",
      "evaluating epoch 18 ...\n",
      "MPII - epoch: 18, epoch status: 1.00, r@1: 0.108, r@5: 0.290, r@10: 0.388, mr: 23\n",
      "epoch: 18\n",
      "Epoch 19, Epoch status: 0.13, Training loss: 0.0066\n",
      "Epoch 19, Epoch status: 0.25, Training loss: 0.0069\n",
      "Epoch 19, Epoch status: 0.38, Training loss: 0.0072\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19, Epoch status: 0.51, Training loss: 0.0073\n",
      "Epoch 19, Epoch status: 0.63, Training loss: 0.0075\n",
      "Epoch 19, Epoch status: 0.76, Training loss: 0.0076\n",
      "Epoch 19, Epoch status: 0.89, Training loss: 0.0079\n",
      "evaluating epoch 19 ...\n",
      "MPII - epoch: 19, epoch status: 1.00, r@1: 0.104, r@5: 0.288, r@10: 0.399, mr: 23\n",
      "epoch: 19\n",
      "Epoch 20, Epoch status: 0.13, Training loss: 0.0062\n",
      "Epoch 20, Epoch status: 0.25, Training loss: 0.0065\n",
      "Epoch 20, Epoch status: 0.38, Training loss: 0.0067\n",
      "Epoch 20, Epoch status: 0.51, Training loss: 0.0068\n",
      "Epoch 20, Epoch status: 0.63, Training loss: 0.0069\n",
      "Epoch 20, Epoch status: 0.76, Training loss: 0.0072\n",
      "Epoch 20, Epoch status: 0.89, Training loss: 0.0073\n",
      "evaluating epoch 20 ...\n",
      "MPII - epoch: 20, epoch status: 1.00, r@1: 0.104, r@5: 0.284, r@10: 0.384, mr: 22\n",
      "epoch: 20\n",
      "Epoch 21, Epoch status: 0.13, Training loss: 0.0059\n",
      "Epoch 21, Epoch status: 0.25, Training loss: 0.0061\n",
      "Epoch 21, Epoch status: 0.38, Training loss: 0.0062\n",
      "Epoch 21, Epoch status: 0.51, Training loss: 0.0063\n",
      "Epoch 21, Epoch status: 0.63, Training loss: 0.0065\n",
      "Epoch 21, Epoch status: 0.76, Training loss: 0.0068\n",
      "Epoch 21, Epoch status: 0.89, Training loss: 0.0067\n",
      "evaluating epoch 21 ...\n",
      "MPII - epoch: 21, epoch status: 1.00, r@1: 0.111, r@5: 0.282, r@10: 0.392, mr: 23\n",
      "epoch: 21\n",
      "Epoch 22, Epoch status: 0.13, Training loss: 0.0055\n",
      "Epoch 22, Epoch status: 0.25, Training loss: 0.0058\n",
      "Epoch 22, Epoch status: 0.38, Training loss: 0.0060\n",
      "Epoch 22, Epoch status: 0.51, Training loss: 0.0060\n",
      "Epoch 22, Epoch status: 0.63, Training loss: 0.0061\n",
      "Epoch 22, Epoch status: 0.76, Training loss: 0.0063\n",
      "Epoch 22, Epoch status: 0.89, Training loss: 0.0064\n",
      "evaluating epoch 22 ...\n",
      "MPII - epoch: 22, epoch status: 1.00, r@1: 0.103, r@5: 0.289, r@10: 0.391, mr: 22\n",
      "epoch: 22\n",
      "Epoch 23, Epoch status: 0.13, Training loss: 0.0053\n",
      "Epoch 23, Epoch status: 0.25, Training loss: 0.0054\n",
      "Epoch 23, Epoch status: 0.38, Training loss: 0.0056\n",
      "Epoch 23, Epoch status: 0.51, Training loss: 0.0057\n",
      "Epoch 23, Epoch status: 0.63, Training loss: 0.0058\n",
      "Epoch 23, Epoch status: 0.76, Training loss: 0.0059\n",
      "Epoch 23, Epoch status: 0.89, Training loss: 0.0060\n",
      "evaluating epoch 23 ...\n",
      "MPII - epoch: 23, epoch status: 1.00, r@1: 0.110, r@5: 0.279, r@10: 0.381, mr: 24\n",
      "epoch: 23\n",
      "Epoch 24, Epoch status: 0.13, Training loss: 0.0051\n",
      "Epoch 24, Epoch status: 0.25, Training loss: 0.0052\n",
      "Epoch 24, Epoch status: 0.38, Training loss: 0.0052\n",
      "Epoch 24, Epoch status: 0.51, Training loss: 0.0054\n",
      "Epoch 24, Epoch status: 0.63, Training loss: 0.0055\n",
      "Epoch 24, Epoch status: 0.76, Training loss: 0.0055\n",
      "Epoch 24, Epoch status: 0.89, Training loss: 0.0058\n",
      "evaluating epoch 24 ...\n",
      "MPII - epoch: 24, epoch status: 1.00, r@1: 0.103, r@5: 0.283, r@10: 0.383, mr: 23\n",
      "epoch: 24\n",
      "Epoch 25, Epoch status: 0.13, Training loss: 0.0047\n",
      "Epoch 25, Epoch status: 0.25, Training loss: 0.0048\n",
      "Epoch 25, Epoch status: 0.38, Training loss: 0.0049\n",
      "Epoch 25, Epoch status: 0.51, Training loss: 0.0051\n",
      "Epoch 25, Epoch status: 0.63, Training loss: 0.0052\n",
      "Epoch 25, Epoch status: 0.76, Training loss: 0.0053\n",
      "Epoch 25, Epoch status: 0.89, Training loss: 0.0054\n",
      "evaluating epoch 25 ...\n",
      "MPII - epoch: 25, epoch status: 1.00, r@1: 0.101, r@5: 0.279, r@10: 0.384, mr: 24\n",
      "epoch: 25\n",
      "Epoch 26, Epoch status: 0.13, Training loss: 0.0045\n",
      "Epoch 26, Epoch status: 0.25, Training loss: 0.0046\n",
      "Epoch 26, Epoch status: 0.38, Training loss: 0.0048\n",
      "Epoch 26, Epoch status: 0.51, Training loss: 0.0048\n",
      "Epoch 26, Epoch status: 0.63, Training loss: 0.0050\n",
      "Epoch 26, Epoch status: 0.76, Training loss: 0.0050\n",
      "Epoch 26, Epoch status: 0.89, Training loss: 0.0050\n",
      "evaluating epoch 26 ...\n",
      "MPII - epoch: 26, epoch status: 1.00, r@1: 0.102, r@5: 0.273, r@10: 0.374, mr: 24\n",
      "epoch: 26\n",
      "Epoch 27, Epoch status: 0.13, Training loss: 0.0043\n",
      "Epoch 27, Epoch status: 0.25, Training loss: 0.0045\n",
      "Epoch 27, Epoch status: 0.38, Training loss: 0.0045\n",
      "Epoch 27, Epoch status: 0.51, Training loss: 0.0046\n",
      "Epoch 27, Epoch status: 0.63, Training loss: 0.0046\n",
      "Epoch 27, Epoch status: 0.76, Training loss: 0.0048\n",
      "Epoch 27, Epoch status: 0.89, Training loss: 0.0049\n",
      "evaluating epoch 27 ...\n",
      "MPII - epoch: 27, epoch status: 1.00, r@1: 0.106, r@5: 0.274, r@10: 0.394, mr: 23\n",
      "epoch: 27\n",
      "Epoch 28, Epoch status: 0.13, Training loss: 0.0041\n",
      "Epoch 28, Epoch status: 0.25, Training loss: 0.0042\n",
      "Epoch 28, Epoch status: 0.38, Training loss: 0.0042\n",
      "Epoch 28, Epoch status: 0.51, Training loss: 0.0043\n",
      "Epoch 28, Epoch status: 0.63, Training loss: 0.0044\n",
      "Epoch 28, Epoch status: 0.76, Training loss: 0.0046\n",
      "Epoch 28, Epoch status: 0.89, Training loss: 0.0046\n",
      "evaluating epoch 28 ...\n",
      "MPII - epoch: 28, epoch status: 1.00, r@1: 0.102, r@5: 0.287, r@10: 0.388, mr: 24\n",
      "epoch: 28\n",
      "Epoch 29, Epoch status: 0.13, Training loss: 0.0040\n",
      "Epoch 29, Epoch status: 0.25, Training loss: 0.0040\n",
      "Epoch 29, Epoch status: 0.38, Training loss: 0.0041\n",
      "Epoch 29, Epoch status: 0.51, Training loss: 0.0042\n",
      "Epoch 29, Epoch status: 0.63, Training loss: 0.0043\n",
      "Epoch 29, Epoch status: 0.76, Training loss: 0.0043\n",
      "Epoch 29, Epoch status: 0.89, Training loss: 0.0044\n",
      "evaluating epoch 29 ...\n",
      "MPII - epoch: 29, epoch status: 1.00, r@1: 0.104, r@5: 0.276, r@10: 0.376, mr: 23\n",
      "epoch: 29\n",
      "Epoch 30, Epoch status: 0.13, Training loss: 0.0038\n",
      "Epoch 30, Epoch status: 0.25, Training loss: 0.0038\n",
      "Epoch 30, Epoch status: 0.38, Training loss: 0.0039\n",
      "Epoch 30, Epoch status: 0.51, Training loss: 0.0040\n",
      "Epoch 30, Epoch status: 0.63, Training loss: 0.0041\n",
      "Epoch 30, Epoch status: 0.76, Training loss: 0.0041\n",
      "Epoch 30, Epoch status: 0.89, Training loss: 0.0042\n",
      "evaluating epoch 30 ...\n",
      "MPII - epoch: 30, epoch status: 1.00, r@1: 0.100, r@5: 0.269, r@10: 0.385, mr: 23\n",
      "epoch: 30\n",
      "Epoch 31, Epoch status: 0.13, Training loss: 0.0036\n",
      "Epoch 31, Epoch status: 0.25, Training loss: 0.0037\n",
      "Epoch 31, Epoch status: 0.38, Training loss: 0.0037\n",
      "Epoch 31, Epoch status: 0.51, Training loss: 0.0038\n",
      "Epoch 31, Epoch status: 0.63, Training loss: 0.0039\n",
      "Epoch 31, Epoch status: 0.76, Training loss: 0.0039\n",
      "Epoch 31, Epoch status: 0.89, Training loss: 0.0041\n",
      "evaluating epoch 31 ...\n",
      "MPII - epoch: 31, epoch status: 1.00, r@1: 0.099, r@5: 0.278, r@10: 0.388, mr: 25\n",
      "epoch: 31\n",
      "Epoch 32, Epoch status: 0.13, Training loss: 0.0035\n",
      "Epoch 32, Epoch status: 0.25, Training loss: 0.0035\n",
      "Epoch 32, Epoch status: 0.38, Training loss: 0.0037\n",
      "Epoch 32, Epoch status: 0.51, Training loss: 0.0037\n",
      "Epoch 32, Epoch status: 0.63, Training loss: 0.0038\n",
      "Epoch 32, Epoch status: 0.76, Training loss: 0.0038\n",
      "Epoch 32, Epoch status: 0.89, Training loss: 0.0038\n",
      "evaluating epoch 32 ...\n",
      "MPII - epoch: 32, epoch status: 1.00, r@1: 0.103, r@5: 0.273, r@10: 0.374, mr: 25\n",
      "epoch: 32\n",
      "Epoch 33, Epoch status: 0.13, Training loss: 0.0034\n",
      "Epoch 33, Epoch status: 0.25, Training loss: 0.0035\n",
      "Epoch 33, Epoch status: 0.38, Training loss: 0.0035\n",
      "Epoch 33, Epoch status: 0.51, Training loss: 0.0035\n",
      "Epoch 33, Epoch status: 0.63, Training loss: 0.0036\n",
      "Epoch 33, Epoch status: 0.76, Training loss: 0.0037\n",
      "Epoch 33, Epoch status: 0.89, Training loss: 0.0037\n",
      "evaluating epoch 33 ...\n",
      "MPII - epoch: 33, epoch status: 1.00, r@1: 0.101, r@5: 0.270, r@10: 0.371, mr: 27\n",
      "epoch: 33\n",
      "Epoch 34, Epoch status: 0.13, Training loss: 0.0033\n",
      "Epoch 34, Epoch status: 0.25, Training loss: 0.0033\n",
      "Epoch 34, Epoch status: 0.38, Training loss: 0.0034\n",
      "Epoch 34, Epoch status: 0.51, Training loss: 0.0034\n",
      "Epoch 34, Epoch status: 0.63, Training loss: 0.0035\n",
      "Epoch 34, Epoch status: 0.76, Training loss: 0.0035\n",
      "Epoch 34, Epoch status: 0.89, Training loss: 0.0036\n",
      "evaluating epoch 34 ...\n",
      "MPII - epoch: 34, epoch status: 1.00, r@1: 0.100, r@5: 0.270, r@10: 0.382, mr: 26\n",
      "epoch: 34\n",
      "Epoch 35, Epoch status: 0.13, Training loss: 0.0032\n",
      "Epoch 35, Epoch status: 0.25, Training loss: 0.0032\n",
      "Epoch 35, Epoch status: 0.38, Training loss: 0.0032\n",
      "Epoch 35, Epoch status: 0.51, Training loss: 0.0033\n",
      "Epoch 35, Epoch status: 0.63, Training loss: 0.0034\n",
      "Epoch 35, Epoch status: 0.76, Training loss: 0.0034\n",
      "Epoch 35, Epoch status: 0.89, Training loss: 0.0035\n",
      "evaluating epoch 35 ...\n",
      "MPII - epoch: 35, epoch status: 1.00, r@1: 0.091, r@5: 0.275, r@10: 0.370, mr: 26\n",
      "epoch: 35\n",
      "Epoch 36, Epoch status: 0.13, Training loss: 0.0031\n",
      "Epoch 36, Epoch status: 0.25, Training loss: 0.0031\n",
      "Epoch 36, Epoch status: 0.38, Training loss: 0.0032\n",
      "Epoch 36, Epoch status: 0.51, Training loss: 0.0032\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36, Epoch status: 0.63, Training loss: 0.0032\n",
      "Epoch 36, Epoch status: 0.76, Training loss: 0.0033\n",
      "Epoch 36, Epoch status: 0.89, Training loss: 0.0034\n",
      "evaluating epoch 36 ...\n",
      "MPII - epoch: 36, epoch status: 1.00, r@1: 0.101, r@5: 0.270, r@10: 0.369, mr: 25\n",
      "epoch: 36\n",
      "Epoch 37, Epoch status: 0.13, Training loss: 0.0030\n",
      "Epoch 37, Epoch status: 0.25, Training loss: 0.0030\n",
      "Epoch 37, Epoch status: 0.38, Training loss: 0.0031\n",
      "Epoch 37, Epoch status: 0.51, Training loss: 0.0031\n",
      "Epoch 37, Epoch status: 0.63, Training loss: 0.0032\n",
      "Epoch 37, Epoch status: 0.76, Training loss: 0.0032\n",
      "Epoch 37, Epoch status: 0.89, Training loss: 0.0033\n",
      "evaluating epoch 37 ...\n",
      "MPII - epoch: 37, epoch status: 1.00, r@1: 0.096, r@5: 0.266, r@10: 0.364, mr: 25\n",
      "epoch: 37\n",
      "Epoch 38, Epoch status: 0.13, Training loss: 0.0029\n",
      "Epoch 38, Epoch status: 0.25, Training loss: 0.0030\n",
      "Epoch 38, Epoch status: 0.38, Training loss: 0.0030\n",
      "Epoch 38, Epoch status: 0.51, Training loss: 0.0030\n",
      "Epoch 38, Epoch status: 0.63, Training loss: 0.0031\n",
      "Epoch 38, Epoch status: 0.76, Training loss: 0.0031\n",
      "Epoch 38, Epoch status: 0.89, Training loss: 0.0032\n",
      "evaluating epoch 38 ...\n",
      "MPII - epoch: 38, epoch status: 1.00, r@1: 0.103, r@5: 0.263, r@10: 0.376, mr: 25\n",
      "epoch: 38\n",
      "Epoch 39, Epoch status: 0.13, Training loss: 0.0028\n",
      "Epoch 39, Epoch status: 0.25, Training loss: 0.0029\n",
      "Epoch 39, Epoch status: 0.38, Training loss: 0.0029\n",
      "Epoch 39, Epoch status: 0.51, Training loss: 0.0030\n",
      "Epoch 39, Epoch status: 0.63, Training loss: 0.0030\n",
      "Epoch 39, Epoch status: 0.76, Training loss: 0.0030\n",
      "Epoch 39, Epoch status: 0.89, Training loss: 0.0030\n",
      "evaluating epoch 39 ...\n",
      "MPII - epoch: 39, epoch status: 1.00, r@1: 0.097, r@5: 0.260, r@10: 0.361, mr: 28\n",
      "epoch: 39\n",
      "Epoch 40, Epoch status: 0.13, Training loss: 0.0028\n",
      "Epoch 40, Epoch status: 0.25, Training loss: 0.0028\n",
      "Epoch 40, Epoch status: 0.38, Training loss: 0.0028\n",
      "Epoch 40, Epoch status: 0.51, Training loss: 0.0029\n",
      "Epoch 40, Epoch status: 0.63, Training loss: 0.0030\n",
      "Epoch 40, Epoch status: 0.76, Training loss: 0.0029\n",
      "Epoch 40, Epoch status: 0.89, Training loss: 0.0030\n",
      "evaluating epoch 40 ...\n",
      "MPII - epoch: 40, epoch status: 1.00, r@1: 0.096, r@5: 0.266, r@10: 0.361, mr: 27\n",
      "epoch: 40\n",
      "Epoch 41, Epoch status: 0.13, Training loss: 0.0027\n",
      "Epoch 41, Epoch status: 0.25, Training loss: 0.0028\n",
      "Epoch 41, Epoch status: 0.38, Training loss: 0.0028\n",
      "Epoch 41, Epoch status: 0.51, Training loss: 0.0028\n",
      "Epoch 41, Epoch status: 0.63, Training loss: 0.0028\n",
      "Epoch 41, Epoch status: 0.76, Training loss: 0.0029\n",
      "Epoch 41, Epoch status: 0.89, Training loss: 0.0029\n",
      "evaluating epoch 41 ...\n",
      "MPII - epoch: 41, epoch status: 1.00, r@1: 0.095, r@5: 0.261, r@10: 0.361, mr: 27\n",
      "epoch: 41\n",
      "Epoch 42, Epoch status: 0.13, Training loss: 0.0027\n",
      "Epoch 42, Epoch status: 0.25, Training loss: 0.0027\n",
      "Epoch 42, Epoch status: 0.38, Training loss: 0.0027\n",
      "Epoch 42, Epoch status: 0.51, Training loss: 0.0028\n",
      "Epoch 42, Epoch status: 0.63, Training loss: 0.0028\n",
      "Epoch 42, Epoch status: 0.76, Training loss: 0.0028\n",
      "Epoch 42, Epoch status: 0.89, Training loss: 0.0028\n",
      "evaluating epoch 42 ...\n",
      "MPII - epoch: 42, epoch status: 1.00, r@1: 0.094, r@5: 0.262, r@10: 0.362, mr: 28\n",
      "epoch: 42\n",
      "Epoch 43, Epoch status: 0.13, Training loss: 0.0027\n",
      "Epoch 43, Epoch status: 0.25, Training loss: 0.0027\n",
      "Epoch 43, Epoch status: 0.38, Training loss: 0.0027\n",
      "Epoch 43, Epoch status: 0.51, Training loss: 0.0027\n",
      "Epoch 43, Epoch status: 0.63, Training loss: 0.0027\n",
      "Epoch 43, Epoch status: 0.76, Training loss: 0.0027\n",
      "Epoch 43, Epoch status: 0.89, Training loss: 0.0028\n",
      "evaluating epoch 43 ...\n",
      "MPII - epoch: 43, epoch status: 1.00, r@1: 0.103, r@5: 0.257, r@10: 0.365, mr: 27\n",
      "epoch: 43\n",
      "Epoch 44, Epoch status: 0.13, Training loss: 0.0026\n",
      "Epoch 44, Epoch status: 0.25, Training loss: 0.0026\n",
      "Epoch 44, Epoch status: 0.38, Training loss: 0.0026\n",
      "Epoch 44, Epoch status: 0.51, Training loss: 0.0026\n",
      "Epoch 44, Epoch status: 0.63, Training loss: 0.0027\n",
      "Epoch 44, Epoch status: 0.76, Training loss: 0.0027\n",
      "Epoch 44, Epoch status: 0.89, Training loss: 0.0027\n",
      "evaluating epoch 44 ...\n",
      "MPII - epoch: 44, epoch status: 1.00, r@1: 0.095, r@5: 0.265, r@10: 0.367, mr: 27\n",
      "epoch: 44\n",
      "Epoch 45, Epoch status: 0.13, Training loss: 0.0025\n",
      "Epoch 45, Epoch status: 0.25, Training loss: 0.0026\n",
      "Epoch 45, Epoch status: 0.38, Training loss: 0.0026\n",
      "Epoch 45, Epoch status: 0.51, Training loss: 0.0026\n",
      "Epoch 45, Epoch status: 0.63, Training loss: 0.0026\n",
      "Epoch 45, Epoch status: 0.76, Training loss: 0.0027\n",
      "Epoch 45, Epoch status: 0.89, Training loss: 0.0027\n",
      "evaluating epoch 45 ...\n",
      "MPII - epoch: 45, epoch status: 1.00, r@1: 0.096, r@5: 0.266, r@10: 0.363, mr: 28\n",
      "epoch: 45\n",
      "Epoch 46, Epoch status: 0.13, Training loss: 0.0025\n",
      "Epoch 46, Epoch status: 0.25, Training loss: 0.0025\n",
      "Epoch 46, Epoch status: 0.38, Training loss: 0.0025\n",
      "Epoch 46, Epoch status: 0.51, Training loss: 0.0025\n",
      "Epoch 46, Epoch status: 0.63, Training loss: 0.0026\n",
      "Epoch 46, Epoch status: 0.76, Training loss: 0.0026\n",
      "Epoch 46, Epoch status: 0.89, Training loss: 0.0027\n",
      "evaluating epoch 46 ...\n",
      "MPII - epoch: 46, epoch status: 1.00, r@1: 0.096, r@5: 0.257, r@10: 0.362, mr: 27\n",
      "epoch: 46\n",
      "Epoch 47, Epoch status: 0.13, Training loss: 0.0025\n",
      "Epoch 47, Epoch status: 0.25, Training loss: 0.0025\n",
      "Epoch 47, Epoch status: 0.38, Training loss: 0.0025\n",
      "Epoch 47, Epoch status: 0.51, Training loss: 0.0025\n",
      "Epoch 47, Epoch status: 0.63, Training loss: 0.0026\n",
      "Epoch 47, Epoch status: 0.76, Training loss: 0.0026\n",
      "Epoch 47, Epoch status: 0.89, Training loss: 0.0026\n",
      "evaluating epoch 47 ...\n",
      "MPII - epoch: 47, epoch status: 1.00, r@1: 0.096, r@5: 0.261, r@10: 0.354, mr: 29\n",
      "epoch: 47\n",
      "Epoch 48, Epoch status: 0.13, Training loss: 0.0024\n",
      "Epoch 48, Epoch status: 0.25, Training loss: 0.0024\n",
      "Epoch 48, Epoch status: 0.38, Training loss: 0.0025\n",
      "Epoch 48, Epoch status: 0.51, Training loss: 0.0025\n",
      "Epoch 48, Epoch status: 0.63, Training loss: 0.0025\n",
      "Epoch 48, Epoch status: 0.76, Training loss: 0.0025\n",
      "Epoch 48, Epoch status: 0.89, Training loss: 0.0025\n",
      "evaluating epoch 48 ...\n",
      "MPII - epoch: 48, epoch status: 1.00, r@1: 0.096, r@5: 0.267, r@10: 0.357, mr: 28\n",
      "epoch: 48\n",
      "Epoch 49, Epoch status: 0.13, Training loss: 0.0024\n",
      "Epoch 49, Epoch status: 0.25, Training loss: 0.0024\n",
      "Epoch 49, Epoch status: 0.38, Training loss: 0.0024\n",
      "Epoch 49, Epoch status: 0.51, Training loss: 0.0025\n",
      "Epoch 49, Epoch status: 0.63, Training loss: 0.0025\n",
      "Epoch 49, Epoch status: 0.76, Training loss: 0.0025\n",
      "Epoch 49, Epoch status: 0.89, Training loss: 0.0025\n",
      "evaluating epoch 49 ...\n",
      "MPII - epoch: 49, epoch status: 1.00, r@1: 0.094, r@5: 0.262, r@10: 0.355, mr: 27\n",
      "epoch: 49\n",
      "Epoch 50, Epoch status: 0.13, Training loss: 0.0024\n",
      "Epoch 50, Epoch status: 0.25, Training loss: 0.0024\n",
      "Epoch 50, Epoch status: 0.38, Training loss: 0.0024\n",
      "Epoch 50, Epoch status: 0.51, Training loss: 0.0024\n",
      "Epoch 50, Epoch status: 0.63, Training loss: 0.0024\n",
      "Epoch 50, Epoch status: 0.76, Training loss: 0.0024\n",
      "Epoch 50, Epoch status: 0.89, Training loss: 0.0025\n",
      "evaluating epoch 50 ...\n",
      "MPII - epoch: 50, epoch status: 1.00, r@1: 0.092, r@5: 0.257, r@10: 0.351, mr: 30\n"
     ]
    }
   ],
   "source": [
    "%run train.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch]",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
